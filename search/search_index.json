{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Azure Solution Architecture Guidance \u2693\ufe0e This Azure Solution Guidance describes the approach for guiding Kyndryl's clients in their cloud adoption journey. The purpose of this guidance is to provide a comprehensive approach, from understanding and defining client needs, to design a solution which addresses the client needs for cloud adoption, as well as lay the foundation for client's cloud expansion. This guidance is in alignment with Microsoft's Cloud Adoption Framework (CAF) and Kyndryl's Cloud Architecture Design (CAD) , and is applicable across client engagements. Target Audience \u2693\ufe0e This document provides a framework for Kyndryl's Solution Architects to develop solution design for Kyndryl's clients on Microsoft's Azure cloud platform. Re-Use of existing Solutions \u2693\ufe0e This guidance re-uses existing Kyndryl solutions and assets wherever possible. As an example, this guidance will point to Kyndryl's CMM for migration to Azure cloud.","title":"Introduction"},{"location":"#welcome-to-azure-solution-architecture-guidance","text":"This Azure Solution Guidance describes the approach for guiding Kyndryl's clients in their cloud adoption journey. The purpose of this guidance is to provide a comprehensive approach, from understanding and defining client needs, to design a solution which addresses the client needs for cloud adoption, as well as lay the foundation for client's cloud expansion. This guidance is in alignment with Microsoft's Cloud Adoption Framework (CAF) and Kyndryl's Cloud Architecture Design (CAD) , and is applicable across client engagements.","title":"Welcome to Azure Solution Architecture Guidance"},{"location":"#target-audience","text":"This document provides a framework for Kyndryl's Solution Architects to develop solution design for Kyndryl's clients on Microsoft's Azure cloud platform.","title":"Target Audience"},{"location":"#re-use-of-existing-solutions","text":"This guidance re-uses existing Kyndryl solutions and assets wherever possible. As an example, this guidance will point to Kyndryl's CMM for migration to Azure cloud.","title":"Re-Use of existing Solutions"},{"location":"about/1authors/","text":"Contributors \u2693\ufe0e Lead : Sreekrishnan Venkiteswaran, Distinguished Engineer Authors : Alwyn Lobo, Chief Architect, MCMS Ashok Ambati, Principal Consulting Architect Chandrashekar Arya Somayajula, Hybrid Cloud Solution Architect Rajesh Garg, Cloud Solution Architect Sam Swaminathan, Sr. Technical Staff Member & Cloud Solution Architect Contributions from extended team members providing critical inputs: To be Added","title":"Contributors"},{"location":"about/1authors/#contributors","text":"Lead : Sreekrishnan Venkiteswaran, Distinguished Engineer Authors : Alwyn Lobo, Chief Architect, MCMS Ashok Ambati, Principal Consulting Architect Chandrashekar Arya Somayajula, Hybrid Cloud Solution Architect Rajesh Garg, Cloud Solution Architect Sam Swaminathan, Sr. Technical Staff Member & Cloud Solution Architect Contributions from extended team members providing critical inputs: To be Added","title":"Contributors"},{"location":"about/author/","text":"Scope \u2693\ufe0e Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload . What is not covered \u2693\ufe0e This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"Scope"},{"location":"about/author/#scope","text":"Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload .","title":"Scope"},{"location":"about/author/#what-is-not-covered","text":"This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"What is not covered"},{"location":"about/scope/","text":"Scope \u2693\ufe0e Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload . What is not covered \u2693\ufe0e This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"Scope"},{"location":"about/scope/#scope","text":"Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload .","title":"Scope"},{"location":"about/scope/#what-is-not-covered","text":"This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"What is not covered"},{"location":"automation/automation/automation-architecture-copy/","text":"2. Automation Architecture \u2693\ufe0e This section explains how the above automation will be achieved for Kyndryl managed cloud platforms. 2.1 Build/On-Boarding Automation \u2693\ufe0e As explained in the previous section, build automation addresses landing zone creation as part of initial on-boarding, as well as subsequent Application/Workload creations to existing landing zones OR new landing zones creations. Landing Zone automation will implement requirements/recommendations from Landing Zone design, as well as application/workload specific requirements. Landing Zone automation itself can be categorized as 2 parts: 1) LZ Common Service(OR Shared) element Automation: Recommendations that apply to all/most common Azure services, and services which are managed by Kyndryl. These include, for example, Monitoring, Updates, Security, Backups etc, Governance policies, which can be applied to all/any workloads. 2) LZ Application specific elements: These are specific to application workloads - creation of VNets, Subnets, Web Application Firewalls, NSGs, VMs, AKS, DBs & other Azure services As an example, for Enterprise Scale Hub-Spoke Architecture , automation can be seperated into above classifications as shown below: The common service element automation, with configuration parameters, will be standard ( Common Services/Standard Templates ) and reusable for most deployments, whereas the client's application specific landing zone aspects ( Client Templates ) will be developed unique for each client. 2.1.1 Build Automation Architecture \u2693\ufe0e Build Automation can be implemented using various tools, either as standalone or using a combination of these tools, - Azure Native ARM Templates, Blueprints, Terraforms, Ansible playbooks, etc. Refer below links for Kyndryl's automation architecture defined by D4PC Offering : Detailed Functional Requirements for automation Detailed Non-Functional Requirements for automation Automation Architecture: Refer Automation architecture for Azure 2.1.1.1 Key Elements of D4PC Automation Architecture \u2693\ufe0e 1.IaC & DevOps \u2693\ufe0e Infrastructure-as-code (IaC) is the foundation for build automations on Azure, via the Azure Resource Manager, well as Terraform & Ansible. IaC code will be source controlled in GitHub, and deployed using a combination Azure DevOps tools as well as external tools, as shown below: PIC to be Updated for Azure : Additional reference on using Azure pipelines for deploying IaC 2.TEMPLATES \u2693\ufe0e Above architecture defines Standard vs Client Templates , however Standard templates does not address all the platform elements defined in Kyndryl's Landing Zone requirements. A new Standard Template should be created covering the platform elements: 1) LZ Common Services . 2) Policies for commonly used services: - VM Policies. - Storage Policies. Network Policies . - AKS Policies. - ASB Policies (optional). - Add more as needed. There are a few standard templates already published by PCID , which needs to be synched with the above LZ automation approach. 3.DIGITAL INTAKE \u2693\ufe0e \"For this we have an excel tool that asks you to provide a series of answers and then provides the most likely hierarchical structure for the landing zone\" 2.2 Migrate Automation \u2693\ufe0e Refer to CMM Tools addressing various aspects of migration to Azure 2.3 Manage Automation \u2693\ufe0e Automation Framework for manage automation addresses operational & governance aspects once a client/application/workload is onboarded onto Azure cloud platform. In addition, this also involves making changes to existing resources, as well as adding resources to existing landing zones. In an enterprise world, changes and additions to a production environment should be performed in a controlled but automated manner. 2.3.1 Manage Automation Architecture \u2693\ufe0e In an enterprise world, changes and additions to a production environment should be performed in an automated but controlled manner. Kyndryl's target architecture for managing cloud platform & services should address through a combination of Service Management Tools & processes AND GitOps Model. ITSM based Architecture: Requests for additions and changes will be initiated through a Cloud Management Platform (CMP) tool which will have service catalogues defined for provisioning new services as well as for modifying existing resources. Requests will be approved in ITSM tool, where workflows will be created to auto-approve standard changes. Below picture shows the Automation Framework for manage automations. 2.3.1.1 Automation Architecture Components \u2693\ufe0e Cloud Management Platform (CMP) \u2693\ufe0e Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and evaluate various custom/In-built Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities. Business Approval & IT Service Management \u2693\ufe0e There should be capability to raise and track service request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should be capability of raising Change Requests for operations requiring infrastructure changes. There should also exist an Enterprise grade Configuration Management Database ( CMDB ). An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of sequencing various tasks for make manage, agents installation, custom steps execution etc. This should also be able to handle Day 2 Change Request based service flow execution which can coordinate various tasks. Monitoring & Management Tools \u2693\ufe0e Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc. Automation Engine \u2693\ufe0e Automation Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution. GitHub \u2693\ufe0e Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well. Resource Delegations / Azure LightHouse \u2693\ufe0e All automation activities can be performed on the client resources without establishing any connections by using Azure Light House which delegates client resources to be managed by Kyndryl (or any Service Provider/SP) onto the managing SP subscription. All Kyndryl managed clients will have Azure LightHouse delegations enabled. 2.3.1.2 Configuration Updates \u2693\ufe0e All updates to resources as well as to the management tools like monitoring, updates, backup etc, will be performed through Azure native tools and/or Terraform and Ansible","title":"Automation architecture copy"},{"location":"automation/automation/automation-architecture-copy/#2-automation-architecture","text":"This section explains how the above automation will be achieved for Kyndryl managed cloud platforms.","title":"2. Automation Architecture"},{"location":"automation/automation/automation-architecture-copy/#21-buildon-boarding-automation","text":"As explained in the previous section, build automation addresses landing zone creation as part of initial on-boarding, as well as subsequent Application/Workload creations to existing landing zones OR new landing zones creations. Landing Zone automation will implement requirements/recommendations from Landing Zone design, as well as application/workload specific requirements. Landing Zone automation itself can be categorized as 2 parts: 1) LZ Common Service(OR Shared) element Automation: Recommendations that apply to all/most common Azure services, and services which are managed by Kyndryl. These include, for example, Monitoring, Updates, Security, Backups etc, Governance policies, which can be applied to all/any workloads. 2) LZ Application specific elements: These are specific to application workloads - creation of VNets, Subnets, Web Application Firewalls, NSGs, VMs, AKS, DBs & other Azure services As an example, for Enterprise Scale Hub-Spoke Architecture , automation can be seperated into above classifications as shown below: The common service element automation, with configuration parameters, will be standard ( Common Services/Standard Templates ) and reusable for most deployments, whereas the client's application specific landing zone aspects ( Client Templates ) will be developed unique for each client.","title":"2.1 Build/On-Boarding Automation"},{"location":"automation/automation/automation-architecture-copy/#211-build-automation-architecture","text":"Build Automation can be implemented using various tools, either as standalone or using a combination of these tools, - Azure Native ARM Templates, Blueprints, Terraforms, Ansible playbooks, etc. Refer below links for Kyndryl's automation architecture defined by D4PC Offering : Detailed Functional Requirements for automation Detailed Non-Functional Requirements for automation Automation Architecture: Refer Automation architecture for Azure","title":"2.1.1 Build Automation Architecture"},{"location":"automation/automation/automation-architecture-copy/#2111-key-elements-of-d4pc-automation-architecture","text":"","title":"2.1.1.1 Key Elements of D4PC Automation Architecture"},{"location":"automation/automation/automation-architecture-copy/#1iac-devops","text":"Infrastructure-as-code (IaC) is the foundation for build automations on Azure, via the Azure Resource Manager, well as Terraform & Ansible. IaC code will be source controlled in GitHub, and deployed using a combination Azure DevOps tools as well as external tools, as shown below: PIC to be Updated for Azure : Additional reference on using Azure pipelines for deploying IaC","title":"1.IaC &amp; DevOps"},{"location":"automation/automation/automation-architecture-copy/#2templates","text":"Above architecture defines Standard vs Client Templates , however Standard templates does not address all the platform elements defined in Kyndryl's Landing Zone requirements. A new Standard Template should be created covering the platform elements: 1) LZ Common Services . 2) Policies for commonly used services: - VM Policies. - Storage Policies. Network Policies . - AKS Policies. - ASB Policies (optional). - Add more as needed. There are a few standard templates already published by PCID , which needs to be synched with the above LZ automation approach.","title":"2.TEMPLATES"},{"location":"automation/automation/automation-architecture-copy/#3digital-intake","text":"\"For this we have an excel tool that asks you to provide a series of answers and then provides the most likely hierarchical structure for the landing zone\"","title":"3.DIGITAL INTAKE"},{"location":"automation/automation/automation-architecture-copy/#22-migrate-automation","text":"Refer to CMM Tools addressing various aspects of migration to Azure","title":"2.2 Migrate Automation"},{"location":"automation/automation/automation-architecture-copy/#23-manage-automation","text":"Automation Framework for manage automation addresses operational & governance aspects once a client/application/workload is onboarded onto Azure cloud platform. In addition, this also involves making changes to existing resources, as well as adding resources to existing landing zones. In an enterprise world, changes and additions to a production environment should be performed in a controlled but automated manner.","title":"2.3 Manage Automation"},{"location":"automation/automation/automation-architecture-copy/#231-manage-automation-architecture","text":"In an enterprise world, changes and additions to a production environment should be performed in an automated but controlled manner. Kyndryl's target architecture for managing cloud platform & services should address through a combination of Service Management Tools & processes AND GitOps Model. ITSM based Architecture: Requests for additions and changes will be initiated through a Cloud Management Platform (CMP) tool which will have service catalogues defined for provisioning new services as well as for modifying existing resources. Requests will be approved in ITSM tool, where workflows will be created to auto-approve standard changes. Below picture shows the Automation Framework for manage automations.","title":"2.3.1 Manage Automation Architecture"},{"location":"automation/automation/automation-architecture-copy/#2311-automation-architecture-components","text":"","title":"2.3.1.1 Automation Architecture Components"},{"location":"automation/automation/automation-architecture-copy/#cloud-management-platform-cmp","text":"Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and evaluate various custom/In-built Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities.","title":"Cloud Management Platform (CMP)"},{"location":"automation/automation/automation-architecture-copy/#business-approval-it-service-management","text":"There should be capability to raise and track service request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should be capability of raising Change Requests for operations requiring infrastructure changes. There should also exist an Enterprise grade Configuration Management Database ( CMDB ). An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of sequencing various tasks for make manage, agents installation, custom steps execution etc. This should also be able to handle Day 2 Change Request based service flow execution which can coordinate various tasks.","title":"Business Approval &amp; IT Service Management"},{"location":"automation/automation/automation-architecture-copy/#monitoring-management-tools","text":"Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc.","title":"Monitoring &amp; Management Tools"},{"location":"automation/automation/automation-architecture-copy/#automation-engine","text":"Automation Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution.","title":"Automation Engine"},{"location":"automation/automation/automation-architecture-copy/#github","text":"Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well.","title":"GitHub"},{"location":"automation/automation/automation-architecture-copy/#resource-delegations-azure-lighthouse","text":"All automation activities can be performed on the client resources without establishing any connections by using Azure Light House which delegates client resources to be managed by Kyndryl (or any Service Provider/SP) onto the managing SP subscription. All Kyndryl managed clients will have Azure LightHouse delegations enabled.","title":"Resource Delegations / Azure LightHouse"},{"location":"automation/automation/automation-architecture-copy/#2312-configuration-updates","text":"All updates to resources as well as to the management tools like monitoring, updates, backup etc, will be performed through Azure native tools and/or Terraform and Ansible","title":"2.3.1.2 Configuration Updates"},{"location":"automation/automation/automation-architecture/","text":"2. Cloud Automation Architecture \u2693\ufe0e This section explains how the above automation will be achieved for Kyndryl managed cloud platforms. Automation Platform and Architecture for Kyndryl will address Build, Migrate and Manage automation requirements. Initial focus of this document is focused on Build and Manage aspects of automation. 2.1 Automation Architecture - D4PC \u2693\ufe0e D4PC has defined an automation architecture focusing mostly on Landing Zone build/On-boarding aspect. D4PC uses various tools - Azure Native ARM Templates, Blueprints, Terraforms, Ansible playbooks, etc. for the automation. Refer below links for automation architecture defined by D4PC Offering : Automation Architecture: D4PC Automation architecture for Azure Component Model: D4PC Architecture - Component Model Operational Model: D4PC Architecture - Operational Model Functional Requirements for automation Non-Functional Requirements for automation 2.1.1 Key Elements of D4PC Cloud Automation Architecture \u2693\ufe0e 1.IaC & DevOps \u2693\ufe0e Infrastructure-as-code (IaC) is the foundation for build automations on Azure, via the Azure Resource Manager, well as Terraform & Ansible. IaC code will be source controlled in GitHub, and deployed using a combination Azure DevOps tools as well as external tools, as shown below: PIC to be Updated for Azure : Additional reference on using Azure pipelines for deploying IaC 2.TEMPLATES \u2693\ufe0e Above architecture defines Standard vs Client Templates , Standard templates for most common application topologies/scenarios, whereas Client templates are application specific configurations. There are a few standard templates already published by PCID . GAP : However above Standard templates does not address all the platform elements** defined in Kyndryl's Landing Zone requirements. A new Standard Template should be created covering the platform elements: 1) LZ Common Services . 2) Policies for commonly used services: - VM Policies. - Storage Policies. - Network Policies . - AKS Policies. - ASB Policies (optional). - Additions as needed. 3.DIGITAL INTAKE \u2693\ufe0e There is an excel tool that prompts to provide a series of answers and then provides the most likely hierarchical structure for the landing zone (not tested by Tiger team) 2.2 Kyndryl's Cloud Automation Architecture \u2693\ufe0e Kyndryl's Cloud Automation Architecture & platform will be common for Build & Manage. (Migration aspect will be reviewed later) Automation Framework for manage automation addresses operational & governance aspects once a client/application/workload is onboarded onto Azure cloud platform, after the first Landing Zone creation. Manage aspect also involves making changes to existing resources , as well as adding resources to existing landing zones. In an enterprise world, changes and additions to a production environment should be performed in a controlled but automated manner . Kyndryl's cloud automation solution will follow an Operating Model which will cater to organizations following traditional IT/ITSM based model for cloud operations, as well as for organizations adopting OR have already adopted GitOps model. It is also possible organizations are following both modes of operations, which will be addressed by Kyndryl's operating model Kyndryl's target architecture for will address both models through a combination of Service Management tools, Cloud Native and Open Source Tools. Below picture shows the Automation Framework for Kyndryl, addressing both models. ITSM/Service Management based Automation : In this model, requests for additions and changes will be initiated through by selecting pre-defined Service Catalogs on ServiceNow tool (or equivalent ITSM tool) OR on a Cloud Management Portal(CMP) which will have backend workflows to obtain approvals and hooks onto automation tools like Ansible, Terraform & Cloud Natice tools to perform the automation. Workflows will be created to auto-approve standard changes. Note that all automation assets - ARM templates, Terraform templates, Ansible playbooks, scripts and other automation constructs will be source controlled in GitHub . GitOps Model : In this model, Developers &/or SRES define declarative templates specifying the desired configuration in IBM and/or Client's GitHub repositories and submit a pull request(PR). Once PR is approved in GitHub, Azure Pipelines OR Jenkins Pipelines can be initiated manually or automatically based on PRs, which then executes IaC templates on client subscriptions. Note that, in this model, it is also possible to link PRs or pipelines with ITSM workflows to approve change requests before the IaC templates are executed. 2.2.1 Automation Architecture Components \u2693\ufe0e Cloud Management Platform (CMP) \u2693\ufe0e Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and evaluate various custom/In-built Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities. Business Approval & IT Service Management \u2693\ufe0e There should be capability to raise and track service request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should be capability of raising Change Requests for operations requiring infrastructure changes. There should also exist an Enterprise grade Configuration Management Database ( CMDB ). An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of sequencing various tasks for make manage, agents installation, custom steps execution etc. This should also be able to handle Day 2 Change Request based service flow execution which can coordinate various tasks. Automation Engine \u2693\ufe0e Automation Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution. Resource Delegations / Azure LightHouse \u2693\ufe0e All automation activities can be performed on the client resources without establishing any connections by using Azure Light House which delegates client resources to be managed by Kyndryl (or any Service Provider/SP) onto the managing SP subscription. All Kyndryl managed clients will have Azure LightHouse delegations enabled. Monitoring & Management Tools \u2693\ufe0e Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc. GitHub \u2693\ufe0e Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well. 2.3 Migrate Automation \u2693\ufe0e Refer to CMM Tools addressing various aspects of migration to Azure","title":"Automation Architecture"},{"location":"automation/automation/automation-architecture/#2-cloud-automation-architecture","text":"This section explains how the above automation will be achieved for Kyndryl managed cloud platforms. Automation Platform and Architecture for Kyndryl will address Build, Migrate and Manage automation requirements. Initial focus of this document is focused on Build and Manage aspects of automation.","title":"2. Cloud Automation Architecture"},{"location":"automation/automation/automation-architecture/#21-automation-architecture-d4pc","text":"D4PC has defined an automation architecture focusing mostly on Landing Zone build/On-boarding aspect. D4PC uses various tools - Azure Native ARM Templates, Blueprints, Terraforms, Ansible playbooks, etc. for the automation. Refer below links for automation architecture defined by D4PC Offering : Automation Architecture: D4PC Automation architecture for Azure Component Model: D4PC Architecture - Component Model Operational Model: D4PC Architecture - Operational Model Functional Requirements for automation Non-Functional Requirements for automation","title":"2.1 Automation Architecture - D4PC"},{"location":"automation/automation/automation-architecture/#211-key-elements-of-d4pc-cloud-automation-architecture","text":"","title":"2.1.1 Key Elements of D4PC Cloud Automation Architecture"},{"location":"automation/automation/automation-architecture/#1iac-devops","text":"Infrastructure-as-code (IaC) is the foundation for build automations on Azure, via the Azure Resource Manager, well as Terraform & Ansible. IaC code will be source controlled in GitHub, and deployed using a combination Azure DevOps tools as well as external tools, as shown below: PIC to be Updated for Azure : Additional reference on using Azure pipelines for deploying IaC","title":"1.IaC &amp; DevOps"},{"location":"automation/automation/automation-architecture/#2templates","text":"Above architecture defines Standard vs Client Templates , Standard templates for most common application topologies/scenarios, whereas Client templates are application specific configurations. There are a few standard templates already published by PCID . GAP : However above Standard templates does not address all the platform elements** defined in Kyndryl's Landing Zone requirements. A new Standard Template should be created covering the platform elements: 1) LZ Common Services . 2) Policies for commonly used services: - VM Policies. - Storage Policies. - Network Policies . - AKS Policies. - ASB Policies (optional). - Additions as needed.","title":"2.TEMPLATES"},{"location":"automation/automation/automation-architecture/#3digital-intake","text":"There is an excel tool that prompts to provide a series of answers and then provides the most likely hierarchical structure for the landing zone (not tested by Tiger team)","title":"3.DIGITAL INTAKE"},{"location":"automation/automation/automation-architecture/#22-kyndryls-cloud-automation-architecture","text":"Kyndryl's Cloud Automation Architecture & platform will be common for Build & Manage. (Migration aspect will be reviewed later) Automation Framework for manage automation addresses operational & governance aspects once a client/application/workload is onboarded onto Azure cloud platform, after the first Landing Zone creation. Manage aspect also involves making changes to existing resources , as well as adding resources to existing landing zones. In an enterprise world, changes and additions to a production environment should be performed in a controlled but automated manner . Kyndryl's cloud automation solution will follow an Operating Model which will cater to organizations following traditional IT/ITSM based model for cloud operations, as well as for organizations adopting OR have already adopted GitOps model. It is also possible organizations are following both modes of operations, which will be addressed by Kyndryl's operating model Kyndryl's target architecture for will address both models through a combination of Service Management tools, Cloud Native and Open Source Tools. Below picture shows the Automation Framework for Kyndryl, addressing both models. ITSM/Service Management based Automation : In this model, requests for additions and changes will be initiated through by selecting pre-defined Service Catalogs on ServiceNow tool (or equivalent ITSM tool) OR on a Cloud Management Portal(CMP) which will have backend workflows to obtain approvals and hooks onto automation tools like Ansible, Terraform & Cloud Natice tools to perform the automation. Workflows will be created to auto-approve standard changes. Note that all automation assets - ARM templates, Terraform templates, Ansible playbooks, scripts and other automation constructs will be source controlled in GitHub . GitOps Model : In this model, Developers &/or SRES define declarative templates specifying the desired configuration in IBM and/or Client's GitHub repositories and submit a pull request(PR). Once PR is approved in GitHub, Azure Pipelines OR Jenkins Pipelines can be initiated manually or automatically based on PRs, which then executes IaC templates on client subscriptions. Note that, in this model, it is also possible to link PRs or pipelines with ITSM workflows to approve change requests before the IaC templates are executed.","title":"2.2 Kyndryl's Cloud Automation Architecture"},{"location":"automation/automation/automation-architecture/#221-automation-architecture-components","text":"","title":"2.2.1 Automation Architecture Components"},{"location":"automation/automation/automation-architecture/#cloud-management-platform-cmp","text":"Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and evaluate various custom/In-built Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities.","title":"Cloud Management Platform (CMP)"},{"location":"automation/automation/automation-architecture/#business-approval-it-service-management","text":"There should be capability to raise and track service request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should be capability of raising Change Requests for operations requiring infrastructure changes. There should also exist an Enterprise grade Configuration Management Database ( CMDB ). An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of sequencing various tasks for make manage, agents installation, custom steps execution etc. This should also be able to handle Day 2 Change Request based service flow execution which can coordinate various tasks.","title":"Business Approval &amp; IT Service Management"},{"location":"automation/automation/automation-architecture/#automation-engine","text":"Automation Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution.","title":"Automation Engine"},{"location":"automation/automation/automation-architecture/#resource-delegations-azure-lighthouse","text":"All automation activities can be performed on the client resources without establishing any connections by using Azure Light House which delegates client resources to be managed by Kyndryl (or any Service Provider/SP) onto the managing SP subscription. All Kyndryl managed clients will have Azure LightHouse delegations enabled.","title":"Resource Delegations / Azure LightHouse"},{"location":"automation/automation/automation-architecture/#monitoring-management-tools","text":"Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc.","title":"Monitoring &amp; Management Tools"},{"location":"automation/automation/automation-architecture/#github","text":"Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well.","title":"GitHub"},{"location":"automation/automation/automation-architecture/#23-migrate-automation","text":"Refer to CMM Tools addressing various aspects of migration to Azure","title":"2.3 Migrate Automation"},{"location":"automation/automation/automation-framework-copy/","text":"1. Cloud Automation Framework \u2693\ufe0e While cloud provides automated on-demand services, the process of onboarding a client onto a cloud platform requires the creation of an enterprise grade landing zone which includes various elements from network, security, operational and management aspects as well as setting the foundations for day two operations. This is in addition to creating application/workload specific elements running on VMs or containers and integrating with various PaaS elements. The purpose of this document is to identify the elements of cloud automation (What) and the automation approach and framework (How) that will be used to achieve the automations for Kyndryl managed clients . In order to address automation across all stages in a client's cloud adoption journey, the framework for cloud automation is classified into 3 areas: Build/On-boarding Automation Migrate Automation Manage Automation Some of the elements within each of these areas will overlap. 1.1 Build/On-Boarding Automation \u2693\ufe0e Build Automation comprises the foundation required for onboarding an account or client onto a cloud platform, which is the Landing Zone , as well as automation for onboarding subsequent applications & workloads. The foundation for initial onboarding involves setting up Network topology (Hub-Spoke/VWan), management and governance elements including monitoring, setting up common policies including security compliance, which is all included as part of the initial Landing Zone creation. Some of the elements of build automation are:. \u00b7 Resource Group Automation (Create, Delete). \u00b7 Resource Tagging Automation. \u00b7 Identity & Access Management (Azure AD User, Group and Roles Assignment \u00b7 Network Automation. \u00b7 Security & Compliance Automation. \u00b7 Monitor and Log Analytics Deployment Automation. \u00b7 Policy Automation. \u00b7 Landing Zone Deployment Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Workload Deployment Automation. 1.2 Migrate Automation \u2693\ufe0e Migration is a critical aspect of cloud adoption journey, and automations should address at least the below aspects of migration: \u00b7 Discovery. \u00b7 Workload Migration. \u00b7 Workload Deployment. \u00b7 Make Manage Onboarding. Guidance for migration automation is included as part of Cloud Migration & Modernization(CMM) program. 1.3 Manage Automation \u2693\ufe0e Manage Automation addresses the day two operational aspects after a client/application/workload is onboarded to the cloud platform. The focus of manage automation is to ensure that resources are in compliance with the defined organizational governance policies covering Monitoring, Patching/Updates, Security, Costing/Billing etc. Manage automation should cover atleast the following elements: \u00b7 Update Management(Patching). \u00b7 Configuration Management. \u00b7 Reduce manual operational effort. \u00b7 Resource Consistency. \u00b7 Cost & Billing Automation. \u00b7 Security & Compliance Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Event Based Orchestration. \u00b7 Adding or removing resources to an existing landing zone.","title":"Automation framework copy"},{"location":"automation/automation/automation-framework-copy/#1-cloud-automation-framework","text":"While cloud provides automated on-demand services, the process of onboarding a client onto a cloud platform requires the creation of an enterprise grade landing zone which includes various elements from network, security, operational and management aspects as well as setting the foundations for day two operations. This is in addition to creating application/workload specific elements running on VMs or containers and integrating with various PaaS elements. The purpose of this document is to identify the elements of cloud automation (What) and the automation approach and framework (How) that will be used to achieve the automations for Kyndryl managed clients . In order to address automation across all stages in a client's cloud adoption journey, the framework for cloud automation is classified into 3 areas: Build/On-boarding Automation Migrate Automation Manage Automation Some of the elements within each of these areas will overlap.","title":"1. Cloud Automation Framework"},{"location":"automation/automation/automation-framework-copy/#11-buildon-boarding-automation","text":"Build Automation comprises the foundation required for onboarding an account or client onto a cloud platform, which is the Landing Zone , as well as automation for onboarding subsequent applications & workloads. The foundation for initial onboarding involves setting up Network topology (Hub-Spoke/VWan), management and governance elements including monitoring, setting up common policies including security compliance, which is all included as part of the initial Landing Zone creation. Some of the elements of build automation are:. \u00b7 Resource Group Automation (Create, Delete). \u00b7 Resource Tagging Automation. \u00b7 Identity & Access Management (Azure AD User, Group and Roles Assignment \u00b7 Network Automation. \u00b7 Security & Compliance Automation. \u00b7 Monitor and Log Analytics Deployment Automation. \u00b7 Policy Automation. \u00b7 Landing Zone Deployment Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Workload Deployment Automation.","title":"1.1  Build/On-Boarding Automation"},{"location":"automation/automation/automation-framework-copy/#12-migrate-automation","text":"Migration is a critical aspect of cloud adoption journey, and automations should address at least the below aspects of migration: \u00b7 Discovery. \u00b7 Workload Migration. \u00b7 Workload Deployment. \u00b7 Make Manage Onboarding. Guidance for migration automation is included as part of Cloud Migration & Modernization(CMM) program.","title":"1.2  Migrate Automation"},{"location":"automation/automation/automation-framework-copy/#13-manage-automation","text":"Manage Automation addresses the day two operational aspects after a client/application/workload is onboarded to the cloud platform. The focus of manage automation is to ensure that resources are in compliance with the defined organizational governance policies covering Monitoring, Patching/Updates, Security, Costing/Billing etc. Manage automation should cover atleast the following elements: \u00b7 Update Management(Patching). \u00b7 Configuration Management. \u00b7 Reduce manual operational effort. \u00b7 Resource Consistency. \u00b7 Cost & Billing Automation. \u00b7 Security & Compliance Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Event Based Orchestration. \u00b7 Adding or removing resources to an existing landing zone.","title":"1.3  Manage Automation"},{"location":"automation/automation/automation-usecases/","text":"1. Cloud Automation Use Cases \u2693\ufe0e While cloud provides automated on-demand services, the process of onboarding a client onto a cloud platform requires the creation of an enterprise grade landing zone which includes various elements from network, security, operational and management aspects as well as setting the foundations for day two operations. This is in addition to creating application/workload specific elements running on VMs or containers and integrating with various PaaS elements. The purpose of this document is to identify the elements of cloud automation (What) and the automation framework & architecture that will be used**(How)** to achieve automations for Kyndryl managed clients . In order to address automation across all stages in a client's cloud adoption journey, requirements for automation can be classified into three broad use cases or areas: Build/On-boarding Automation Migrate Automation Manage Automation Some of the elements within each of these areas will overlap. 1.1 Build/On-Boarding Automation \u2693\ufe0e Build Automation comprises the foundation required for onboarding an account or client onto a cloud platform, which is the Landing Zone , as well as automation for onboarding subsequent applications & workloads. The foundation for initial onboarding involves setting up Network topology (Hub-Spoke/VWan), management and governance elements including monitoring, setting up common policies including security compliance, which is all included as part of the initial Landing Zone creation. Some of the elements of build automation are:. \u00b7 Landing Zone Creation. \u00b7 Policies for resource tagging & Naming standards. \u00b7 Monitor and Log Analytics Platform Enablement. \u00b7 Policies for enforcing Governance & Management (Monitoring, Updates, ..) \u00b7 Policies for Common Resources \u00b7 Automation to create Azure AD IAM Groups, Role Definitions for Kyndryl Operations \u00b7 Identity & Access Management (Azure AD User, Group and Roles Assignment \u00b7 Security & Compliance Automation. \u00b7 Workload Deployment Automation. As listed above build/On-boarding automation addresses requirements/recommendations from Landing Zone design, as well as application/workload specific requirements. Landing Zone automation itself can be categorized as 2 parts: 1) LZ Common Service(OR Shared) element Automation: Recommendations that apply to all/most common Azure services, and services which are managed by Kyndryl. These include, for example, Monitoring, Updates, Security, Backups etc, Governance policies, which can be applied to all/any workloads. 2) LZ Application specific elements: These are specific to application workloads - creation of VNets, Subnets, Web Application Firewalls, NSGs, VMs, AKS, DBs & other Azure services As an example, for Enterprise Scale Hub-Spoke Architecture , automation can be seperated into above classifications as shown below: The common service element automation, with configuration parameters, will be standard ( Common Services/Standard Templates ) and reusable for most deployments, whereas the client's application specific landing zone aspects ( Client Templates ) should be developed uniquely for each client. 1.2 Migrate Automation \u2693\ufe0e Migration is a critical aspect of cloud adoption journey, and automations should address at least the below aspects of migration: \u00b7 Discovery. \u00b7 Workload Migration. \u00b7 Workload Deployment. \u00b7 Make Manage Onboarding. Guidance for migration automation is included as part of Cloud Migration & Modernization(CMM) program. 1.3 Manage Automation \u2693\ufe0e Manage Automation addresses the day two operational aspects after a client/application/workload is onboarded to the cloud platform. The focus of manage automation is to ensure that resources are in compliance with the defined organizational governance policies covering Monitoring, Patching/Updates, Security, Costing/Billing etc. Manage automation should cover atleast the following elements: \u00b7 Update Management(Patching). \u00b7 Configuration Management. \u00b7 Reduce manual operational effort. \u00b7 Resource Consistency. \u00b7 Cost & Billing Automation. \u00b7 Security & Compliance Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Event Based Orchestration. \u00b7 Adding or removing resources to an existing landing zone.","title":"Automation UseCases"},{"location":"automation/automation/automation-usecases/#1-cloud-automation-use-cases","text":"While cloud provides automated on-demand services, the process of onboarding a client onto a cloud platform requires the creation of an enterprise grade landing zone which includes various elements from network, security, operational and management aspects as well as setting the foundations for day two operations. This is in addition to creating application/workload specific elements running on VMs or containers and integrating with various PaaS elements. The purpose of this document is to identify the elements of cloud automation (What) and the automation framework & architecture that will be used**(How)** to achieve automations for Kyndryl managed clients . In order to address automation across all stages in a client's cloud adoption journey, requirements for automation can be classified into three broad use cases or areas: Build/On-boarding Automation Migrate Automation Manage Automation Some of the elements within each of these areas will overlap.","title":"1. Cloud Automation Use Cases"},{"location":"automation/automation/automation-usecases/#11-buildon-boarding-automation","text":"Build Automation comprises the foundation required for onboarding an account or client onto a cloud platform, which is the Landing Zone , as well as automation for onboarding subsequent applications & workloads. The foundation for initial onboarding involves setting up Network topology (Hub-Spoke/VWan), management and governance elements including monitoring, setting up common policies including security compliance, which is all included as part of the initial Landing Zone creation. Some of the elements of build automation are:. \u00b7 Landing Zone Creation. \u00b7 Policies for resource tagging & Naming standards. \u00b7 Monitor and Log Analytics Platform Enablement. \u00b7 Policies for enforcing Governance & Management (Monitoring, Updates, ..) \u00b7 Policies for Common Resources \u00b7 Automation to create Azure AD IAM Groups, Role Definitions for Kyndryl Operations \u00b7 Identity & Access Management (Azure AD User, Group and Roles Assignment \u00b7 Security & Compliance Automation. \u00b7 Workload Deployment Automation. As listed above build/On-boarding automation addresses requirements/recommendations from Landing Zone design, as well as application/workload specific requirements. Landing Zone automation itself can be categorized as 2 parts: 1) LZ Common Service(OR Shared) element Automation: Recommendations that apply to all/most common Azure services, and services which are managed by Kyndryl. These include, for example, Monitoring, Updates, Security, Backups etc, Governance policies, which can be applied to all/any workloads. 2) LZ Application specific elements: These are specific to application workloads - creation of VNets, Subnets, Web Application Firewalls, NSGs, VMs, AKS, DBs & other Azure services As an example, for Enterprise Scale Hub-Spoke Architecture , automation can be seperated into above classifications as shown below: The common service element automation, with configuration parameters, will be standard ( Common Services/Standard Templates ) and reusable for most deployments, whereas the client's application specific landing zone aspects ( Client Templates ) should be developed uniquely for each client.","title":"1.1  Build/On-Boarding Automation"},{"location":"automation/automation/automation-usecases/#12-migrate-automation","text":"Migration is a critical aspect of cloud adoption journey, and automations should address at least the below aspects of migration: \u00b7 Discovery. \u00b7 Workload Migration. \u00b7 Workload Deployment. \u00b7 Make Manage Onboarding. Guidance for migration automation is included as part of Cloud Migration & Modernization(CMM) program.","title":"1.2  Migrate Automation"},{"location":"automation/automation/automation-usecases/#13-manage-automation","text":"Manage Automation addresses the day two operational aspects after a client/application/workload is onboarded to the cloud platform. The focus of manage automation is to ensure that resources are in compliance with the defined organizational governance policies covering Monitoring, Patching/Updates, Security, Costing/Billing etc. Manage automation should cover atleast the following elements: \u00b7 Update Management(Patching). \u00b7 Configuration Management. \u00b7 Reduce manual operational effort. \u00b7 Resource Consistency. \u00b7 Cost & Billing Automation. \u00b7 Security & Compliance Automation. \u00b7 Policy Enforcement and Automation. \u00b7 Event Based Orchestration. \u00b7 Adding or removing resources to an existing landing zone.","title":"1.3  Manage Automation"},{"location":"automation/automation/azure-automationoptions/","text":"3. Azure Automation Options \u2693\ufe0e Azure provides ways to create, configure and manage various Azure services via Azure Portal, REST API, CLI and PowerShell. There are various Hybrid tools, Azure services and automation options as below which can be leveraged for automation. 3.1 Infrastructure as Code (IaC) \u2693\ufe0e IaC is about defining and maintaining infrastructure (Virtual Machines, Network, Storage, Load Balancer etc.) in descriptive model, which can be maintained and versioned by DevOps team as source code. IaC usually contains input configuration file (target state), variables and logic (resource configuration) for resource deployment based on the given configuration. Azure provides native support for IaC via PowerShell and Azure Resource Manager (ARM), and this is also supported by popular third party platforms, such as Terraform, Ansible, and Chef to deploy and manage infrastructure in an automated fashion. 3.2 Azure Policies \u2693\ufe0e Azure Policy provides alerting, denial, and remediation capabilities for the Azure resources for a chosen Azure scope. It helps to evaluate overall state of the environment, drill down to each policy/resource, enforce organizational standards, assess compliance at-scale and bulk remediation for existing resources and automatic remediation for new resources. Common use cases for Azure Policy include implementing governance for resource consistency, regulatory compliance, security, cost visibility etc. Azure policies can be grouped together with Azure Initiatives for easy apply and controls. A new Azure policy or initiative assignment takes about 30 minutes to be applied while New or updated resources within scope of an existing assignment become available in about 15 minutes. A standard compliance scan occurs every 24 hours. For Kyndryl recommended Azure built-in & custom policies, and Azure Initiatives please refer link . 3.3 Azure Blueprint \u2693\ufe0e Azure Blueprint are reusable blueprints consisting of packages of key environment artifacts like Azure Resource Manager templates, role-based access controls and policies simplifying largescale Azure deployments. These templates can be maintained at centralized place with version controlled and can be leveraged for deployment to multiple subscriptions with single click. For deploying the chosen Azure Blueprint, below steps should be followed: \u00b7 Create a new blueprint from the sample/scratch \u00b7 Mark the blueprint as Published \u00b7 Assign the blueprint to an existing subscription 3.4 Azure Automation \u2693\ufe0e Azure Automation is a cloud-based automation and configuration service which can be leveraged for Azure and non-Azure environments. With Azure Automation, runbooks can be authored graphically, in PowerShell, or using Python and can be used for process automation, configuration management, update management, shared capabilities, and heterogeneous features and can be used during deployment, operations, and decommissioning of workloads and resources. Azure automation has changed over time from classic, Azure Service Management (ASM) model to recent, Azure Resource Manager (ARM) model. ARM templates additionally provides resource groups, role-based access control, template deployments, tagging, resource policy etc. capabilities. Along with Azure Graphical runbooks option, ARM templates provides capability to create, maintain and deploy JSON based templates in version control manner. Templates helps in deploying resources consistently and repeatedly. Azure Automation needs an automation account which can integrate with Operations Management Suite (OMS), and the solutions connected to it. Alternately, webhooks can be created for runbooks and can be executed based on OMS search criteria. Azure Automation can also execute runbooks in on-premises environment through on-prem Azure automation hybrid workers, connected with the Azure Automation account. 3.5 PowerShell \u2693\ufe0e Azure PowerShell is a set of commandlets for managing Azure resources directly from the PowerShell command line and provides powerful features for automation. There are number of readily available PowerShell modules as well which can be imported into the Automation account to run the runbooks from PowerShell Module Gallery. If runbook uses any of these commands, the respective modules should be imported to the Automation account before executing the runbook The runbooks in Azure automation are completely based on PowerShell. There are below four types of runbooks available: \u00b7 PowerShell \u2013 PowerShell based runbooks are available in Azure Automation which can do basic operations. These are similar as executing Azure PowerShell module-based commands from the Azure portal. \u00b7 PowerShell Workflow - PowerShell Workflow can be used for advanced automations as it can execute more-complex tasks that involve executing steps in parallel, calling other child runbooks, and so forth. It in turn uses Windows Workflow Foundation and allows to set checkpoints in script so that script can be restarted from the checkpoint if an exception occurs during execution. \u00b7 Graphical : Graphical runbooks can be created only from Azure Portal and is good for administrators with no/less PowerShell knowledge. This type of runbook uses a visual authoring model and represents the data flow pictorially in an easy-to-understand fashion. \u00b7 Graphical PowerShell Workflow : Graphical PowerShell Workflow runbooks are based on PowerShell workflows in the back end and can be created and edited only from Azure Portal. Though based on PowerShell, each runbook type has its own features and limitations. Nested run books can be executed. 3.6 Terraform \u2693\ufe0e Terraform is an open-source tool, and provides configuration as code for cloud infrastructure provisioning and management. The Terraform CLI provides a simple mechanism to deploy and version the configuration files to Azure. Common Azure compute, storage, database and network activities can be performed using Terraform templates. 3.7 Runbooks \u2693\ufe0e Azure Runbooks are the basic building blocks of Azure Automation and can be created from scratch, or can be imported from Runbook Gallery which are published by Microsoft or community contributors. These runbooks can be customized and scheduled as needed. Few examples of common runbooks are start/stop all Azure VMs or tagged VMs, turnOn Update Management, backup SQL db to Azure Blob, Send Mail, Backup reports etc. Runbooks can be executed manually or in a scheduled fashion, that results in one or multiple jobs which can run in parallel. An Azure automation worker executes the job(s). Each Job can be managed and drilled down deeper and Job\u2019s input, output and task information can be tracked. Jobs can have status as Completed, Failed, Queued, Running, Stopped, and Suspended. If a runbook is interrupted, it restarts at the beginning. 3.8 Desired State Configuration (DSC) \u2693\ufe0e Azure Desired State Configuration ( DSC ) is an configuration management solution, that helps in maintaining infrastructure configuration as code. DSC is a feature in PowerShell 4.0 and above that helps administrators to automate the configuration of Windows and Linux operating systems. It uses PowerShell and implements the desired state in target machines by leveraging the Local Configuration Manager (LCM). Azure Automation DSC integrates Azure Automation with DSC-based configuration management and can be used to maintain desired state across on-premises physical/virtual machines as well as cloud resources. 3.9 Azure Automanage \u2693\ufe0e Azure provides Azure Automanage feature for Linux and Windows Servers Day2 tasks management in dev/test and Production category based on Azure best practices. This feature is currently in preview. Various Day2 common tasks are as below: \u00b7 Machine Insights Monitoring (Production only) \u00b7 Backup (Production only) \u00b7 Azure Security Center \u00b7 Microsoft Antimalware \u00b7 Update Management \u00b7 Change Tracking and Inventory \u00b7 Guest Configuration \u00b7 Boot Diagnostics \u00b7 Windows Admin Center \u00b7 Azure Automation Account \u00b7 Log Analytics Workspace Built-in Azure Automanage enrols, configures, and monitors virtual machines with best practice as defined in the Microsoft Cloud Adoption Framework for Azure. Below are few current limitations for same. \u00b7 Automanage can be used for the selected scope. \u00b7 By default, this assignment takes effect on newly created resources. \u00b7 Existing resources can be updated via a remediation task after the policy is assigned. For deployIfNotExists policies, the remediation task deploys the specified template. \u00b7 For modify policies, the remediation task edits tags on the existing resources. \u00b7 Up to 500 non-complaint resources can be made complaint 3.10 Other Azure Services \u2693\ufe0e Azure provides various other services to manage and control Azure environment in automated fashion as below: \u00b7 Azure Backup for protecting data and backing up entire Windows/Linux VMs using backup extensions and backing up files, folders, and system state using the MARS agent \u00b7 Azure Security Center with Azure Defender for monitoring workloads and finding and fixing vulnerabilities to protect hybrid cloud workloads \u00b7 Azure Advisor analyses Azure services configurations and usage telemetry and offers personalized, actionable recommendations for reliability, security, operational aspects, performance, and cost. This also includes suggested actions that can be taken right away, postpone or dismiss. Advisor Quick Fix makes optimization at scale faster and easier by allowing users to remediate recommendations for multiple resources simultaneously and with only a few clicks. Same can be accessed by Azure Portal, REST API, CLI and PowerShell. 3.11 Non-Native Tools: Ansible & CACF \u2693\ufe0e Ansible is an open source provisioning and configuration management tool and can automate complex applications and provision resources in multiple clouds. Ansible provides collection of Az modules for various Azure activities automation. Ansible Tower Based, Cloud Automation Community Framework ( CACF ) is Kyndryl\u2019s strategic solution that allows automation at enterprise level with a target to get to zero touch with reduced development and release cycle addressed by the community model. CACF leverages cloud native RedHat Automation platform \u2013 Ansible Tower on OpenShift and integrates with Services management processes, the Git repository and the automation playbooks. There is a rich set of are pre-built automations available using Ansible playbooks. Major CACF addressed use cases (on-prem) for existing clients are as below: \u00b7 Event/Incident remediation \u00b7 Patch Scanning and execution \u00b7 Security Health Check \u00b7 Service Request \u00b7 Build and Decommission \u00b7 Other Automation use cases","title":"Azure Automation Options"},{"location":"automation/automation/azure-automationoptions/#3-azure-automation-options","text":"Azure provides ways to create, configure and manage various Azure services via Azure Portal, REST API, CLI and PowerShell. There are various Hybrid tools, Azure services and automation options as below which can be leveraged for automation.","title":"3. Azure Automation Options"},{"location":"automation/automation/azure-automationoptions/#31-infrastructure-as-code-iac","text":"IaC is about defining and maintaining infrastructure (Virtual Machines, Network, Storage, Load Balancer etc.) in descriptive model, which can be maintained and versioned by DevOps team as source code. IaC usually contains input configuration file (target state), variables and logic (resource configuration) for resource deployment based on the given configuration. Azure provides native support for IaC via PowerShell and Azure Resource Manager (ARM), and this is also supported by popular third party platforms, such as Terraform, Ansible, and Chef to deploy and manage infrastructure in an automated fashion.","title":"3.1  Infrastructure as Code (IaC)"},{"location":"automation/automation/azure-automationoptions/#32-azure-policies","text":"Azure Policy provides alerting, denial, and remediation capabilities for the Azure resources for a chosen Azure scope. It helps to evaluate overall state of the environment, drill down to each policy/resource, enforce organizational standards, assess compliance at-scale and bulk remediation for existing resources and automatic remediation for new resources. Common use cases for Azure Policy include implementing governance for resource consistency, regulatory compliance, security, cost visibility etc. Azure policies can be grouped together with Azure Initiatives for easy apply and controls. A new Azure policy or initiative assignment takes about 30 minutes to be applied while New or updated resources within scope of an existing assignment become available in about 15 minutes. A standard compliance scan occurs every 24 hours. For Kyndryl recommended Azure built-in & custom policies, and Azure Initiatives please refer link .","title":"3.2  Azure Policies"},{"location":"automation/automation/azure-automationoptions/#33-azure-blueprint","text":"Azure Blueprint are reusable blueprints consisting of packages of key environment artifacts like Azure Resource Manager templates, role-based access controls and policies simplifying largescale Azure deployments. These templates can be maintained at centralized place with version controlled and can be leveraged for deployment to multiple subscriptions with single click. For deploying the chosen Azure Blueprint, below steps should be followed: \u00b7 Create a new blueprint from the sample/scratch \u00b7 Mark the blueprint as Published \u00b7 Assign the blueprint to an existing subscription","title":"3.3  Azure Blueprint"},{"location":"automation/automation/azure-automationoptions/#34-azure-automation","text":"Azure Automation is a cloud-based automation and configuration service which can be leveraged for Azure and non-Azure environments. With Azure Automation, runbooks can be authored graphically, in PowerShell, or using Python and can be used for process automation, configuration management, update management, shared capabilities, and heterogeneous features and can be used during deployment, operations, and decommissioning of workloads and resources. Azure automation has changed over time from classic, Azure Service Management (ASM) model to recent, Azure Resource Manager (ARM) model. ARM templates additionally provides resource groups, role-based access control, template deployments, tagging, resource policy etc. capabilities. Along with Azure Graphical runbooks option, ARM templates provides capability to create, maintain and deploy JSON based templates in version control manner. Templates helps in deploying resources consistently and repeatedly. Azure Automation needs an automation account which can integrate with Operations Management Suite (OMS), and the solutions connected to it. Alternately, webhooks can be created for runbooks and can be executed based on OMS search criteria. Azure Automation can also execute runbooks in on-premises environment through on-prem Azure automation hybrid workers, connected with the Azure Automation account.","title":"3.4  Azure Automation"},{"location":"automation/automation/azure-automationoptions/#35-powershell","text":"Azure PowerShell is a set of commandlets for managing Azure resources directly from the PowerShell command line and provides powerful features for automation. There are number of readily available PowerShell modules as well which can be imported into the Automation account to run the runbooks from PowerShell Module Gallery. If runbook uses any of these commands, the respective modules should be imported to the Automation account before executing the runbook The runbooks in Azure automation are completely based on PowerShell. There are below four types of runbooks available: \u00b7 PowerShell \u2013 PowerShell based runbooks are available in Azure Automation which can do basic operations. These are similar as executing Azure PowerShell module-based commands from the Azure portal. \u00b7 PowerShell Workflow - PowerShell Workflow can be used for advanced automations as it can execute more-complex tasks that involve executing steps in parallel, calling other child runbooks, and so forth. It in turn uses Windows Workflow Foundation and allows to set checkpoints in script so that script can be restarted from the checkpoint if an exception occurs during execution. \u00b7 Graphical : Graphical runbooks can be created only from Azure Portal and is good for administrators with no/less PowerShell knowledge. This type of runbook uses a visual authoring model and represents the data flow pictorially in an easy-to-understand fashion. \u00b7 Graphical PowerShell Workflow : Graphical PowerShell Workflow runbooks are based on PowerShell workflows in the back end and can be created and edited only from Azure Portal. Though based on PowerShell, each runbook type has its own features and limitations. Nested run books can be executed.","title":"3.5  PowerShell"},{"location":"automation/automation/azure-automationoptions/#36-terraform","text":"Terraform is an open-source tool, and provides configuration as code for cloud infrastructure provisioning and management. The Terraform CLI provides a simple mechanism to deploy and version the configuration files to Azure. Common Azure compute, storage, database and network activities can be performed using Terraform templates.","title":"3.6  Terraform"},{"location":"automation/automation/azure-automationoptions/#37-runbooks","text":"Azure Runbooks are the basic building blocks of Azure Automation and can be created from scratch, or can be imported from Runbook Gallery which are published by Microsoft or community contributors. These runbooks can be customized and scheduled as needed. Few examples of common runbooks are start/stop all Azure VMs or tagged VMs, turnOn Update Management, backup SQL db to Azure Blob, Send Mail, Backup reports etc. Runbooks can be executed manually or in a scheduled fashion, that results in one or multiple jobs which can run in parallel. An Azure automation worker executes the job(s). Each Job can be managed and drilled down deeper and Job\u2019s input, output and task information can be tracked. Jobs can have status as Completed, Failed, Queued, Running, Stopped, and Suspended. If a runbook is interrupted, it restarts at the beginning.","title":"3.7  Runbooks"},{"location":"automation/automation/azure-automationoptions/#38-desired-state-configuration-dsc","text":"Azure Desired State Configuration ( DSC ) is an configuration management solution, that helps in maintaining infrastructure configuration as code. DSC is a feature in PowerShell 4.0 and above that helps administrators to automate the configuration of Windows and Linux operating systems. It uses PowerShell and implements the desired state in target machines by leveraging the Local Configuration Manager (LCM). Azure Automation DSC integrates Azure Automation with DSC-based configuration management and can be used to maintain desired state across on-premises physical/virtual machines as well as cloud resources.","title":"3.8  Desired State Configuration (DSC)"},{"location":"automation/automation/azure-automationoptions/#39-azure-automanage","text":"Azure provides Azure Automanage feature for Linux and Windows Servers Day2 tasks management in dev/test and Production category based on Azure best practices. This feature is currently in preview. Various Day2 common tasks are as below: \u00b7 Machine Insights Monitoring (Production only) \u00b7 Backup (Production only) \u00b7 Azure Security Center \u00b7 Microsoft Antimalware \u00b7 Update Management \u00b7 Change Tracking and Inventory \u00b7 Guest Configuration \u00b7 Boot Diagnostics \u00b7 Windows Admin Center \u00b7 Azure Automation Account \u00b7 Log Analytics Workspace Built-in Azure Automanage enrols, configures, and monitors virtual machines with best practice as defined in the Microsoft Cloud Adoption Framework for Azure. Below are few current limitations for same. \u00b7 Automanage can be used for the selected scope. \u00b7 By default, this assignment takes effect on newly created resources. \u00b7 Existing resources can be updated via a remediation task after the policy is assigned. For deployIfNotExists policies, the remediation task deploys the specified template. \u00b7 For modify policies, the remediation task edits tags on the existing resources. \u00b7 Up to 500 non-complaint resources can be made complaint","title":"3.9  Azure Automanage"},{"location":"automation/automation/azure-automationoptions/#310-other-azure-services","text":"Azure provides various other services to manage and control Azure environment in automated fashion as below: \u00b7 Azure Backup for protecting data and backing up entire Windows/Linux VMs using backup extensions and backing up files, folders, and system state using the MARS agent \u00b7 Azure Security Center with Azure Defender for monitoring workloads and finding and fixing vulnerabilities to protect hybrid cloud workloads \u00b7 Azure Advisor analyses Azure services configurations and usage telemetry and offers personalized, actionable recommendations for reliability, security, operational aspects, performance, and cost. This also includes suggested actions that can be taken right away, postpone or dismiss. Advisor Quick Fix makes optimization at scale faster and easier by allowing users to remediate recommendations for multiple resources simultaneously and with only a few clicks. Same can be accessed by Azure Portal, REST API, CLI and PowerShell.","title":"3.10   Other Azure Services"},{"location":"automation/automation/azure-automationoptions/#311-non-native-tools-ansible-cacf","text":"Ansible is an open source provisioning and configuration management tool and can automate complex applications and provision resources in multiple clouds. Ansible provides collection of Az modules for various Azure activities automation. Ansible Tower Based, Cloud Automation Community Framework ( CACF ) is Kyndryl\u2019s strategic solution that allows automation at enterprise level with a target to get to zero touch with reduced development and release cycle addressed by the community model. CACF leverages cloud native RedHat Automation platform \u2013 Ansible Tower on OpenShift and integrates with Services management processes, the Git repository and the automation playbooks. There is a rich set of are pre-built automations available using Ansible playbooks. Major CACF addressed use cases (on-prem) for existing clients are as below: \u00b7 Event/Incident remediation \u00b7 Patch Scanning and execution \u00b7 Security Health Check \u00b7 Service Request \u00b7 Build and Decommission \u00b7 Other Automation use cases","title":"3.11 Non-Native Tools: Ansible &amp; CACF"},{"location":"automation/automation/lz-policies/","text":"Azure Policies \u2693\ufe0e This section explains all standard policies that will be enabled by default through automation, for accounts managed by Kyndryl. For management & operational purposes, these policies are categorized as below: 1) Landing Zone/Platform policies : These are policies which enforces recommendations from landing zone design. These cover the foundational aspects of Azure platform and not specific to any service, and will be enabled as part of account onboarding. This will include in-built & custom policies 2)Azure Service specific policies : These cover specific services like- * Compute * Network * Storage * Containers * Key Vault (move under 1#?) * Databases - should be a sub category under #3 covering various DBs * Messaging - Same as above Intention is to enable these generic policies as part of Landing Zone Automation, which will then be automatically applied against new resources created in a managed subscription. NOTE : For #1 & #2 , any policy which is covered by CIS or ASB will not be inclded. 3) Security Policies 3.1)CIS Benchmark Policies : These policies will be enabled by default without any modification. However, it will be ensured that service specific policies that overlap or duplicate CIS policies will not be enabled to avoid duplicate actions. Need to investiage if other compliances(Fedramp, Hipaa) are independent or above CIS 3.2) Azure Security Benchmark(ASB) policies : ASB is in addition to CIS, containing security validations specific to Azure services, bringing value-add to Kyndryl and clients. However, these will be OPTIONALLY enabled, based on client needs as well as delivery team's (MCMS, ISPwW) scope. What will the Policy Standards/Recommendations contain : (To be followed for #1 & #2 ) Policy Name & Link Purpose/Short description, if needed Type: Built-In or Custom Enablement: Mandatory/Optional Mode: Audit/Deny/Disable/Enable Env: Dev/Prod/Test/All Assignments: Subscription/Mgmt Group/Resource Group/specific resources Parameters: External parameters (like Log Analytics workspace for an/each account) that need to be applied to the policy Default Actions: Notification/ LogWorkspace/ Security Center etc...","title":"Azure Policies"},{"location":"automation/automation/lz-policies/#azure-policies","text":"This section explains all standard policies that will be enabled by default through automation, for accounts managed by Kyndryl. For management & operational purposes, these policies are categorized as below: 1) Landing Zone/Platform policies : These are policies which enforces recommendations from landing zone design. These cover the foundational aspects of Azure platform and not specific to any service, and will be enabled as part of account onboarding. This will include in-built & custom policies 2)Azure Service specific policies : These cover specific services like- * Compute * Network * Storage * Containers * Key Vault (move under 1#?) * Databases - should be a sub category under #3 covering various DBs * Messaging - Same as above Intention is to enable these generic policies as part of Landing Zone Automation, which will then be automatically applied against new resources created in a managed subscription. NOTE : For #1 & #2 , any policy which is covered by CIS or ASB will not be inclded. 3) Security Policies 3.1)CIS Benchmark Policies : These policies will be enabled by default without any modification. However, it will be ensured that service specific policies that overlap or duplicate CIS policies will not be enabled to avoid duplicate actions. Need to investiage if other compliances(Fedramp, Hipaa) are independent or above CIS 3.2) Azure Security Benchmark(ASB) policies : ASB is in addition to CIS, containing security validations specific to Azure services, bringing value-add to Kyndryl and clients. However, these will be OPTIONALLY enabled, based on client needs as well as delivery team's (MCMS, ISPwW) scope. What will the Policy Standards/Recommendations contain : (To be followed for #1 & #2 ) Policy Name & Link Purpose/Short description, if needed Type: Built-In or Custom Enablement: Mandatory/Optional Mode: Audit/Deny/Disable/Enable Env: Dev/Prod/Test/All Assignments: Subscription/Mgmt Group/Resource Group/specific resources Parameters: External parameters (like Log Analytics workspace for an/each account) that need to be applied to the policy Default Actions: Notification/ LogWorkspace/ Security Center etc...","title":"Azure Policies"},{"location":"automation/automation/makemanage/","text":"For Client\u2019s green field and/or brown field workload management, Kyndryl executes make manage process for cloud workloads onboarding. This make manage process executes various tasks as below: Azure native services and tools can be used for various activities as below. SR No Make Manage Process Available Azure Services/Tools 1. Monitoring Azure Monitor (Extensions can be installed using Azure Policies) 2. Antivirus Microsoft Antimalware for Azure/ ARM Templates + Azure Policies 3. Backup Azure Backup along with Recovery Vault & Log Analytics Workspace 4. Sudo Configuration Azure CLI, Azure Functions 5. Patching Azure Update Manager 6. Health Check, Process Automation, Tech Specs Azure DSC, Azure Functions, Azure Policies, Azure VM Insight 7. H/W & S/W Scan Azure Functions, Azure Policies, Azure Inventory 8. CI discovery & update ITSM connector (TBD) 9. Domain Join To be validated","title":"Make Manage Automation"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/","text":"Azure LightHouse Admin Guide \u2693\ufe0e Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Azure Inventory Dashboard Workbook MCMS/Kyndryl as an MSP would require an Azure Inventory Dashboard - that provides a single pane glass view of the inventory/estate of the Customers (managed by MCMS). This requirement is met by building the \u2018Azure Inventory Dashboard\u2019 custom solution using workbook feature of Azure. The scope of this section is to provide a detailed implementation guidance on building of this solution. Kindly note that this solution as well works in the Azure Lighthouse context, by providing inventory view (single pane of glass view) of the delegated Customers tenants/subscriptions (managed by MSP/MCMS). NOTE : This \u2018Azure Inventory Dashboard\u2019 is a custom solution built using gallery template and is NOT an out of box solution from Azure. Azure Inventory Dashboard screenshots: Implementation Plan (Azure Inventory Dashboard) Pre-requisite: a) Contributor access required on the subscription to create the workbook. b) Reader access is required on the delegated tenants/subscriptions to view to inventory information through Dashboard. Step 1: Go to Azure monitor -> search for workbooks -> open an empty workbook -> click on \u2018\u2019 icon on the top (screenshot below) Step 2: Once you click on \u2018\u2019 icon, choose \u2018gallery template\u2019 and replace the contents in the window with contents from the below github link & click on apply and wait for few minutes for the workbook to load. https://github.com/scautomation/Azure-Inventory-Workbook/blob/master/galleryTemplate/template.json Step 3: The workbook edit screen would appear, every section would have an edit option and \u2018\u2026\u2019 option; click on \u2018\u2026\u2019 option to delete/remove any section from the inventory dashboard. As a practice, you may remove the first three writeup sections of the workbook. Step 4: Once you remove the initial three write-up sections of the workbook, the workbook will look lean as below; Now - you can click on done editing and click on save option to save the workbook. Step 5: This workbook can be published as shared dashboard. Click on the \u2018pin\u2019 option to publish the workbook as dashboard. Step 6: Under dashboards, the Azure inventory dashboard will be visible and since it is marked as shared \u2013 other users will be able to see the dashboard. References/Links (Azure Inventory Dashboard) https://github.com/scautomation/Azure-Inventory-Workbook https://raw.githubusercontent.com/scautomation/Azure-Inventory-Workbook/master/armTemplate/template.json https://github.com/scautomation/Azure-Inventory-Workbook/blob/master/galleryTemplate/template.json https://www.cloudsma.com/2020/10/ultimate-azure-inventory-dashboard/ https://docs.microsoft.com/en-us/azure/azure-monitor/visualize/workbooks-overview Cloud Native Monitoring (Azure Monitor) Typically, the Cloud Monitoring tools (third parties) such as Datadog and ScienceLogic - use up the Azure Monitor APIs in the backend to collect the metrics data from Cloud. Once metrics data has been collected, monitors/filters/KPIs are configured on the tool to report on the threshold breaches to the target Netcool webhook. The target Netcool webhook receives the event data in the JSON format, which is then further parsed/processed and passed to the pre-existing Ser-viceNow integration \u2013 which then translates the event into ticket and assigns it to the specified resolver group for action. With the Cloud Native Monitoring approach, we intend to circumvent the use of any Third Party / Cloud Monitoring tools; by choosing to send the alerts/events directly from the Cloud native monitoring tool (i.e Azure Monitor for Azure) to the target Netcool webhook (MCMS) \u2013 the Netcool then parses/processes the event data received in JSON format and sends it to the pre-existing ServiceNow integration \u2013 for incident ticket creation. Azure Lighthouse context for enabling Cloud Native Monitoring Azure Lighthouse enables MSPs (Service Provider) to manage multiple customer tenants from within a single control plane (on Azure portal). Basically, using Azure Lighthouse you can get a single cross tenant view across your customers tenants and subscriptions from your single Azure portal login (without the need for switching of the tenant/account to view the target Customers subscriptions). With Azure Lighthouse, a service provider/MSP can perform a wide range of management tasks directly on a customer's subscription (or resource group). This access is achieved through a logical projection, allowing service providers to sign into their own tenant and access resources that belong to the customer's tenant. The customer can determine which subscriptions or resource groups to delegate to the service provider, and the customer maintains full access to those resources. They can also remove the service provider's access at any time. MCMS as an MSP (service provider), and with Azure Lighthouse enabled for delegated access to customer tenants/subscriptions. There will be a separate subscription (that would include Management Hub/VNET) under Kyndryl Production Azure AAD Tenant for each of the MCMS Client/Customer. This Management Hub/VNET will include the management servers/infra such as Jump, Proxy, Ansible Jump etc. \u2013 for the respective MCMS Client/Customer. Kindly refer to the below diagram for clarity on this approach. Customers will provide the delegated access (Azure Lighthouse) for their Tenants/subscriptions to the target MCMS user groups/service principals on the designated MCMS subscription (assigned to the Client) under MCMS/Kyndryl Production AAD Tenant. In summary, by virtue of Azure Lighthouse (i.e Delegated Resource Management) \u2013 MCMS as an MSP gets a single pane of glass view across its Customers tenants/subscriptions from the MCMS/Kyndryl Production Azure AAD Tenant login on the Azure portal. In other words, MCMS SRE/Admin can login to their default MCMS/Kyndryl Production Tenant on Azure Portal using their Kyndryl w3 id and get a single view across all of its Customers tenants/subscriptions resources under various Azure sections/menus (be it Azure monitor , security center, backup center etc.). Azure Monitor access (Cloud Native Monitoring setup) under Azure Lighthouse With Azure Lighthouse (delegated resource management) enabled, MCMS SRE/Admin can access the Azure Monitor section of the Customer subscription from his current Azure portal login (MCMS/Kyndryl directory), without needing to switch the directory to the Customer tenant/directory. Once MCMS SRE/Admin is able to access the Azure Monitor section of the Customer subscription via Azure Lighthouse (delegated resource management), he/she can follow the same steps for Cloud Native monitoring setup on Azure. Key points: a. Action groups & alert rules will reside in target Customer subscription. b. Orchestration layer/Automation to create the alert rules (Cloud Native Monitoring) in Customer subscription will vary in Lighthouse context and requires a significant upgrade in the existing code. https://docs.microsoft.com/en-us/azure/lighthouse/concepts/cross-tenant-management-experience#supported-services-and-scenarios Cloud Native Monitoring Setup Pre-requisites: a) Account onboarding on the Netcool: \u2018Cloud Native Monitoring\u2019 Netcool logic needs to be prepared/onboarded by the Netcool team for the new account. TM (Transition Manager) to raise SR request to Netcool team with account details (three letter code/queue name/region etc.) b) Following CIs to be manually registered on Netcool. TM (Transition Manager) to raise SR request to Netcool team with account details (three letter code/queue name/region etc.): Azure_CI_PaaS. XXX (XXX \u2013 three letter code for the account) Azure_VM_Backup. XXX Azure_ServiceHealth. XXX c) Check with Netcool team, and based on Client region \u2013 get the appropriate Netcool webhook for your account: For instance - US PROD Netcool webhook: https://sldalmsgprb.multicloudmanagedservices.ibm.com:10093 EU PROD Netcool webhook: https://sllonmsgprb.multicloudmanagedservices.ibm.com : 10093 AP PROD Netcool webhook: https://slsydmsgprb.isprodimi.ibm.com : 10093 d) Ensure you specify, the three letter code of the account and the keyword \u2018PaaS\u2019 in the Alert rule description. For example: Alert rule description should be \u201c1M1: PaaS db storage space utilization above the threshold\u201d [1M1 is the three letter code of the account]. e) Whitelisting of the webhook/action group Azure endpoints/IPs (listed on below link) on MCMS firewall. This activity has been already performed. No action required from Transition team (refer section 8 for more details): https://docs.microsoft.com/en-us/azure/azure-monitor/platform/action-groups?WT.mc_id=Portal-Microsoft_Azure_Monitoring Implementation steps Step 1 : Create action group in Azure Monitor alerts section and specify the MCMS Netcool webhook URL. Step 2: Configure alert rules and select the target action group (which consists of webhook) to which the alert needs to be sent. NOTE : Ensure you specify, the three letter code of the account and the keyword \u2018PaaS\u2019 in the Alert rule description. For example: Alert rule description should be \u201c1M1: PaaS db storage space utilization above the threshold\u201d [1M1 is the three letter code of the account] Patch Management - Azure Update Manager Azure update manager is a native patching solution from Azure for managing operating system updates/patches for your Windows and Linux Virtual Machines deployed in Azure (note: it can as well be extended to VMs in non-Azure environments/ on-premises/other cloud environment). With Azure update manager, you can manage available updates/patches, schedule the installation of required updates/patches, and review deployment results after installing updates/patches https://docs.microsoft.com/en-us/azure/automation/update-management/overview Features: \u2022Native patch management solution from Azure. \u2022Highly available/scalable PaaS solution from Azure (In other words, there is no need for setting up your own Patch management central servers/infra) \u2022Offers patch management support for Windows & Linux Operating Systems. \u2022Server \u2013 Agent model (Log analytics agent & Hybrid runbook worker on VMs/endpoints). \u2022Active directory integration supported. \u2022Automated patch deployment supported. \u2022Supports non-Azure machines (on-premises/other clouds) \u2022Include/exclude patch facility available (you can define it in the schedules). \u2022Dashboard/report available on patch installation/endpoint status across all regions/subscriptions under given AD tenant. \u2022This Update Manager solution/module falls under Azure Automation account, which has other modules on Healthcheck Compliance such as Configuration management (inventory/change tracking/DSC-State configuration) and Process automation (runbook execution on endpoints). \u2022All the patches/updates/log data from the VMs/endpoints are stored in centralized Log Analytics Workspace \u2013 which can be used as data source for creating custom intuitive PowerBI dashboards. Pre-requisites: \u2022Azure automation account (Automation account can manage resources across all regions and subscriptions for a given AD tenant) \u2022Log Analytics agent on Windows/Linux VMs/endpoints \u2022PowerShell Desired State Configuration (DSC) extension for Linux VMs \u2022Automation Hybrid Runbook Worker (python based for Linux and Powershell/.NET based for Windows) \u2022Microsoft Update or Windows Server UpdateServices (WSUS) for Windows machines \u2022Either a private or public updaterepository for Linux machines \u2022Outbound internet access (port 443) from VMs Implementation Steps Step1 : Setup Automation account and link it to Log Analytics Workspace. Step 2 : In the update management section of the Automation account, click on add Azure VMs and select the VMs you want to onboard for update management. Step 3: After step 2, the Log analytics agent is installed/enabled on the selected VMs, and the update management section will report on the VMs reporting to it. Step 4: You can now schedule patch deployment on the VMs. Patch deployment schedule execution/report \u2013 WINDOWS endpoint Note: In the deployment schedule, you can choose for \u2018one time\u2019 execution or choose to run it periodically (for automated patch deployment). Patch deployment schedule execution/report \u2013 LINUX endpoint Limitations \u2022Deployment approval workflow is not available. \u2022Single pane of glass across Azure AD tenants is not available. (note: it supports single view of the machines reporting to the central Log Analytics Workspace - to which the Automation account is linked) \u2022Individual patch install facility is not available. \u2022Patches uninstallation facility is not available. \u2022Does not support Middleware/Databases/Applications based updates/patches. \u2022Out of box notification integration to mail/Netcool/ServiceNow is not available [it has to be custom built through log analytics workspace]. \u2022Detailed Out of Box patch reports are not available [for instance: patch report of a certain patch across VMs/endpoints is not available]. Dashboards - Ismail \u2693\ufe0e <<<<<<< HEAD A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. This section explains how the various Azure views/dashboards are enhanced through Light House functions. \u2693\ufe0e A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. This section explains the use of Azure Dashboards to provide a cross-client/cross-account view of Azure various resources, when LightHouse is enabled. It is important to note that single plane of glass view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal. 81dd2f7 Inventory - Ismail \u2693\ufe0e Compliance - Alwyn \u2693\ufe0e for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale Security Center - Alwyn \u2693\ufe0e Azure Monitor \u2693\ufe0e Cost Management \u2693\ufe0e Service Health - Alwyn \u2693\ufe0e Backup Center \u2693\ufe0e Inventory \u2693\ufe0e title2 \u2693\ufe0e title3 \u2693\ufe0e Resource Graphs Explorer, Workbooks (Inventory) - Ismail \u2693\ufe0e Azure Policy management across multiple customers (Sam) \u2693\ufe0e This section explains how Azure policies can be applied across subscriptions from the SP portal. There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription. Azure Policies through ARM templates \u2693\ufe0e Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" Run the following commands in the power shell window: $subs = Get-AzSubscription \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId Search-AzGraph -Query \"Resources | where type =~ 'Microsoft.Storage/storageAccounts' | project name, location, subscriptionId, tenantId, properties.supportsHttpsTrafficOnly\" -subscription $ManagedSubscriptions.subscriptionId | convertto-json Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions. Automations - Sam \u2693\ufe0e Creating Landing Zones through Light House Managing Monitoring - Ismael \u2693\ufe0e Managing Updates/Update Manager - Ismael \u2693\ufe0e AAD Management - Sam \u2693\ufe0e Explore if it is possible to create AAD Groups on customer, from SP Tenant","title":"Lh adminguide ismael prev"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#azure-lighthouse-admin-guide","text":"Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Azure Inventory Dashboard Workbook MCMS/Kyndryl as an MSP would require an Azure Inventory Dashboard - that provides a single pane glass view of the inventory/estate of the Customers (managed by MCMS). This requirement is met by building the \u2018Azure Inventory Dashboard\u2019 custom solution using workbook feature of Azure. The scope of this section is to provide a detailed implementation guidance on building of this solution. Kindly note that this solution as well works in the Azure Lighthouse context, by providing inventory view (single pane of glass view) of the delegated Customers tenants/subscriptions (managed by MSP/MCMS). NOTE : This \u2018Azure Inventory Dashboard\u2019 is a custom solution built using gallery template and is NOT an out of box solution from Azure. Azure Inventory Dashboard screenshots: Implementation Plan (Azure Inventory Dashboard) Pre-requisite: a) Contributor access required on the subscription to create the workbook. b) Reader access is required on the delegated tenants/subscriptions to view to inventory information through Dashboard. Step 1: Go to Azure monitor -> search for workbooks -> open an empty workbook -> click on \u2018\u2019 icon on the top (screenshot below) Step 2: Once you click on \u2018\u2019 icon, choose \u2018gallery template\u2019 and replace the contents in the window with contents from the below github link & click on apply and wait for few minutes for the workbook to load. https://github.com/scautomation/Azure-Inventory-Workbook/blob/master/galleryTemplate/template.json Step 3: The workbook edit screen would appear, every section would have an edit option and \u2018\u2026\u2019 option; click on \u2018\u2026\u2019 option to delete/remove any section from the inventory dashboard. As a practice, you may remove the first three writeup sections of the workbook. Step 4: Once you remove the initial three write-up sections of the workbook, the workbook will look lean as below; Now - you can click on done editing and click on save option to save the workbook. Step 5: This workbook can be published as shared dashboard. Click on the \u2018pin\u2019 option to publish the workbook as dashboard. Step 6: Under dashboards, the Azure inventory dashboard will be visible and since it is marked as shared \u2013 other users will be able to see the dashboard. References/Links (Azure Inventory Dashboard) https://github.com/scautomation/Azure-Inventory-Workbook https://raw.githubusercontent.com/scautomation/Azure-Inventory-Workbook/master/armTemplate/template.json https://github.com/scautomation/Azure-Inventory-Workbook/blob/master/galleryTemplate/template.json https://www.cloudsma.com/2020/10/ultimate-azure-inventory-dashboard/ https://docs.microsoft.com/en-us/azure/azure-monitor/visualize/workbooks-overview Cloud Native Monitoring (Azure Monitor) Typically, the Cloud Monitoring tools (third parties) such as Datadog and ScienceLogic - use up the Azure Monitor APIs in the backend to collect the metrics data from Cloud. Once metrics data has been collected, monitors/filters/KPIs are configured on the tool to report on the threshold breaches to the target Netcool webhook. The target Netcool webhook receives the event data in the JSON format, which is then further parsed/processed and passed to the pre-existing Ser-viceNow integration \u2013 which then translates the event into ticket and assigns it to the specified resolver group for action. With the Cloud Native Monitoring approach, we intend to circumvent the use of any Third Party / Cloud Monitoring tools; by choosing to send the alerts/events directly from the Cloud native monitoring tool (i.e Azure Monitor for Azure) to the target Netcool webhook (MCMS) \u2013 the Netcool then parses/processes the event data received in JSON format and sends it to the pre-existing ServiceNow integration \u2013 for incident ticket creation. Azure Lighthouse context for enabling Cloud Native Monitoring Azure Lighthouse enables MSPs (Service Provider) to manage multiple customer tenants from within a single control plane (on Azure portal). Basically, using Azure Lighthouse you can get a single cross tenant view across your customers tenants and subscriptions from your single Azure portal login (without the need for switching of the tenant/account to view the target Customers subscriptions). With Azure Lighthouse, a service provider/MSP can perform a wide range of management tasks directly on a customer's subscription (or resource group). This access is achieved through a logical projection, allowing service providers to sign into their own tenant and access resources that belong to the customer's tenant. The customer can determine which subscriptions or resource groups to delegate to the service provider, and the customer maintains full access to those resources. They can also remove the service provider's access at any time. MCMS as an MSP (service provider), and with Azure Lighthouse enabled for delegated access to customer tenants/subscriptions. There will be a separate subscription (that would include Management Hub/VNET) under Kyndryl Production Azure AAD Tenant for each of the MCMS Client/Customer. This Management Hub/VNET will include the management servers/infra such as Jump, Proxy, Ansible Jump etc. \u2013 for the respective MCMS Client/Customer. Kindly refer to the below diagram for clarity on this approach. Customers will provide the delegated access (Azure Lighthouse) for their Tenants/subscriptions to the target MCMS user groups/service principals on the designated MCMS subscription (assigned to the Client) under MCMS/Kyndryl Production AAD Tenant. In summary, by virtue of Azure Lighthouse (i.e Delegated Resource Management) \u2013 MCMS as an MSP gets a single pane of glass view across its Customers tenants/subscriptions from the MCMS/Kyndryl Production Azure AAD Tenant login on the Azure portal. In other words, MCMS SRE/Admin can login to their default MCMS/Kyndryl Production Tenant on Azure Portal using their Kyndryl w3 id and get a single view across all of its Customers tenants/subscriptions resources under various Azure sections/menus (be it Azure monitor , security center, backup center etc.). Azure Monitor access (Cloud Native Monitoring setup) under Azure Lighthouse With Azure Lighthouse (delegated resource management) enabled, MCMS SRE/Admin can access the Azure Monitor section of the Customer subscription from his current Azure portal login (MCMS/Kyndryl directory), without needing to switch the directory to the Customer tenant/directory. Once MCMS SRE/Admin is able to access the Azure Monitor section of the Customer subscription via Azure Lighthouse (delegated resource management), he/she can follow the same steps for Cloud Native monitoring setup on Azure. Key points: a. Action groups & alert rules will reside in target Customer subscription. b. Orchestration layer/Automation to create the alert rules (Cloud Native Monitoring) in Customer subscription will vary in Lighthouse context and requires a significant upgrade in the existing code. https://docs.microsoft.com/en-us/azure/lighthouse/concepts/cross-tenant-management-experience#supported-services-and-scenarios Cloud Native Monitoring Setup Pre-requisites: a) Account onboarding on the Netcool: \u2018Cloud Native Monitoring\u2019 Netcool logic needs to be prepared/onboarded by the Netcool team for the new account. TM (Transition Manager) to raise SR request to Netcool team with account details (three letter code/queue name/region etc.) b) Following CIs to be manually registered on Netcool. TM (Transition Manager) to raise SR request to Netcool team with account details (three letter code/queue name/region etc.): Azure_CI_PaaS. XXX (XXX \u2013 three letter code for the account) Azure_VM_Backup. XXX Azure_ServiceHealth. XXX c) Check with Netcool team, and based on Client region \u2013 get the appropriate Netcool webhook for your account: For instance - US PROD Netcool webhook: https://sldalmsgprb.multicloudmanagedservices.ibm.com:10093 EU PROD Netcool webhook: https://sllonmsgprb.multicloudmanagedservices.ibm.com : 10093 AP PROD Netcool webhook: https://slsydmsgprb.isprodimi.ibm.com : 10093 d) Ensure you specify, the three letter code of the account and the keyword \u2018PaaS\u2019 in the Alert rule description. For example: Alert rule description should be \u201c1M1: PaaS db storage space utilization above the threshold\u201d [1M1 is the three letter code of the account]. e) Whitelisting of the webhook/action group Azure endpoints/IPs (listed on below link) on MCMS firewall. This activity has been already performed. No action required from Transition team (refer section 8 for more details): https://docs.microsoft.com/en-us/azure/azure-monitor/platform/action-groups?WT.mc_id=Portal-Microsoft_Azure_Monitoring Implementation steps Step 1 : Create action group in Azure Monitor alerts section and specify the MCMS Netcool webhook URL. Step 2: Configure alert rules and select the target action group (which consists of webhook) to which the alert needs to be sent. NOTE : Ensure you specify, the three letter code of the account and the keyword \u2018PaaS\u2019 in the Alert rule description. For example: Alert rule description should be \u201c1M1: PaaS db storage space utilization above the threshold\u201d [1M1 is the three letter code of the account] Patch Management - Azure Update Manager Azure update manager is a native patching solution from Azure for managing operating system updates/patches for your Windows and Linux Virtual Machines deployed in Azure (note: it can as well be extended to VMs in non-Azure environments/ on-premises/other cloud environment). With Azure update manager, you can manage available updates/patches, schedule the installation of required updates/patches, and review deployment results after installing updates/patches https://docs.microsoft.com/en-us/azure/automation/update-management/overview Features: \u2022Native patch management solution from Azure. \u2022Highly available/scalable PaaS solution from Azure (In other words, there is no need for setting up your own Patch management central servers/infra) \u2022Offers patch management support for Windows & Linux Operating Systems. \u2022Server \u2013 Agent model (Log analytics agent & Hybrid runbook worker on VMs/endpoints). \u2022Active directory integration supported. \u2022Automated patch deployment supported. \u2022Supports non-Azure machines (on-premises/other clouds) \u2022Include/exclude patch facility available (you can define it in the schedules). \u2022Dashboard/report available on patch installation/endpoint status across all regions/subscriptions under given AD tenant. \u2022This Update Manager solution/module falls under Azure Automation account, which has other modules on Healthcheck Compliance such as Configuration management (inventory/change tracking/DSC-State configuration) and Process automation (runbook execution on endpoints). \u2022All the patches/updates/log data from the VMs/endpoints are stored in centralized Log Analytics Workspace \u2013 which can be used as data source for creating custom intuitive PowerBI dashboards. Pre-requisites: \u2022Azure automation account (Automation account can manage resources across all regions and subscriptions for a given AD tenant) \u2022Log Analytics agent on Windows/Linux VMs/endpoints \u2022PowerShell Desired State Configuration (DSC) extension for Linux VMs \u2022Automation Hybrid Runbook Worker (python based for Linux and Powershell/.NET based for Windows) \u2022Microsoft Update or Windows Server UpdateServices (WSUS) for Windows machines \u2022Either a private or public updaterepository for Linux machines \u2022Outbound internet access (port 443) from VMs Implementation Steps Step1 : Setup Automation account and link it to Log Analytics Workspace. Step 2 : In the update management section of the Automation account, click on add Azure VMs and select the VMs you want to onboard for update management. Step 3: After step 2, the Log analytics agent is installed/enabled on the selected VMs, and the update management section will report on the VMs reporting to it. Step 4: You can now schedule patch deployment on the VMs. Patch deployment schedule execution/report \u2013 WINDOWS endpoint Note: In the deployment schedule, you can choose for \u2018one time\u2019 execution or choose to run it periodically (for automated patch deployment). Patch deployment schedule execution/report \u2013 LINUX endpoint Limitations \u2022Deployment approval workflow is not available. \u2022Single pane of glass across Azure AD tenants is not available. (note: it supports single view of the machines reporting to the central Log Analytics Workspace - to which the Automation account is linked) \u2022Individual patch install facility is not available. \u2022Patches uninstallation facility is not available. \u2022Does not support Middleware/Databases/Applications based updates/patches. \u2022Out of box notification integration to mail/Netcool/ServiceNow is not available [it has to be custom built through log analytics workspace]. \u2022Detailed Out of Box patch reports are not available [for instance: patch report of a certain patch across VMs/endpoints is not available].","title":"Azure LightHouse Admin Guide"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#dashboards-ismail","text":"<<<<<<< HEAD","title":"Dashboards - Ismail"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#a-major-advantage-of-light-house-is-to-view-manage-multiple-client-resources-from-within-a-single-view-without-having-to-switch-to-each-customer-directories-in-azure-portal-this-section-explains-how-the-various-azure-viewsdashboards-are-enhanced-through-light-house-functions","text":"A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. This section explains the use of Azure Dashboards to provide a cross-client/cross-account view of Azure various resources, when LightHouse is enabled. It is important to note that single plane of glass view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal. 81dd2f7","title":"A major advantage of Light House is to view &amp; manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. This section explains how the various Azure views/dashboards are enhanced through Light House functions."},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#inventory-ismail","text":"","title":"Inventory - Ismail"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#compliance-alwyn","text":"for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale","title":"Compliance - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#security-center-alwyn","text":"","title":"Security Center  - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#azure-monitor","text":"","title":"Azure Monitor"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#cost-management","text":"","title":"Cost Management"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#service-health-alwyn","text":"","title":"Service Health - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#backup-center","text":"","title":"Backup Center"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#inventory","text":"","title":"Inventory"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#title2","text":"","title":"title2"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#title3","text":"","title":"title3"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#resource-graphs-explorer-workbooks-inventory-ismail","text":"","title":"Resource Graphs Explorer, Workbooks (Inventory) - Ismail"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#azure-policy-management-across-multiple-customers-sam","text":"This section explains how Azure policies can be applied across subscriptions from the SP portal. There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription.","title":"Azure Policy management across multiple customers (Sam)"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#azure-policies-through-arm-templates","text":"Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" Run the following commands in the power shell window: $subs = Get-AzSubscription \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId Search-AzGraph -Query \"Resources | where type =~ 'Microsoft.Storage/storageAccounts' | project name, location, subscriptionId, tenantId, properties.supportsHttpsTrafficOnly\" -subscription $ManagedSubscriptions.subscriptionId | convertto-json Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions.","title":"Azure Policies through ARM templates"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#automations-sam","text":"Creating Landing Zones through Light House","title":"Automations - Sam"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#managing-monitoring-ismael","text":"","title":"Managing Monitoring - Ismael"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#managing-updatesupdate-manager-ismael","text":"","title":"Managing Updates/Update Manager - Ismael"},{"location":"automation/lighthouse/lh-adminguide-ismael-prev/#aad-management-sam","text":"Explore if it is possible to create AAD Groups on customer, from SP Tenant","title":"AAD Management - Sam"},{"location":"automation/lighthouse/lh-adminguide-sam/","text":"Azure LightHouse Admin Guide \u2693\ufe0e Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Administrative Operations that can be performed using Light House can be classified as: 1) DASHBOARD Operations - It is very useful for operations to have a view into all managed accounts from a single console, providing similar to a dashboard view of critical functions & services like Inventory, Compliance, Monitor etc. These operations are READ-ONLY/VIEW-ONLY and does not make any changes on the target client resources. 2) MANAGE Operations - These are write operations essentially to modify and manage resources, like adding/updating monitoring, triggering updates, making configuration changes on resources..etc. These operations are performed generally through UI/Azure Portal 3) AUTOMATIONS - These are write operations used to apply standard rules/definitions/policies across multiple clients, including creation of new resources through automations. DASHBOARDS - Ismael \u2693\ufe0e A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. It is important to note that \"single plane of glass\" view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal. This section explains how dashboards can be created in Azure to have a view of all managed accounts.the various Azure views/dashboards can be configured and used to view resources and status on multiple clients. Inventory - Ismael \u2693\ufe0e Compliance - Alwyn \u2693\ufe0e for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale Security Center - Alwyn \u2693\ufe0e Azure Monitor \u2693\ufe0e Cost Management \u2693\ufe0e Service Health - Alwyn \u2693\ufe0e Backup Center \u2693\ufe0e Resource Graphs Explorer, Workbooks - Ismael \u2693\ufe0e Managing through Light House \u2693\ufe0e Managing Monitoring \u2693\ufe0e As explained above, In summary, by virtue of Azure Lighthouse (i.e Delegated Resource Management) \u2013 MCMS as an MSP gets a single pane of glass view across its Customers tenants/subscriptions from the MCMS/Kyndryl Production Azure AAD Tenant login on the Azure por-tal. In other words, MCMS SRE/Admin can login to their default MCMS/Kyndryl Production Tenant on Azure Portal using their Kyndryl w3 id and get a single view across all of its Customers tenants/subscriptions resources under various Azure sec-tions/menus (be it Azure monitor, security center, backup center etc.). Figure 4b: Azure Lighthouse MCMS model overview Azure Monitor access (Cloud Native Monitoring setup) under Azure Lighthouse: With Azure Lighthouse (delegated resource management) enabled, MCMS SRE/Admin can access the Azure Monitor section of the Customer subscription from his current Azure portal login (MCMS/Kyndryl directory), without needing to switch the directory to the Customer tenant/directory. Once MCMS SRE/Admin is able to access the Azure Monitor section of the Customer subscription via Azure Lighthouse (dele-gated Managing Updates/Update Manager - Ismael \u2693\ufe0e AAD Management - Sam \u2693\ufe0e Explore if it is possible to create AAD Groups on customer, from SP Tenant Automations through LightHouse \u2693\ufe0e Azure Policy management across multiple customers (Sam) \u2693\ufe0e This section explains how Azure policies can be applied across subscriptions from the SP portal. LIMITATIONS : There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription. Azure Policies through ARM templates \u2693\ufe0e Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" . Run the following commands in the power shell window, one by one: (OR copy paste at one go) $subs = Get-AzSubscription . \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId . $ManagedSubscriptions.subscriptionId | convertto-json . Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions. NOTE that there will be policy defined for each subscription on the customer's tenant. The reason for this is that Azure Lighthouse only provides permissions to SP at Subscription level and not to the tenant itself and hence the definitions are created at subscription level if Lighthouse is used to deploy the policies. Recommendation for Azure Policies through LightHouse (FOR REVIEW with Alwyn) \u2693\ufe0e Since it is an overhead to have multiple policies (one for each subscription) in a Tenant, it is recommended to define and apply policies directly at the clients' tenant instead of Light House. Landing Zone Creation through Azure Light House \u2693\ufe0e Landing Zones are created using ARM templates in most cases & hence techinically Landing Zones can be created using ARM templates, similar to above policy template example. Recommendation :However, there is no major advantage or benefit in creating Landing Zones through Light House, since Kyndryl teams will anyway have access to the client's AAD tenant & subscriptions to created required resources directly, without any special network configurations.","title":"Lh adminguide sam"},{"location":"automation/lighthouse/lh-adminguide-sam/#azure-lighthouse-admin-guide","text":"Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Administrative Operations that can be performed using Light House can be classified as: 1) DASHBOARD Operations - It is very useful for operations to have a view into all managed accounts from a single console, providing similar to a dashboard view of critical functions & services like Inventory, Compliance, Monitor etc. These operations are READ-ONLY/VIEW-ONLY and does not make any changes on the target client resources. 2) MANAGE Operations - These are write operations essentially to modify and manage resources, like adding/updating monitoring, triggering updates, making configuration changes on resources..etc. These operations are performed generally through UI/Azure Portal 3) AUTOMATIONS - These are write operations used to apply standard rules/definitions/policies across multiple clients, including creation of new resources through automations.","title":"Azure LightHouse Admin Guide"},{"location":"automation/lighthouse/lh-adminguide-sam/#dashboards-ismael","text":"A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. It is important to note that \"single plane of glass\" view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal. This section explains how dashboards can be created in Azure to have a view of all managed accounts.the various Azure views/dashboards can be configured and used to view resources and status on multiple clients.","title":"DASHBOARDS - Ismael"},{"location":"automation/lighthouse/lh-adminguide-sam/#inventory-ismael","text":"","title":"Inventory - Ismael"},{"location":"automation/lighthouse/lh-adminguide-sam/#compliance-alwyn","text":"for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale","title":"Compliance - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-sam/#security-center-alwyn","text":"","title":"Security Center  - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-sam/#azure-monitor","text":"","title":"Azure Monitor"},{"location":"automation/lighthouse/lh-adminguide-sam/#cost-management","text":"","title":"Cost Management"},{"location":"automation/lighthouse/lh-adminguide-sam/#service-health-alwyn","text":"","title":"Service Health - Alwyn"},{"location":"automation/lighthouse/lh-adminguide-sam/#backup-center","text":"","title":"Backup Center"},{"location":"automation/lighthouse/lh-adminguide-sam/#resource-graphs-explorer-workbooks-ismael","text":"","title":"Resource Graphs Explorer, Workbooks - Ismael"},{"location":"automation/lighthouse/lh-adminguide-sam/#managing-through-light-house","text":"","title":"Managing through Light House"},{"location":"automation/lighthouse/lh-adminguide-sam/#managing-monitoring","text":"As explained above, In summary, by virtue of Azure Lighthouse (i.e Delegated Resource Management) \u2013 MCMS as an MSP gets a single pane of glass view across its Customers tenants/subscriptions from the MCMS/Kyndryl Production Azure AAD Tenant login on the Azure por-tal. In other words, MCMS SRE/Admin can login to their default MCMS/Kyndryl Production Tenant on Azure Portal using their Kyndryl w3 id and get a single view across all of its Customers tenants/subscriptions resources under various Azure sec-tions/menus (be it Azure monitor, security center, backup center etc.). Figure 4b: Azure Lighthouse MCMS model overview Azure Monitor access (Cloud Native Monitoring setup) under Azure Lighthouse: With Azure Lighthouse (delegated resource management) enabled, MCMS SRE/Admin can access the Azure Monitor section of the Customer subscription from his current Azure portal login (MCMS/Kyndryl directory), without needing to switch the directory to the Customer tenant/directory. Once MCMS SRE/Admin is able to access the Azure Monitor section of the Customer subscription via Azure Lighthouse (dele-gated","title":"Managing Monitoring"},{"location":"automation/lighthouse/lh-adminguide-sam/#managing-updatesupdate-manager-ismael","text":"","title":"Managing Updates/Update Manager - Ismael"},{"location":"automation/lighthouse/lh-adminguide-sam/#aad-management-sam","text":"Explore if it is possible to create AAD Groups on customer, from SP Tenant","title":"AAD Management - Sam"},{"location":"automation/lighthouse/lh-adminguide-sam/#automations-through-lighthouse","text":"","title":"Automations through LightHouse"},{"location":"automation/lighthouse/lh-adminguide-sam/#azure-policy-management-across-multiple-customers-sam","text":"This section explains how Azure policies can be applied across subscriptions from the SP portal. LIMITATIONS : There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription.","title":"Azure Policy management across multiple customers (Sam)"},{"location":"automation/lighthouse/lh-adminguide-sam/#azure-policies-through-arm-templates","text":"Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" . Run the following commands in the power shell window, one by one: (OR copy paste at one go) $subs = Get-AzSubscription . \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId . $ManagedSubscriptions.subscriptionId | convertto-json . Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions. NOTE that there will be policy defined for each subscription on the customer's tenant. The reason for this is that Azure Lighthouse only provides permissions to SP at Subscription level and not to the tenant itself and hence the definitions are created at subscription level if Lighthouse is used to deploy the policies.","title":"Azure Policies through ARM templates"},{"location":"automation/lighthouse/lh-adminguide-sam/#recommendation-for-azure-policies-through-lighthouse-for-review-with-alwyn","text":"Since it is an overhead to have multiple policies (one for each subscription) in a Tenant, it is recommended to define and apply policies directly at the clients' tenant instead of Light House.","title":"Recommendation for Azure Policies through LightHouse (FOR REVIEW with Alwyn)"},{"location":"automation/lighthouse/lh-adminguide-sam/#landing-zone-creation-through-azure-light-house","text":"Landing Zones are created using ARM templates in most cases & hence techinically Landing Zones can be created using ARM templates, similar to above policy template example. Recommendation :However, there is no major advantage or benefit in creating Landing Zones through Light House, since Kyndryl teams will anyway have access to the client's AAD tenant & subscriptions to created required resources directly, without any special network configurations.","title":"Landing Zone Creation through Azure Light House"},{"location":"automation/lighthouse/lh-adminguide/","text":"Azure LightHouse Admin Guide \u2693\ufe0e Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Administrative Operations that can be performed using Light House can be classified as: 1) DASHBOARD Operations - It is very useful for operations to have a view into all managed accounts from a single console, providing similar to a dashboard view of critical functions & services like Inventory, Compliance, Monitor etc. These operations are READ-ONLY/VIEW-ONLY and does not make any changes on the target client resources. 2) MANAGE Operations - These are write operations essentially to modify and manage resources, like adding/updating monitoring, triggering updates, making configuration changes on resources..etc. These operations are performed generally through UI/Azure Portal 3) AUTOMATIONS - These are write operations used to apply standard rules/definitions/policies across multiple clients, including creation of new resources through automations. DASHBOARDS \u2693\ufe0e A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. This section explains the use of Azure Dashboards to provide a cross-client/cross-account view of Azure various resources, when LightHouse is enabled. It is important to note that single plane of glass view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal. Service Health Dashboard - Alwyn \u2693\ufe0e Inventory Dashboard - Ismael \u2693\ufe0e Compliance Dashboard - Alwyn \u2693\ufe0e for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale Security Center Dashboard - Alwyn \u2693\ufe0e Azure Monitor Dashboard \u2693\ufe0e Cost Management Dashboard \u2693\ufe0e Backup Center Dashboard \u2693\ufe0e Resource Graphs Explorer, Workbooks - Ismael \u2693\ufe0e Managing through Light House \u2693\ufe0e Managing Monitoring - Ismael/Sam \u2693\ufe0e To manage monitoring on a client resource, it is required to navigate to the specific subscription from the SP Azure Portal, and then perform any operations on Azure Monitor from the target subscription. Managing Updates/Update Manager - Ismael \u2693\ufe0e AAD Management - Sam \u2693\ufe0e Explore if it is possible to create AAD Groups on customer, from SP Tenant Automations through LightHouse \u2693\ufe0e Azure Policy management across multiple customers (Sam) \u2693\ufe0e This section explains how Azure policies can be applied across subscriptions from the SP portal. LIMITATIONS : There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription. Azure Policies through ARM templates \u2693\ufe0e Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" . Run the following commands in the power shell window, one by one: (OR copy paste at one go) $subs = Get-AzSubscription . \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId . $ManagedSubscriptions.subscriptionId | convertto-json . Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions. NOTE that there will be policy defined for each subscription on the customer's tenant. The reason for this is that Azure Lighthouse only provides permissions to SP at Subscription level and not to the tenant itself and hence the definitions are created at subscription level if Lighthouse is used to deploy the policies. Recommendation for Azure Policies through LightHouse (FOR REVIEW with Alwyn) \u2693\ufe0e Since it is an overhead to have multiple policies (one for each subscription) in a Tenant, it is recommended to define and apply policies directly at the clients' tenant instead of Light House. Landing Zone Creation through Azure Light House \u2693\ufe0e Landing Zones are created using ARM templates in most cases & hence techinically Landing Zones can be created using ARM templates, similar to above policy template example. Recommendation :However, there is no major advantage or benefit in creating Landing Zones through Light House, since Kyndryl teams will anyway have access to the client's AAD tenant & subscriptions to created required resources directly, without any special network configurations.","title":"LightHouse Administration"},{"location":"automation/lighthouse/lh-adminguide/#azure-lighthouse-admin-guide","text":"Key benefit of Azure Lighthouse is for service providers like Kyndryl to perform operations at scale across several customers (tenants) at once, making management tasks more efficient. This section provides guidelines on how multiple clients can be managed from Service Provider(SP) tenant using Light House feature Administrative Operations that can be performed using Light House can be classified as: 1) DASHBOARD Operations - It is very useful for operations to have a view into all managed accounts from a single console, providing similar to a dashboard view of critical functions & services like Inventory, Compliance, Monitor etc. These operations are READ-ONLY/VIEW-ONLY and does not make any changes on the target client resources. 2) MANAGE Operations - These are write operations essentially to modify and manage resources, like adding/updating monitoring, triggering updates, making configuration changes on resources..etc. These operations are performed generally through UI/Azure Portal 3) AUTOMATIONS - These are write operations used to apply standard rules/definitions/policies across multiple clients, including creation of new resources through automations.","title":"Azure LightHouse Admin Guide"},{"location":"automation/lighthouse/lh-adminguide/#dashboards","text":"A major advantage of Light House is to view & manage multiple client resources from within a single view without having to switch to each customer directories in Azure portal. LightHouse itself does NOT have any dashboard but Lighthouse provides the underlying wiring thereby allowing Azure native functions to provide a view \"into\" multiple clients. This section explains the use of Azure Dashboards to provide a cross-client/cross-account view of Azure various resources, when LightHouse is enabled. It is important to note that single plane of glass view across multiple account is possible only for certain resources/services whereas for remaining it is possible to still view the contents by going into the specific subscription from Azure Service Provider(SP) portal.","title":"DASHBOARDS"},{"location":"automation/lighthouse/lh-adminguide/#service-health-dashboard-alwyn","text":"","title":"Service Health Dashboard - Alwyn"},{"location":"automation/lighthouse/lh-adminguide/#inventory-dashboard-ismael","text":"","title":"Inventory Dashboard - Ismael"},{"location":"automation/lighthouse/lh-adminguide/#compliance-dashboard-alwyn","text":"for Alwyn;s review: \"While you can deploy policies across multiple tenants, currently you can't view compliance details for non-compliant resources in these tenants.\" SOURCE: https://docs.microsoft.com/en-us/azure/lighthouse/how-to/policy-at-scale","title":"Compliance Dashboard - Alwyn"},{"location":"automation/lighthouse/lh-adminguide/#security-center-dashboard-alwyn","text":"","title":"Security Center Dashboard  - Alwyn"},{"location":"automation/lighthouse/lh-adminguide/#azure-monitor-dashboard","text":"","title":"Azure Monitor Dashboard"},{"location":"automation/lighthouse/lh-adminguide/#cost-management-dashboard","text":"","title":"Cost Management Dashboard"},{"location":"automation/lighthouse/lh-adminguide/#backup-center-dashboard","text":"","title":"Backup Center Dashboard"},{"location":"automation/lighthouse/lh-adminguide/#resource-graphs-explorer-workbooks-ismael","text":"","title":"Resource Graphs Explorer, Workbooks - Ismael"},{"location":"automation/lighthouse/lh-adminguide/#managing-through-light-house","text":"","title":"Managing through Light House"},{"location":"automation/lighthouse/lh-adminguide/#managing-monitoring-ismaelsam","text":"To manage monitoring on a client resource, it is required to navigate to the specific subscription from the SP Azure Portal, and then perform any operations on Azure Monitor from the target subscription.","title":"Managing Monitoring - Ismael/Sam"},{"location":"automation/lighthouse/lh-adminguide/#managing-updatesupdate-manager-ismael","text":"","title":"Managing Updates/Update Manager - Ismael"},{"location":"automation/lighthouse/lh-adminguide/#aad-management-sam","text":"Explore if it is possible to create AAD Groups on customer, from SP Tenant","title":"AAD Management - Sam"},{"location":"automation/lighthouse/lh-adminguide/#automations-through-lighthouse","text":"","title":"Automations through LightHouse"},{"location":"automation/lighthouse/lh-adminguide/#azure-policy-management-across-multiple-customers-sam","text":"This section explains how Azure policies can be applied across subscriptions from the SP portal. LIMITATIONS : There are limitations on how Azure policies can be applied across multiple customers/customer subscriptions: 1) Azure Light House does not allow policies to be applied across multiple clients through the portal (UI) 2) Azure policies cannot be applied using \"az policy\" commands from the SP Cloud Shell. For example the expectation would be to create a policy on the SP Tenant/subscription and use the native az policy commands to define target customer subscription as the scope as shown below: az policy assignment create --scope \"/subscriptions/SubscriptionID-of-customer-subscription \" --policy 36f030bc-8124-470b-97ed-b6da4be98575 However, it does NOT work this way, by design. Azure policies should be created on each customer subscription and then applied to that subscription.","title":"Azure Policy management across multiple customers (Sam)"},{"location":"automation/lighthouse/lh-adminguide/#azure-policies-through-arm-templates","text":"Nevertheless, ARM templates can be used, rather the only way, to centrally deploy and manage policies. ARM templates can be applied against multiple subscriptions through a combination of powershell commands and Azure CLIs. ARM templates can be run against target subscriptions from the SP cloudshell, thereby providing a way to have policies deployed to delegated subscriptions at scale Based on this model, the best practice/standard for Kyndryl is to publish policy templates in github, and apply the same against managed customers through SP/Managing subscription's cloud shell. Here is a sample policy which checks if the VM names start with \"kyndryl\" and if not classify as non-compliant. This template should be executed from the SP cloud shell using Power Shell commands. Install Resource Graph module using command \"Install-Module -Name Az.ResourceGraph\" . Run the following commands in the power shell window, one by one: (OR copy paste at one go) $subs = Get-AzSubscription . \\(ManagedSubscriptions = Search-AzGraph -Query \"ResourceContainers | where type == 'microsoft.resources/subscriptions' | where tenantId != '\\) ($mspTenant)' | project name, subscriptionId, tenantId\" -subscription $subs.subscriptionId . $ManagedSubscriptions.subscriptionId | convertto-json . Write-Output \"In total, there are \\((\\) ManagedSubscriptions.Count) delegated customer subscriptions to be managed\" foreach ($ManagedSub in $ManagedSubscriptions) { Select-AzSubscription -SubscriptionId $ManagedSub.subscriptionId New-AzSubscriptionDeployment -Name mgmt ` -Location eastus ` -TemplateUri \"https://raw.github.kyndryl.net/OCTO/azure/main/docs/policies/lh-vmname-template.json?token=AAAP4K2GDWWLG6YV3E5WEX3BKNQ3K\" ` -AsJob } After a few minutes, validate policies are created by checking on the client subscriptions. NOTE that there will be policy defined for each subscription on the customer's tenant. The reason for this is that Azure Lighthouse only provides permissions to SP at Subscription level and not to the tenant itself and hence the definitions are created at subscription level if Lighthouse is used to deploy the policies.","title":"Azure Policies through ARM templates"},{"location":"automation/lighthouse/lh-adminguide/#recommendation-for-azure-policies-through-lighthouse-for-review-with-alwyn","text":"Since it is an overhead to have multiple policies (one for each subscription) in a Tenant, it is recommended to define and apply policies directly at the clients' tenant instead of Light House.","title":"Recommendation for Azure Policies through LightHouse (FOR REVIEW with Alwyn)"},{"location":"automation/lighthouse/lh-adminguide/#landing-zone-creation-through-azure-light-house","text":"Landing Zones are created using ARM templates in most cases & hence techinically Landing Zones can be created using ARM templates, similar to above policy template example. Recommendation :However, there is no major advantage or benefit in creating Landing Zones through Light House, since Kyndryl teams will anyway have access to the client's AAD tenant & subscriptions to created required resources directly, without any special network configurations.","title":"Landing Zone Creation through Azure Light House"},{"location":"automation/lighthouse/lh-enableguide/","text":"Azure LightHouse Enablement Guide \u2693\ufe0e Azure LightHouse Overview \u2693\ufe0e Azure Lighthouse is designed to provide a secure, granular way for Service Providers(SP) to gain single token access to customer environments using the principles of least privilege to help easily manage multiple customer environments and automate management tasks at scale. Azure LightHouse helps service providers like Kyndryl in effective governance management across customers. Azure LightHouse Concept \u2693\ufe0e Light House allows authorized users in one Azure Active Directory (Azure AD) tenant, referred as Service Provider(SP) , perform management operations across different Azure AD tenants belonging to different clients, referred as Customer/Managed Customer Azure Light House operates on the concept of \u201c Delegated Resource Management \u201d which enables logical projection of resources from one Azure tenant onto another tenant. This concept creates logical control plane access for the service provider onto customer resources thereby providing the service provider ability to view and manage custome resources Customer resources to be projected can be at subscription and/or Resource Group level only and NOT at the customer tenant level. (Delegation at Management Group has been introduced - Read Note below) **How it works-Overview **: When LightHouse is \"enabled\" on a customer subscription, specifying the managed service provider's Azure AD tenant details, Azure backbone framework establishes connectivity between SP & customer resources, using VNET and other services which is transparent and not visible to both the service provider and the customer. Once connectivity is established, customer resources appear in the Light House menu as \"customers\". Type of access for the SP is defined during the enablement process. It is important to note that client has visibility, as well as can control what roles (Azure Roles) are granted to the SP. However the client may not have the ability to control which user from the SP, has access to their resources (explained further in Access Control/RBAC section) Also important to note is the fact that there are no Users/Groups need to be defined for SP on the customer Tenant, but only \"Azure Roles\" are assigned to the SP \"Owner\" Role cannot be assigned to a SP - this is by design and cannot be overridden. This enablement step has to performed on every subscription a customer has. ** Cost**: Light House capability is included by default in every subscription. Any subscription can be used as a Service Provider(SP) or customer, at no cost! More details on Azure Light House Enablement Steps - on Service Provider(SP) Tenant \u2693\ufe0e There are 2 ways for LightHouse enablement on the Service Provider Tenant: 1. Publish a Azure marketplace managed service offer - and the customer can subscribe to this offer through Azure marketplace. 2. Create ARM template - which will then be run on the client subscription to become a Light House customer for the SP. For Kyndryl accounts, #2 will be used, as Kyndryl is still in the process of meeting the pre-reqs to publish any offers in Azure marketplace There are 4 elements that need to be defined in the ARM template: Azure Role(s) that will be used to manage the customer Azure AD(AAD) Groups associated with the defined Azure Role. Though users also can be associated with the roles, this will be an administrative overhead and hence should NOT be used for Kyndryl managed clients. Unique Service Provider Offer Name Azure Active Directory Tenant ID of the Service Provider Step 1 - Define Access Control/RBAC Roles for Service Provider \u2693\ufe0e It is important to define the Azure Roles & Azure AD(AAD) Groups before enablement since each time a new role needs to be added, customer has to be removed from Light House and enablement steps repeated for each customer subscription. Kyndryl Use Case for RBAC \u2693\ufe0e There will be various Kyndryl support teams performing different operations for a customer. An AAD group has to defined for each of the support teams, with associated AZ roles. These groups can be customer specific (recommended) OR can be shared groups to manage multiple customers, depending on the delivery team model and client needs. For Kyndryl managed clients, roles defined in Azure Solution guidance for Landing Zones should be followed, which means AAD groups need to be created for each of the recommended roles (except Platform Admin group) However, for this example, let's assume that there will be 2 Roles defined to manage account named \"mcm\". Account Name: mcm (this is the 3 char BlueID account code assigned to the account) Roles required to manage this account: Contributor - who can perform all actions on \"mcm\" customer except user administration Reader - who can perform read actions for all resources for the customer. Step 2 - Create AAD Groups \u2693\ufe0e Based on above, two AAD groups will have to be defined in the SP tenant, following this naming standard below, where \"acc\" is the 3 char account code. mcm_lighthouse_contributors . mcm_lighthouse_readers . IMPORTANT: a) There is NO need to assign any Azure role(s) to the above groups. Roles will be assigned in the ARM template for each customer, by each customer. b) Since no roles are assigned here, users added to this group will NOT have any privileges on the SP subscription, but only on customer subscriptions ( FOR DISCUSSION WITH ALWYN ) Create other AAD groups as needed for your accounts, ensuring that the group name includes the Azure role name in the 3 rd identifier. This is to make easier RBAC management. Step 2a - Add Users under the above groups \u2693\ufe0e Add users to appropriate respective groups based on their roles to be performed. For example an user performing a \"contributor\" role should be added to the mcm_lighthouse_contributors AAD group. Step 3 - Define Service Provide Offer Name \u2693\ufe0e Any \"unique\" name can be used to define Service Provider offer. However, Kyndryl managed environments should follow the below naming standard. This naming standard will allow the customer to identify who is their SP, as well as help Kndryl teams to identify which Azure instance the customer is being managed from. SP Offer Name Standard : Company-DeliveryOrg-region-Instance/TenantReference . Example: kyndryl-mcms-na-inst001 Note that there will be only one SP Offer in a SP Tenant. Though there is no technical limitation, there is no need to define more than one SP offer in a SP tenant. Step 4 - Identify Tenant ID \u2693\ufe0e After login to Azure portal for SP, open cloud shell window and run the following command to run the identify the Tenant ID: az account show , which will list the Tenant ID. NOTE : this step is not needed when the template is created through portal UI, but given here for reference. Step 5 - Create ARM Template \u2693\ufe0e Microsoft provided sample ARM templates can be used with above details inserted. The same template can also be created through the UI, and UI is recommended, for ease of use. From the Azure Light House portal navigate to Azure Light House \u2192 Manage your customers \u2192 Create ARM Template and make the following choices as shown below: Select \"Add Authorization\" , and define the roles as follows: NOTE: Principal ID : Select the above AAD group defined for contributor role (mcm_lighthouse_contributor). Display Name : Select/Type AAD group name here(mcm_lighthouse_contributors). Though anyname can be specified, it is recommended to follow the AAD group name for easier identification and management through LightHouse UI. Role : Select \" Contributor \" Role. Add Additional Roles . Repeat \"Add Authorization\" , to define the \"Resource Policy Contributor, reader\" roles, against respective groups as shown below: After above step choose \" View template \" and download the template. IMPORTANT : When the customer runs this template on a specific subscription, above defined roles are granted for the respective subscription(s) to the SP. Refer this template created from the above example. All the above constructs (Tenant ID, SP Offer Name, Authorization Roles) can be seen in the ARM template: Defining additional roles & AAD groups in ARM template \u2693\ufe0e Though the above method using Azure portal can be followed for adding new roles to an existing customer, OR to add new customers, the same can be accomplished by updating an existing template. For example, to add a new customer (\"abc\") on the same SP tenant, update only the sections as marked below: a. principalIDDisplayName Create an AAD group with the name as defined \"abc_ lighthouse_ contributors\". b. principalID : Identify the object ID of the AAD group thru UI or CLI (az ad group show --group \"abc_ lighthouse_ contributors\" --query \"objectId\") and update the value below. No need to change any other entries, unless new roles need to be added. If a new role need to be added for example, \"Backup Contributor\" role, then define a new AAD group, identify the object ID using above command, as well as identify the Role Definition object ID (az role definition list --name \"Backup Contributor\" | grep name) and define the \"roleDefinitionID\" entry with this value. Refer Microsoft document for additional details on creating/updating ARM templates Client Onboarding/Enablement - on customer Tenant \u2693\ufe0e Pre-Requisite : Customer need to be made aware of the LightHouse model, and that the customer will be granting all defined \"roles\" to Kyndryl by running the ARM template. (NOTE: Client Justification PPT/DOC to be created in Sprint 3) After the above is completed, send the above template to the customer with below instructions: 1) Login to Azure portal. 2) This template should be run by a customer user with \"Owner\" privilege to subscription. 3) Copy(drag&drop) the template to Azure Cloud Shell. 4) On cloud shell, run \"az account set --subscription subscription ID (eg.4b4ea0d2-145d-4716-b701-8419d8a1f002)\", where the subscription ID should be the ID of the subscription which should be managed by Kyndryl. 5) Validate CLI/Cloud shell subscription context using \"az account show\", which should list the subscription for which delegation is being granted to Kyndryl. 6) Run the following command: az deployment sub create --name somename --location somedestination --template-file ARMTemplateFileName.json --verbose. 7) Repeat steps #4 to #7 for each client subscription that should be managed by Kyndryl. Validation - on customer Tenant \u2693\ufe0e Navigate to Azure Light House \u2192 View Service Provider Offers \u2192 Service Provider Offers, you should see service provide offer, similar to screen shot below. There will be a row for each subscription Delegations section shows additional details including Subscription names & Role assignments: Validation - on SP Tenant \u2693\ufe0e It may take a few mins for the customers to appear on the SP portal, as shown below: Troubleshooting \u2693\ufe0e If the customer is not visible on the SP tenant, the most common problem is to select the subscriptions in your profile. Though this should happen automatically, most times it does not. Select your user profile \u2192 Switch directory \u2192 and select all subscriptions under \"Current + delegated directories\": Repeat the same(select all subscriptions) from \"Subscriptions\" pull down beneath the above menu Continue to Light House Admin Section... Delegating Management Groups vs Subscriptions \u2693\ufe0e Instead or on-boarding every subscription from a client, It is possible to delegate a Management Group on the client side. For this to work, a management group should be created on customer tenant - with all the Kyndryl managed subscriptions be defined under this group. And then delegate the management group to the Service Provider. Refer Microsoft docs on how to achieve this. However, there may be challenges with this approach, since Kyndryl managed subscriptions, in most cases, are already be part of another management group designed & defined for other business reasons by the client. Hence use this model as appropriate. Using LightHouse with Azure AAD PIM (Preview) \u2693\ufe0e Some customers may not be comfortable with LightHouse's current model whereby anyone from the SP can perform any action on the client subscriptions without any controls, after the initial delegation. Microsoft recently announced the public preview of Azure Lighthouse support for Azure AAD PIM . This integration enforces the the principle of least privilege through through just-enough and just-in-time (JIT) access model, facilitated by Azure AD PIM. From a Kyndryl perspective, PIM (JIT, Just Enough) has to be studied at the broader level beyond Light House, which will be addressed by the security competency.","title":"LightHouse Enablement"},{"location":"automation/lighthouse/lh-enableguide/#azure-lighthouse-enablement-guide","text":"","title":"Azure LightHouse Enablement Guide"},{"location":"automation/lighthouse/lh-enableguide/#azure-lighthouse-overview","text":"Azure Lighthouse is designed to provide a secure, granular way for Service Providers(SP) to gain single token access to customer environments using the principles of least privilege to help easily manage multiple customer environments and automate management tasks at scale. Azure LightHouse helps service providers like Kyndryl in effective governance management across customers.","title":"Azure LightHouse Overview"},{"location":"automation/lighthouse/lh-enableguide/#azure-lighthouse-concept","text":"Light House allows authorized users in one Azure Active Directory (Azure AD) tenant, referred as Service Provider(SP) , perform management operations across different Azure AD tenants belonging to different clients, referred as Customer/Managed Customer Azure Light House operates on the concept of \u201c Delegated Resource Management \u201d which enables logical projection of resources from one Azure tenant onto another tenant. This concept creates logical control plane access for the service provider onto customer resources thereby providing the service provider ability to view and manage custome resources Customer resources to be projected can be at subscription and/or Resource Group level only and NOT at the customer tenant level. (Delegation at Management Group has been introduced - Read Note below) **How it works-Overview **: When LightHouse is \"enabled\" on a customer subscription, specifying the managed service provider's Azure AD tenant details, Azure backbone framework establishes connectivity between SP & customer resources, using VNET and other services which is transparent and not visible to both the service provider and the customer. Once connectivity is established, customer resources appear in the Light House menu as \"customers\". Type of access for the SP is defined during the enablement process. It is important to note that client has visibility, as well as can control what roles (Azure Roles) are granted to the SP. However the client may not have the ability to control which user from the SP, has access to their resources (explained further in Access Control/RBAC section) Also important to note is the fact that there are no Users/Groups need to be defined for SP on the customer Tenant, but only \"Azure Roles\" are assigned to the SP \"Owner\" Role cannot be assigned to a SP - this is by design and cannot be overridden. This enablement step has to performed on every subscription a customer has. ** Cost**: Light House capability is included by default in every subscription. Any subscription can be used as a Service Provider(SP) or customer, at no cost! More details on Azure Light House","title":"Azure LightHouse Concept"},{"location":"automation/lighthouse/lh-enableguide/#enablement-steps-on-service-providersp-tenant","text":"There are 2 ways for LightHouse enablement on the Service Provider Tenant: 1. Publish a Azure marketplace managed service offer - and the customer can subscribe to this offer through Azure marketplace. 2. Create ARM template - which will then be run on the client subscription to become a Light House customer for the SP. For Kyndryl accounts, #2 will be used, as Kyndryl is still in the process of meeting the pre-reqs to publish any offers in Azure marketplace There are 4 elements that need to be defined in the ARM template: Azure Role(s) that will be used to manage the customer Azure AD(AAD) Groups associated with the defined Azure Role. Though users also can be associated with the roles, this will be an administrative overhead and hence should NOT be used for Kyndryl managed clients. Unique Service Provider Offer Name Azure Active Directory Tenant ID of the Service Provider","title":"Enablement Steps - on Service Provider(SP) Tenant"},{"location":"automation/lighthouse/lh-enableguide/#step-1-define-access-controlrbac-roles-for-service-provider","text":"It is important to define the Azure Roles & Azure AD(AAD) Groups before enablement since each time a new role needs to be added, customer has to be removed from Light House and enablement steps repeated for each customer subscription.","title":"Step 1 - Define Access Control/RBAC Roles for Service Provider"},{"location":"automation/lighthouse/lh-enableguide/#kyndryl-use-case-for-rbac","text":"There will be various Kyndryl support teams performing different operations for a customer. An AAD group has to defined for each of the support teams, with associated AZ roles. These groups can be customer specific (recommended) OR can be shared groups to manage multiple customers, depending on the delivery team model and client needs. For Kyndryl managed clients, roles defined in Azure Solution guidance for Landing Zones should be followed, which means AAD groups need to be created for each of the recommended roles (except Platform Admin group) However, for this example, let's assume that there will be 2 Roles defined to manage account named \"mcm\". Account Name: mcm (this is the 3 char BlueID account code assigned to the account) Roles required to manage this account: Contributor - who can perform all actions on \"mcm\" customer except user administration Reader - who can perform read actions for all resources for the customer.","title":"Kyndryl Use Case for RBAC"},{"location":"automation/lighthouse/lh-enableguide/#step-2-create-aad-groups","text":"Based on above, two AAD groups will have to be defined in the SP tenant, following this naming standard below, where \"acc\" is the 3 char account code. mcm_lighthouse_contributors . mcm_lighthouse_readers . IMPORTANT: a) There is NO need to assign any Azure role(s) to the above groups. Roles will be assigned in the ARM template for each customer, by each customer. b) Since no roles are assigned here, users added to this group will NOT have any privileges on the SP subscription, but only on customer subscriptions ( FOR DISCUSSION WITH ALWYN ) Create other AAD groups as needed for your accounts, ensuring that the group name includes the Azure role name in the 3 rd identifier. This is to make easier RBAC management.","title":"Step 2 - Create AAD Groups"},{"location":"automation/lighthouse/lh-enableguide/#step-2a-add-users-under-the-above-groups","text":"Add users to appropriate respective groups based on their roles to be performed. For example an user performing a \"contributor\" role should be added to the mcm_lighthouse_contributors AAD group.","title":"Step 2a - Add Users under the above groups"},{"location":"automation/lighthouse/lh-enableguide/#step-3-define-service-provide-offer-name","text":"Any \"unique\" name can be used to define Service Provider offer. However, Kyndryl managed environments should follow the below naming standard. This naming standard will allow the customer to identify who is their SP, as well as help Kndryl teams to identify which Azure instance the customer is being managed from. SP Offer Name Standard : Company-DeliveryOrg-region-Instance/TenantReference . Example: kyndryl-mcms-na-inst001 Note that there will be only one SP Offer in a SP Tenant. Though there is no technical limitation, there is no need to define more than one SP offer in a SP tenant.","title":"Step 3 - Define Service Provide Offer Name"},{"location":"automation/lighthouse/lh-enableguide/#step-4-identify-tenant-id","text":"After login to Azure portal for SP, open cloud shell window and run the following command to run the identify the Tenant ID: az account show , which will list the Tenant ID. NOTE : this step is not needed when the template is created through portal UI, but given here for reference.","title":"Step 4 - Identify Tenant ID"},{"location":"automation/lighthouse/lh-enableguide/#step-5-create-arm-template","text":"Microsoft provided sample ARM templates can be used with above details inserted. The same template can also be created through the UI, and UI is recommended, for ease of use. From the Azure Light House portal navigate to Azure Light House \u2192 Manage your customers \u2192 Create ARM Template and make the following choices as shown below: Select \"Add Authorization\" , and define the roles as follows: NOTE: Principal ID : Select the above AAD group defined for contributor role (mcm_lighthouse_contributor). Display Name : Select/Type AAD group name here(mcm_lighthouse_contributors). Though anyname can be specified, it is recommended to follow the AAD group name for easier identification and management through LightHouse UI. Role : Select \" Contributor \" Role. Add Additional Roles . Repeat \"Add Authorization\" , to define the \"Resource Policy Contributor, reader\" roles, against respective groups as shown below: After above step choose \" View template \" and download the template. IMPORTANT : When the customer runs this template on a specific subscription, above defined roles are granted for the respective subscription(s) to the SP. Refer this template created from the above example. All the above constructs (Tenant ID, SP Offer Name, Authorization Roles) can be seen in the ARM template:","title":"Step 5 - Create ARM Template"},{"location":"automation/lighthouse/lh-enableguide/#defining-additional-roles-aad-groups-in-arm-template","text":"Though the above method using Azure portal can be followed for adding new roles to an existing customer, OR to add new customers, the same can be accomplished by updating an existing template. For example, to add a new customer (\"abc\") on the same SP tenant, update only the sections as marked below: a. principalIDDisplayName Create an AAD group with the name as defined \"abc_ lighthouse_ contributors\". b. principalID : Identify the object ID of the AAD group thru UI or CLI (az ad group show --group \"abc_ lighthouse_ contributors\" --query \"objectId\") and update the value below. No need to change any other entries, unless new roles need to be added. If a new role need to be added for example, \"Backup Contributor\" role, then define a new AAD group, identify the object ID using above command, as well as identify the Role Definition object ID (az role definition list --name \"Backup Contributor\" | grep name) and define the \"roleDefinitionID\" entry with this value. Refer Microsoft document for additional details on creating/updating ARM templates","title":"Defining additional roles &amp; AAD groups in ARM template"},{"location":"automation/lighthouse/lh-enableguide/#client-onboardingenablement-on-customer-tenant","text":"Pre-Requisite : Customer need to be made aware of the LightHouse model, and that the customer will be granting all defined \"roles\" to Kyndryl by running the ARM template. (NOTE: Client Justification PPT/DOC to be created in Sprint 3) After the above is completed, send the above template to the customer with below instructions: 1) Login to Azure portal. 2) This template should be run by a customer user with \"Owner\" privilege to subscription. 3) Copy(drag&drop) the template to Azure Cloud Shell. 4) On cloud shell, run \"az account set --subscription subscription ID (eg.4b4ea0d2-145d-4716-b701-8419d8a1f002)\", where the subscription ID should be the ID of the subscription which should be managed by Kyndryl. 5) Validate CLI/Cloud shell subscription context using \"az account show\", which should list the subscription for which delegation is being granted to Kyndryl. 6) Run the following command: az deployment sub create --name somename --location somedestination --template-file ARMTemplateFileName.json --verbose. 7) Repeat steps #4 to #7 for each client subscription that should be managed by Kyndryl.","title":"Client Onboarding/Enablement - on customer Tenant"},{"location":"automation/lighthouse/lh-enableguide/#validation-on-customer-tenant","text":"Navigate to Azure Light House \u2192 View Service Provider Offers \u2192 Service Provider Offers, you should see service provide offer, similar to screen shot below. There will be a row for each subscription Delegations section shows additional details including Subscription names & Role assignments:","title":"Validation - on customer Tenant"},{"location":"automation/lighthouse/lh-enableguide/#validation-on-sp-tenant","text":"It may take a few mins for the customers to appear on the SP portal, as shown below:","title":"Validation - on SP Tenant"},{"location":"automation/lighthouse/lh-enableguide/#troubleshooting","text":"If the customer is not visible on the SP tenant, the most common problem is to select the subscriptions in your profile. Though this should happen automatically, most times it does not. Select your user profile \u2192 Switch directory \u2192 and select all subscriptions under \"Current + delegated directories\": Repeat the same(select all subscriptions) from \"Subscriptions\" pull down beneath the above menu Continue to Light House Admin Section...","title":"Troubleshooting"},{"location":"automation/lighthouse/lh-enableguide/#delegating-management-groups-vs-subscriptions","text":"Instead or on-boarding every subscription from a client, It is possible to delegate a Management Group on the client side. For this to work, a management group should be created on customer tenant - with all the Kyndryl managed subscriptions be defined under this group. And then delegate the management group to the Service Provider. Refer Microsoft docs on how to achieve this. However, there may be challenges with this approach, since Kyndryl managed subscriptions, in most cases, are already be part of another management group designed & defined for other business reasons by the client. Hence use this model as appropriate.","title":"Delegating Management Groups vs Subscriptions"},{"location":"automation/lighthouse/lh-enableguide/#using-lighthouse-with-azure-aad-pim-preview","text":"Some customers may not be comfortable with LightHouse's current model whereby anyone from the SP can perform any action on the client subscriptions without any controls, after the initial delegation. Microsoft recently announced the public preview of Azure Lighthouse support for Azure AAD PIM . This integration enforces the the principle of least privilege through through just-enough and just-in-time (JIT) access model, facilitated by Azure AD PIM. From a Kyndryl perspective, PIM (JIT, Just Enough) has to be studied at the broader level beyond Light House, which will be addressed by the security competency.","title":"Using LightHouse with Azure AAD PIM (Preview)"},{"location":"automation/lighthouse/lh-enablement/","text":"Azure LightHouse PoC Goals: \u2693\ufe0e 1.Define Standards at SP (including Naming). 2.Define Standards at Client (if any). 3.Define RBAC at SP & Client. - Address Kyndryl Users or Groups for RBAC. 4. How to use LH for Day 2 Operations. 5. Automation for onboarding. Azure LightHouse PoC Environment \u2693\ufe0e CoE Tenant as Service Provider (SP). MCMS Subscriptions as Client/Customers. Link to CMP-Light House Box Folder . Box Folder Contents : 1) LightHouse Overview & PoC Overview updates (in ppt). 2) ARM Template for CoE SP. ARM Template : 3) Sample ARM Templates: https://github.com/Azure/Azure-Lighthouse-samples/tree/master/templates/delegated-resource-management/subscription Above template can be run on any subscription using Azure CloudShell/Powershell : (after dragging and dropping the ARM template file to cloud shell) \"az deployment sub create --name lhdeployment1 --location westus3 --template-file coe-lh-template.json --verbose\" Use Cases \u2693\ufe0e 1.Applying Policies against a customer (sam_policy). 2.Dashboards/Workbooks for common view across customers \u2013 Cost Mgmt, Sec Center, etc. 3.Creating/Applying Monitoring Metrics & Actions from SP to customers. 3b. Update Management 3c. Advisor 3d. Service Health 4.Executing LZ/Platform Policies creation using ARM template. 5.Role Definitions - based on LZ Design as well as above. 6.Standards \u2013 AD Group Names/Offer Names etc. 7. Backup 7. For DISCUSSION - Cascading Ansible/CACF Proxy on client account, enabling use of System Managed Identities vs Automation user IDs (OUTSIDE THIS SCOPE - for Alwyn's followup) Sprint 1 (ending Sep 17 th ) \u2693\ufe0e 1) Setup LAB environment. 2) Test Applying policies from SP env to Customer Environment. 3) Test Applying Monitoring Metrics/Actions from SP to customer. 4) Create document on LH Conceptual level doc (pre-enablement). COMPLETED/PUBLISHED Sprint 2 (Sep 20 - Oct 1 st ) \u2693\ufe0e 1) Test Customer Landing Zone Creation through ARM templates from SP. 2) Update Mgmt 3) Define RBAC needed for each LH customer. 4) Finalize LH SP Offer template. 5) Publish Enablement & User Guide - v1 6) LH Enablement pitch/procedure/justification for clients - TENTATIVE. 7) AAD Management. INPROGRESS Sprint 3 \u2693\ufe0e 1) TBD Followups: \u2693\ufe0e Support Ticket Billing. Action Rule - in Client Subscriptions. Action Groups - Actions to be performed by Kyndryl - AG on SP subscription (NOT NEEDED). Actions to be performed by client - AG on Client Subscription. Suppression Rules - seperate rules. Availability Monitoring / heartbeats & suppression.","title":"Lh enablement"},{"location":"automation/lighthouse/lh-enablement/#azure-lighthouse-poc-goals","text":"1.Define Standards at SP (including Naming). 2.Define Standards at Client (if any). 3.Define RBAC at SP & Client. - Address Kyndryl Users or Groups for RBAC. 4. How to use LH for Day 2 Operations. 5. Automation for onboarding.","title":"Azure LightHouse PoC Goals:"},{"location":"automation/lighthouse/lh-enablement/#azure-lighthouse-poc-environment","text":"CoE Tenant as Service Provider (SP). MCMS Subscriptions as Client/Customers. Link to CMP-Light House Box Folder . Box Folder Contents : 1) LightHouse Overview & PoC Overview updates (in ppt). 2) ARM Template for CoE SP. ARM Template : 3) Sample ARM Templates: https://github.com/Azure/Azure-Lighthouse-samples/tree/master/templates/delegated-resource-management/subscription Above template can be run on any subscription using Azure CloudShell/Powershell : (after dragging and dropping the ARM template file to cloud shell) \"az deployment sub create --name lhdeployment1 --location westus3 --template-file coe-lh-template.json --verbose\"","title":"Azure LightHouse PoC Environment"},{"location":"automation/lighthouse/lh-enablement/#use-cases","text":"1.Applying Policies against a customer (sam_policy). 2.Dashboards/Workbooks for common view across customers \u2013 Cost Mgmt, Sec Center, etc. 3.Creating/Applying Monitoring Metrics & Actions from SP to customers. 3b. Update Management 3c. Advisor 3d. Service Health 4.Executing LZ/Platform Policies creation using ARM template. 5.Role Definitions - based on LZ Design as well as above. 6.Standards \u2013 AD Group Names/Offer Names etc. 7. Backup 7. For DISCUSSION - Cascading Ansible/CACF Proxy on client account, enabling use of System Managed Identities vs Automation user IDs (OUTSIDE THIS SCOPE - for Alwyn's followup)","title":"Use Cases"},{"location":"automation/lighthouse/lh-enablement/#sprint-1-ending-sep-17th","text":"1) Setup LAB environment. 2) Test Applying policies from SP env to Customer Environment. 3) Test Applying Monitoring Metrics/Actions from SP to customer. 4) Create document on LH Conceptual level doc (pre-enablement). COMPLETED/PUBLISHED","title":"Sprint 1  (ending Sep 17th)"},{"location":"automation/lighthouse/lh-enablement/#sprint-2-sep-20-oct-1st","text":"1) Test Customer Landing Zone Creation through ARM templates from SP. 2) Update Mgmt 3) Define RBAC needed for each LH customer. 4) Finalize LH SP Offer template. 5) Publish Enablement & User Guide - v1 6) LH Enablement pitch/procedure/justification for clients - TENTATIVE. 7) AAD Management. INPROGRESS","title":"Sprint 2 (Sep 20 - Oct 1st)"},{"location":"automation/lighthouse/lh-enablement/#sprint-3","text":"1) TBD","title":"Sprint 3"},{"location":"automation/lighthouse/lh-enablement/#followups","text":"Support Ticket Billing. Action Rule - in Client Subscriptions. Action Groups - Actions to be performed by Kyndryl - AG on SP subscription (NOT NEEDED). Actions to be performed by client - AG on Client Subscription. Suppression Rules - seperate rules. Availability Monitoring / heartbeats & suppression.","title":"Followups:"},{"location":"home/prereq-learning/","text":"Pre-Requisite Learning \u2693\ufe0e Mandatory Learning \u2693\ufe0e Readers of this document are expected to have basic knowledge of Azure and should have completed below learning: Microsoft Azure concepts Microsft Azure terminologies Azure fundamentals Azure acronyms . Five R\u2019s of Cloud Rationalization Optional Learning \u2693\ufe0e Below optional learning are not mandatory but recommended for solution architects which will enrich their approach to solution design on Azure Microsoft's Cloud Adoption Framework/CAF Microsoft's Well-Architected Framework","title":"Pre-Requisite Learning"},{"location":"home/prereq-learning/#pre-requisite-learning","text":"","title":"Pre-Requisite Learning"},{"location":"home/prereq-learning/#mandatory-learning","text":"Readers of this document are expected to have basic knowledge of Azure and should have completed below learning: Microsoft Azure concepts Microsft Azure terminologies Azure fundamentals Azure acronyms . Five R\u2019s of Cloud Rationalization","title":"Mandatory Learning"},{"location":"home/prereq-learning/#optional-learning","text":"Below optional learning are not mandatory but recommended for solution architects which will enrich their approach to solution design on Azure Microsoft's Cloud Adoption Framework/CAF Microsoft's Well-Architected Framework","title":"Optional Learning"},{"location":"home/purpose/","text":"purpose \u2693\ufe0e","title":"purpose"},{"location":"home/purpose/#purpose","text":"","title":"purpose"},{"location":"home/scope/","text":"Scope \u2693\ufe0e Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload . What is not covered \u2693\ufe0e This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"Scope"},{"location":"home/scope/#scope","text":"Scope of this initial release is limited to the following: How to define a cloud adoption strategy & plan for the client. How to design a landing zone . Applying the landing zone design principles against a sample workload .","title":"Scope"},{"location":"home/scope/#what-is-not-covered","text":"This release of the document does not cover the below topics which will be included in future releases: 1. Platform aspects related to container services such as ARO, Service Fabric etc. 2. Platform aspects related to Private cloud \u2013 Azure VMware Solution. 3. Solution guidance on Hybrid cloud services \u2013 Azure stack, Azure stack Hub, Azure stack edge, Azure stack HCI. 4. Kyndryl does not have a CSP agreement yet and hence solution guidance for CSP model is not covered in this release. 5. Automation of policies for security and enterprise governance. 6. Disaster Recovery(DR) & Capacity Planning","title":"What is not covered"},{"location":"lzdesign/bcdr/","text":"Business Continuity & Disaster Recovery \u2693\ufe0e This section will cover Backup and Restore solutions. Disaster Recovery will be covered in the next phase of this design effort. Azure Backup is a cloud-based data protection solution for Azure cloud resources as well as for on-premises systems like VMs and databases. This new solution is reliable, secure, and cost competitive. Azure offers backup support that ranges from \u201ctypical\u201d Windows or Linux machines to fine-grained protection for Exchange, SQL, or SharePoint services. You can backup Hyper-V, VMWare or even capture system state and do a bare-metal recovery if needed. It can backup files, folders, system state, disks. For further details pls refer Azure and Backup and Restore services Backup and Restore Use cases \u2693\ufe0e Backup Azure VMs \u2013 both for windows and Linux VMs Backup Files and folders \u2013 both for windows and Linux VMs Backup Databases \u2013 SQL Server, DB2, Oracle, MySQL etc running on VMs Backup of Keys and secrets Policy based backup configuration. Support archival (long term retention) backup. Support Azure File share backup Support on-premises VM backup to cloud Support offline bulk data import to cloud. Recover a VM from backup Restoration of data on the same or different VMs Support partial recovery of volume/disk. Restore files and folders. Restore databases. Provide recovery audit logs. Backup monitoring: Notify backup failures. Backup reporting a. Compliance reporting b. Success & failure reporting c. Backup dashboard Support backup scheduling Backup performance \u2013 should be able to complete the backup within a given window. Backup SLA - Success rate is 98% of the time Azure Backup architecture \u2693\ufe0e The high-level Azure backup solution architecture is as below. Workloads \u2693\ufe0e Two types of workloads are supported by this solution. a) On-Premises workloads - VMs and Bare metal servers b) Cloud native workload - VMs, SAP HANA, SQL in Azure VMs and Azure File Data Plane \u2693\ufe0e \u00b7 Automated storage management \u2013 Azure Backup automates provisioning and managing storage accounts for the backup data to ensure it scales as the backup data grows. \u00b7 Malicious delete protection \u2013 Protect against any accidental and malicious attempts for deleting your backups via soft delete of backups. The deleted backup data is stored for 14 days free of charge and allows it to be recovered from this state. \u00b7 Secure encrypted backups - Azure Backup ensures your backup data is stored in a secure manner, leveraging built-in security capabilities of the Azure platform like Azure RBAC and Encryption. \u00b7 Backup data lifecycle management - Azure Backup automatically cleans up older backup data to comply with the retention policies. You can also tier your data from operational storage to vault storage. Management Plane \u2693\ufe0e \u00b7 Access control \u2013 Vaults (Recovery Services and Backup vaults) provide the management capabilities and are accessible via the Azure portal, Backup Center, Vault dashboards, SDK, CLI, and even REST APIs. It's also an Azure RBAC boundary, providing the option to restrict access to backups only to authorized Backup Admins. \u00b7 Policy management \u2013 Azure Backup Policies within each vault define when the backups should be triggered and how long they need to be retained. You can also manage these policies and apply them across multiple items. \u00b7 Monitoring and Reporting \u2013 Azure Backup integrates with Log Analytics and provides the ability to see reports via Workbooks as well. \u00b7 Snapshot management \u2013 Azure Backup takes snapshots for some Azure native workloads (VMs and Azure Files), manages these snapshots and allows fast restores from them. This option drastically reduces the time to recover your data to the original storage. Solution Guidance \u2693\ufe0e This sub-section provides guidance at a high level on the key design considerations and best practices for creating a backup solution using the above described architecture. Design considerations \u2693\ufe0e Azure offers wide range of backup options. Your backup strategy will differ depending on the workload you need to protect. There are constraints and limits. Refer the support matrix to make sure the selected option meets the target workload backup and restore requirements. Backups can be stored in the vault or in the storage account. How much storage space is needed? Select the option based on reliability, data encryption, cost, security, recovery objectives (RPO, RTO) and operational ease of use. Some industries are bound by laws and regulations or market trends to ensure the high availability and disaster recovery of their service. They need to be ready to recover data and applications in an orchestrated manner if a critical outage takes place at a primary location. Number of copies of backups and the geo location. Company policies or regulations may dictate that backup need to be available both in the primary location and in a secondary DR location. If you are backing up on-premises data to cloud, network bandwidth is an important criterion. How much bandwidth would be required to back up the data to Azure? Backup costs. The backup charges for Azure VMs and on-premises servers can be summarized as follows. Size of each instance Azure backup price per month Instance < or = 50 GB $5 + storage consumed Instance is > 50 but < or = 500 GB $10 + storage consumed Instance > 500 GB $10 for each 500 GB increment + storage consumed Backup Performance and Backup Time. The acceptable timeframe for backup-related tasks, performance expectations and recovery point objectives Backup policy consideration. This includes backup schedule, frequency, start time and retention settings. 10.Security considerations. This includes access control, integrity of the data, confidentiality, availability assurances against attacks and abuse of your valuable data and systems**.** Monitoring and Alerting considerations. Backup data encryption using user provided encryption keys. Design Actions by Solution Architects \u2693\ufe0e 1. Define Backup solutions \u2693\ufe0e Following backup solutions are recommended for Azure Resources. Backup Use case Description Backup Option Azure VMs \u2013 Windows Back up the entire VM Azure Recovery Services Vault. Azure VMs \u2013 Windows Backup specific files/folders/volume. Azure Recovery Services Vault. Azure VMs \u2013 Linux Back up the entire VM Azure Recovery Services Vault. Azure VMs \u2013 Linux Backup specific files/folders/volume. Azure Backup Server MABS/Backup server (basically a scaled down version of DPM) is setup on a Windows VM, and the workload VMs with DPM/backup agent on it \u2013 sends the backup data to Azure backup serv-er/MABS, and the MABS server in-turn stores the backup data onto Azure Recovery Services Vault Storage. SQL Server running on VM Workload aware backups that support all backup types - full, differential, and log Azure Backup Service for SQL Server Azure Backup service installs a workload backup extension on the VM by the name AzureBackupWindowsWorkload extension and backup the database to the recovery vault Disk Backup Disk snapshot Azure Disk Backup. It is basically a disk snapshot service that provides snapshot lifecycle management for managed disks by automating periodic creation of snapshots and retaining it for configured duration using backup policy. File share backup Backup the file shares Azure File share backup Its syncs the files in the File share with Azure Recovery vault. Offline data backup Transfer large amounts of data to cloud storage Azure Data box Key and Secret backup Securely backup the keys and secretes stored in Azure Key Vault Azure Backup service does not provide a capability to backup the keys and secrets from Key Vault. The recommended approach is a) Backup the keys to a storage account using Key Vault backup option. b) Backup the data from the storage account to the vault using Azure backup service. 2. Define Backup Monitoring \u2693\ufe0e As per GTS/Kyndryl Backup and Restore process, backup failure must be monitored and notified to the operations team. Azure Backup Center provides the single pane of glass to monitor and manage backup across a large and distributed Azure environment. You can use Backup Center to efficiently manage backups spanning multiple workload types, vaults, subscriptions, regions, customer tenants. The backup monitoring and notification service flow is depicted as below. \u00b7 Approved Backup policy is applied to the centralized backup service \u00b7 The Centralized backup service backups the cloud resources present inside the given Organization/Subscription/Account as per the policy \u00b7 Centralized backup service leverages the cloud vault to store the backups. \u00b7 Centralized Backup Center generates the logs for the backup jobs \u00b7 Configure the centralized Log Analytics workspace to pull the backup logs. \u00b7 Configure Azure Monitor to generate alerts from backup logs from the Log Analytics workspace. \u00b7 Send the alerts to cut an incident ticket in the ITSM tool. \u00b7 Based on the ticket, the platform SRE investigates the problem and resolves it 3. Define Backup Reporting \u2693\ufe0e It is recommended to have a central backup console/single pane of glass across a large Azure environment. Azure provides a native service called Backup Center , which provides consolidated reporting dashboard across regions, subscriptions, and tenants. Configure the Backup Center to do the following. \u00b7 View Azure Policies for backup \u00b7 View compliance of your resources as per the backup policy. \u00b7 View all non-compliant resources as per the backup policy. \u00b7 View backup failure report (backup job status) \u00b7 Auditing of backups and restores. \u00b7 Forecasting of cloud storage consumed. 4. Establish plan for test Restoration of data \u2693\ufe0e If backup is enabled for any protected resource or data type, then it can be restored from the backup service. It is recommended to periodically test recovery of a resource to make sure it can be restored as per the design. There are several ways a VM, database, file or a folder can be restored from the backup. We recommend the following - Automated Recovery : Put in a place a recovery process/workflow to recover a resource as per the application/business requirement. Automated workflow should use Azure native APIs to recover the data or resource from the recovery vault or from the backup server. 4.2 Manual Recovery : It is possible to manually recover the Azure resources like VM, databases and the data from the recovery vault. It is recommended to use Backup Center to recover the data, resources from the configured vault. You can restore your workloads regardless of your location or subscription from the centralized Backup Center.","title":"7.Backup & DR"},{"location":"lzdesign/bcdr/#business-continuity-disaster-recovery","text":"This section will cover Backup and Restore solutions. Disaster Recovery will be covered in the next phase of this design effort. Azure Backup is a cloud-based data protection solution for Azure cloud resources as well as for on-premises systems like VMs and databases. This new solution is reliable, secure, and cost competitive. Azure offers backup support that ranges from \u201ctypical\u201d Windows or Linux machines to fine-grained protection for Exchange, SQL, or SharePoint services. You can backup Hyper-V, VMWare or even capture system state and do a bare-metal recovery if needed. It can backup files, folders, system state, disks. For further details pls refer Azure and Backup and Restore services","title":"Business Continuity &amp; Disaster Recovery"},{"location":"lzdesign/bcdr/#backup-and-restore-use-cases","text":"Backup Azure VMs \u2013 both for windows and Linux VMs Backup Files and folders \u2013 both for windows and Linux VMs Backup Databases \u2013 SQL Server, DB2, Oracle, MySQL etc running on VMs Backup of Keys and secrets Policy based backup configuration. Support archival (long term retention) backup. Support Azure File share backup Support on-premises VM backup to cloud Support offline bulk data import to cloud. Recover a VM from backup Restoration of data on the same or different VMs Support partial recovery of volume/disk. Restore files and folders. Restore databases. Provide recovery audit logs. Backup monitoring: Notify backup failures. Backup reporting a. Compliance reporting b. Success & failure reporting c. Backup dashboard Support backup scheduling Backup performance \u2013 should be able to complete the backup within a given window. Backup SLA - Success rate is 98% of the time","title":"Backup and Restore Use cases"},{"location":"lzdesign/bcdr/#azure-backup-architecture","text":"The high-level Azure backup solution architecture is as below.","title":"Azure Backup architecture"},{"location":"lzdesign/bcdr/#workloads","text":"Two types of workloads are supported by this solution. a) On-Premises workloads - VMs and Bare metal servers b) Cloud native workload - VMs, SAP HANA, SQL in Azure VMs and Azure File","title":"Workloads"},{"location":"lzdesign/bcdr/#data-plane","text":"\u00b7 Automated storage management \u2013 Azure Backup automates provisioning and managing storage accounts for the backup data to ensure it scales as the backup data grows. \u00b7 Malicious delete protection \u2013 Protect against any accidental and malicious attempts for deleting your backups via soft delete of backups. The deleted backup data is stored for 14 days free of charge and allows it to be recovered from this state. \u00b7 Secure encrypted backups - Azure Backup ensures your backup data is stored in a secure manner, leveraging built-in security capabilities of the Azure platform like Azure RBAC and Encryption. \u00b7 Backup data lifecycle management - Azure Backup automatically cleans up older backup data to comply with the retention policies. You can also tier your data from operational storage to vault storage.","title":"Data Plane"},{"location":"lzdesign/bcdr/#management-plane","text":"\u00b7 Access control \u2013 Vaults (Recovery Services and Backup vaults) provide the management capabilities and are accessible via the Azure portal, Backup Center, Vault dashboards, SDK, CLI, and even REST APIs. It's also an Azure RBAC boundary, providing the option to restrict access to backups only to authorized Backup Admins. \u00b7 Policy management \u2013 Azure Backup Policies within each vault define when the backups should be triggered and how long they need to be retained. You can also manage these policies and apply them across multiple items. \u00b7 Monitoring and Reporting \u2013 Azure Backup integrates with Log Analytics and provides the ability to see reports via Workbooks as well. \u00b7 Snapshot management \u2013 Azure Backup takes snapshots for some Azure native workloads (VMs and Azure Files), manages these snapshots and allows fast restores from them. This option drastically reduces the time to recover your data to the original storage.","title":"Management Plane"},{"location":"lzdesign/bcdr/#solution-guidance","text":"This sub-section provides guidance at a high level on the key design considerations and best practices for creating a backup solution using the above described architecture.","title":"Solution Guidance"},{"location":"lzdesign/bcdr/#design-considerations","text":"Azure offers wide range of backup options. Your backup strategy will differ depending on the workload you need to protect. There are constraints and limits. Refer the support matrix to make sure the selected option meets the target workload backup and restore requirements. Backups can be stored in the vault or in the storage account. How much storage space is needed? Select the option based on reliability, data encryption, cost, security, recovery objectives (RPO, RTO) and operational ease of use. Some industries are bound by laws and regulations or market trends to ensure the high availability and disaster recovery of their service. They need to be ready to recover data and applications in an orchestrated manner if a critical outage takes place at a primary location. Number of copies of backups and the geo location. Company policies or regulations may dictate that backup need to be available both in the primary location and in a secondary DR location. If you are backing up on-premises data to cloud, network bandwidth is an important criterion. How much bandwidth would be required to back up the data to Azure? Backup costs. The backup charges for Azure VMs and on-premises servers can be summarized as follows. Size of each instance Azure backup price per month Instance < or = 50 GB $5 + storage consumed Instance is > 50 but < or = 500 GB $10 + storage consumed Instance > 500 GB $10 for each 500 GB increment + storage consumed Backup Performance and Backup Time. The acceptable timeframe for backup-related tasks, performance expectations and recovery point objectives Backup policy consideration. This includes backup schedule, frequency, start time and retention settings. 10.Security considerations. This includes access control, integrity of the data, confidentiality, availability assurances against attacks and abuse of your valuable data and systems**.** Monitoring and Alerting considerations. Backup data encryption using user provided encryption keys.","title":"Design considerations"},{"location":"lzdesign/bcdr/#design-actions-by-solution-architects","text":"","title":"Design Actions by Solution Architects"},{"location":"lzdesign/bcdr/#1-define-backup-solutions","text":"Following backup solutions are recommended for Azure Resources. Backup Use case Description Backup Option Azure VMs \u2013 Windows Back up the entire VM Azure Recovery Services Vault. Azure VMs \u2013 Windows Backup specific files/folders/volume. Azure Recovery Services Vault. Azure VMs \u2013 Linux Back up the entire VM Azure Recovery Services Vault. Azure VMs \u2013 Linux Backup specific files/folders/volume. Azure Backup Server MABS/Backup server (basically a scaled down version of DPM) is setup on a Windows VM, and the workload VMs with DPM/backup agent on it \u2013 sends the backup data to Azure backup serv-er/MABS, and the MABS server in-turn stores the backup data onto Azure Recovery Services Vault Storage. SQL Server running on VM Workload aware backups that support all backup types - full, differential, and log Azure Backup Service for SQL Server Azure Backup service installs a workload backup extension on the VM by the name AzureBackupWindowsWorkload extension and backup the database to the recovery vault Disk Backup Disk snapshot Azure Disk Backup. It is basically a disk snapshot service that provides snapshot lifecycle management for managed disks by automating periodic creation of snapshots and retaining it for configured duration using backup policy. File share backup Backup the file shares Azure File share backup Its syncs the files in the File share with Azure Recovery vault. Offline data backup Transfer large amounts of data to cloud storage Azure Data box Key and Secret backup Securely backup the keys and secretes stored in Azure Key Vault Azure Backup service does not provide a capability to backup the keys and secrets from Key Vault. The recommended approach is a) Backup the keys to a storage account using Key Vault backup option. b) Backup the data from the storage account to the vault using Azure backup service.","title":"1. Define Backup solutions"},{"location":"lzdesign/bcdr/#2-define-backup-monitoring","text":"As per GTS/Kyndryl Backup and Restore process, backup failure must be monitored and notified to the operations team. Azure Backup Center provides the single pane of glass to monitor and manage backup across a large and distributed Azure environment. You can use Backup Center to efficiently manage backups spanning multiple workload types, vaults, subscriptions, regions, customer tenants. The backup monitoring and notification service flow is depicted as below. \u00b7 Approved Backup policy is applied to the centralized backup service \u00b7 The Centralized backup service backups the cloud resources present inside the given Organization/Subscription/Account as per the policy \u00b7 Centralized backup service leverages the cloud vault to store the backups. \u00b7 Centralized Backup Center generates the logs for the backup jobs \u00b7 Configure the centralized Log Analytics workspace to pull the backup logs. \u00b7 Configure Azure Monitor to generate alerts from backup logs from the Log Analytics workspace. \u00b7 Send the alerts to cut an incident ticket in the ITSM tool. \u00b7 Based on the ticket, the platform SRE investigates the problem and resolves it","title":"2. Define Backup Monitoring"},{"location":"lzdesign/bcdr/#3-define-backup-reporting","text":"It is recommended to have a central backup console/single pane of glass across a large Azure environment. Azure provides a native service called Backup Center , which provides consolidated reporting dashboard across regions, subscriptions, and tenants. Configure the Backup Center to do the following. \u00b7 View Azure Policies for backup \u00b7 View compliance of your resources as per the backup policy. \u00b7 View all non-compliant resources as per the backup policy. \u00b7 View backup failure report (backup job status) \u00b7 Auditing of backups and restores. \u00b7 Forecasting of cloud storage consumed.","title":"3. Define Backup Reporting"},{"location":"lzdesign/bcdr/#4-establish-plan-for-test-restoration-of-data","text":"If backup is enabled for any protected resource or data type, then it can be restored from the backup service. It is recommended to periodically test recovery of a resource to make sure it can be restored as per the design. There are several ways a VM, database, file or a folder can be restored from the backup. We recommend the following - Automated Recovery : Put in a place a recovery process/workflow to recover a resource as per the application/business requirement. Automated workflow should use Azure native APIs to recover the data or resource from the recovery vault or from the backup server. 4.2 Manual Recovery : It is possible to manually recover the Azure resources like VM, databases and the data from the recovery vault. It is recommended to use Backup Center to recover the data, resources from the configured vault. You can restore your workloads regardless of your location or subscription from the centralized Backup Center.","title":"4. Establish plan for test Restoration of data"},{"location":"lzdesign/eaenrolment/","text":"Enterprise Enrolment & Azure AD Tenants \u2693\ufe0e Purpose of this section is to explain how subscriptions are/should be organized within Enterprise Agreements, which is key for designing critical aspects of LZ, mainly Identity & Access management and management group and subscription hierarchies. Microsoft Cloud Licensing Agreements \u2693\ufe0e There are two ways for organizations to buy Azure cloud services from Microsoft, Enterprise Agreement (EA) & Cloud Services Provider (CSP) program Enterprise Agreement (EA) Organizations can buy Azure cloud services and software licenses under one agreement for three years, and pricing is based on a scaled volume discount model, whereby the larger the size of the organization and usage of Azure resources, the less cost for services. CSP/Cloud Service Provider license Enables Managed Service Providers (MSP), like Kyndryl, to have end-to-end ownership of the customer lifecycle and relationship for Microsoft Azure. Empowers MSP to manage sales, own the billing relationship, provide technical and billing support and be the customers' single point of contact. In addition, CSP provides a full set of tools including a self-service portal and accompanying APIs to easily provision, manage and provide billing for their customers and subscriptions \u2022 EA Agreement is the most common scenario currently in Kyndryl customer accounts. \u2022 Kyndryl\u2019s adoption of CSP model, including the billing and costing aspect needs to be explored further. Landing Zone Design scope: EA (vs CSP) \u2693\ufe0e This solution guidance assumes that Kyndryl\u2019s client base will use Enterprise Agreement model, and hence all design considerations are based on EA model. Note : Kyndryl does not have a CSP agreement yet and hence guidance will be updated when Kyndryl follows CSP model for licensing. Enterprise Agreement (EA) Enrollment \u2693\ufe0e The Enterprise enrollment represents a billing contract, also referred to as an Enterprise Agreement (EA), that an organization has with Microsoft to use Azure. The enrollment gives the organization access to the Azure EA Portal where the organization access their billing information, manage cost, billing, invoicing aspects at enterprise level, including setting up spending quotas. This process should be handled by the client using existing procurement channels EA Hierarchy & Landing Zone Design \u2693\ufe0e \u2022 EA hierarchy does not have a relation or impact on Landing Zone (LZ) design. \u2022 However it is important that the client follows design recommendations for EA hierarchy, by assigning subscriptions to the appropriate hierarchy which is important for cost management purposes. Onboarding to EA Portal \u2693\ufe0e Initial access to Azure EA portal will be defined as part of the Enterprise Agreement (EA) enrollment process, after which the \u201cEnterprise Administrator\u201d will receive an email invite to login to the portal. Users to EA portal will have an ID defined in Azure Active Directory (AD). Note that this AD may be different from the AD defined for each tenant within the enterprise agreement. Refer get started with EA onboarding for further details. Azure EA Roles and portals \u2693\ufe0e To administer subscriptions and service within the enterprise enrollment, there are various roles defined which can be executed using different portals, as shown below: Tasks Portal Azure EA Roles Permissions Creation and management of EA(enterprise agreement) hierarchy EA Portal \u2022 Enterprise administrator, EA purchaser, Department administrator, Account owner, Notification contact Link to EA Portal Roles & Permissions Create Subscriptions, Services & Manage services Management Portal Service Administrator (defined in EA portal) Row 2 Azure EA portal Hierarchy \u2693\ufe0e Azure EA portal hierarchy, aka EA hierarchy , is illustrated in the below diagram. Enterprise Portal \u2013 Online management portal that helps manage costs for your Azure EA services through an hierarchy of Deparments, Accounts & Subscriptions. \u2022 Reconcile the costs of your consumed services, download usage reports, and view price lists. \u2022 Create API keys for your enrollment. Departments help segment costs into logical groupings, enables setting a budget or quota at the department level. Accounts are organizational units in the Azure Enterprise portal. You can use accounts to manage subscriptions and access reports. Subscriptions are the smallest unit in the Azure Enterprise portal. They are containers for Azure services managed by the service administrator NOTE : There is NO relation between this EA hierarchy and the Management Group hierarchy, defined in Azure portal ( https://portal.azure.com ) Azure EA hierarchy - examples \u2693\ufe0e There are a few examples provided by Azure, which could be used as a guideline to define hierarchy within the enterprise enrollment Azure EA hierarchy sample \u2013 Kyndryl MSP \u2693\ufe0e Below is an example/proposal for how Kyndryl subscriptions to provide managed services to Kyndryl\u2019s clients can be rolled into an enterprise agreement. Note on Accounts & Account Owner Role \u2693\ufe0e o Accounts can only have a single Account Owner. o And that Account Owner cannot be assigned as an Account Owner to any other Accounts. o So, Accounts, and subsequently Account Owners, are primarily responsible for creating and managing Subscriptions. o When an Account Owner creates a Subscription, that Subscription is assigned to that Account and the Account Owner becomes the first Subscription Owner for the new Subscription. o Subscriptions can be moved between Accounts but a Subscription can only belong to a single Account at any given time. o Think of Accounts, and Account Owners, as being the individuals that you want to create and manage Subscriptions within your Departments (or within your Enrollment if you\u2019re not using Departments). It\u2019s fairly common to have between 1-3 Accounts per Department. Accounts are created and managed in the Azure EA portal Azure EA hierarchy & Azure AD Tenant \u2693\ufe0e o Azure AD(AAD) tenants are defined through Azure Management portal. o Multiple AD tenants can be part of the same EA enterprise enrollment. o Azure AD tenant does not play a role in the enterprise hierarchy. Subscriptions which are grouped under the EA \u201cAccounts\u201d entity can be from the same AD tenant or from different AD tenants. Subscriptions \u2693\ufe0e Subscriptions are a unit of management, billing, and scale within Azure. Subscriptions play a critical role when designing for large-scale Azure adoption as well as for future scaling. Subscriptions are created on the Azure Management portal. Design and recommendations for the number of subscriptions, scope of subscriptions and naming for subscription is derived as part of Organization & Management design . Only action required as part of EA enrolment, is to ensure that subscriptions are assigned under the appropriate EA hierarchy explained above (after the subscriptions are created). Design Actions by Solution Architects \u2693\ufe0e In most cases, client already has an EA agreement and Kyndryl has no play in defining the hierarchy. Nevertheless, Solution architects should review the EA enrolment aspect and recommend best practices to the client: o Identify/Recommend the appropriate EA hierarchy working with client IT teams. o Identify/Recommend the appropriate Subscription Model for an account as part of Organization & Mgmt design . o Ensure EA focal follows recommended subscription naming standards to create the subscriptions. o Above steps, along with Identity & Access Management (described in next section) are the foundational elements of Landing Zone design.","title":"1.Enterprise Enrollment"},{"location":"lzdesign/eaenrolment/#enterprise-enrolment-azure-ad-tenants","text":"Purpose of this section is to explain how subscriptions are/should be organized within Enterprise Agreements, which is key for designing critical aspects of LZ, mainly Identity & Access management and management group and subscription hierarchies.","title":"Enterprise Enrolment &amp; Azure AD Tenants"},{"location":"lzdesign/eaenrolment/#microsoft-cloud-licensing-agreements","text":"There are two ways for organizations to buy Azure cloud services from Microsoft, Enterprise Agreement (EA) & Cloud Services Provider (CSP) program Enterprise Agreement (EA) Organizations can buy Azure cloud services and software licenses under one agreement for three years, and pricing is based on a scaled volume discount model, whereby the larger the size of the organization and usage of Azure resources, the less cost for services. CSP/Cloud Service Provider license Enables Managed Service Providers (MSP), like Kyndryl, to have end-to-end ownership of the customer lifecycle and relationship for Microsoft Azure. Empowers MSP to manage sales, own the billing relationship, provide technical and billing support and be the customers' single point of contact. In addition, CSP provides a full set of tools including a self-service portal and accompanying APIs to easily provision, manage and provide billing for their customers and subscriptions \u2022 EA Agreement is the most common scenario currently in Kyndryl customer accounts. \u2022 Kyndryl\u2019s adoption of CSP model, including the billing and costing aspect needs to be explored further.","title":"Microsoft Cloud Licensing Agreements"},{"location":"lzdesign/eaenrolment/#landing-zone-design-scope-ea-vs-csp","text":"This solution guidance assumes that Kyndryl\u2019s client base will use Enterprise Agreement model, and hence all design considerations are based on EA model. Note : Kyndryl does not have a CSP agreement yet and hence guidance will be updated when Kyndryl follows CSP model for licensing.","title":"Landing Zone Design scope: EA (vs CSP)"},{"location":"lzdesign/eaenrolment/#enterprise-agreement-ea-enrollment","text":"The Enterprise enrollment represents a billing contract, also referred to as an Enterprise Agreement (EA), that an organization has with Microsoft to use Azure. The enrollment gives the organization access to the Azure EA Portal where the organization access their billing information, manage cost, billing, invoicing aspects at enterprise level, including setting up spending quotas. This process should be handled by the client using existing procurement channels","title":"Enterprise Agreement (EA) Enrollment"},{"location":"lzdesign/eaenrolment/#ea-hierarchy-landing-zone-design","text":"\u2022 EA hierarchy does not have a relation or impact on Landing Zone (LZ) design. \u2022 However it is important that the client follows design recommendations for EA hierarchy, by assigning subscriptions to the appropriate hierarchy which is important for cost management purposes.","title":"EA Hierarchy &amp; Landing Zone Design"},{"location":"lzdesign/eaenrolment/#onboarding-to-ea-portal","text":"Initial access to Azure EA portal will be defined as part of the Enterprise Agreement (EA) enrollment process, after which the \u201cEnterprise Administrator\u201d will receive an email invite to login to the portal. Users to EA portal will have an ID defined in Azure Active Directory (AD). Note that this AD may be different from the AD defined for each tenant within the enterprise agreement. Refer get started with EA onboarding for further details.","title":"Onboarding to EA Portal"},{"location":"lzdesign/eaenrolment/#azure-ea-roles-and-portals","text":"To administer subscriptions and service within the enterprise enrollment, there are various roles defined which can be executed using different portals, as shown below: Tasks Portal Azure EA Roles Permissions Creation and management of EA(enterprise agreement) hierarchy EA Portal \u2022 Enterprise administrator, EA purchaser, Department administrator, Account owner, Notification contact Link to EA Portal Roles & Permissions Create Subscriptions, Services & Manage services Management Portal Service Administrator (defined in EA portal) Row 2","title":"Azure EA Roles and portals"},{"location":"lzdesign/eaenrolment/#azure-ea-portal-hierarchy","text":"Azure EA portal hierarchy, aka EA hierarchy , is illustrated in the below diagram. Enterprise Portal \u2013 Online management portal that helps manage costs for your Azure EA services through an hierarchy of Deparments, Accounts & Subscriptions. \u2022 Reconcile the costs of your consumed services, download usage reports, and view price lists. \u2022 Create API keys for your enrollment. Departments help segment costs into logical groupings, enables setting a budget or quota at the department level. Accounts are organizational units in the Azure Enterprise portal. You can use accounts to manage subscriptions and access reports. Subscriptions are the smallest unit in the Azure Enterprise portal. They are containers for Azure services managed by the service administrator NOTE : There is NO relation between this EA hierarchy and the Management Group hierarchy, defined in Azure portal ( https://portal.azure.com )","title":"Azure EA portal Hierarchy"},{"location":"lzdesign/eaenrolment/#azure-ea-hierarchy-examples","text":"There are a few examples provided by Azure, which could be used as a guideline to define hierarchy within the enterprise enrollment","title":"Azure EA hierarchy - examples"},{"location":"lzdesign/eaenrolment/#azure-ea-hierarchy-sample-kyndryl-msp","text":"Below is an example/proposal for how Kyndryl subscriptions to provide managed services to Kyndryl\u2019s clients can be rolled into an enterprise agreement.","title":"Azure EA hierarchy sample \u2013 Kyndryl MSP"},{"location":"lzdesign/eaenrolment/#note-on-accounts-account-owner-role","text":"o Accounts can only have a single Account Owner. o And that Account Owner cannot be assigned as an Account Owner to any other Accounts. o So, Accounts, and subsequently Account Owners, are primarily responsible for creating and managing Subscriptions. o When an Account Owner creates a Subscription, that Subscription is assigned to that Account and the Account Owner becomes the first Subscription Owner for the new Subscription. o Subscriptions can be moved between Accounts but a Subscription can only belong to a single Account at any given time. o Think of Accounts, and Account Owners, as being the individuals that you want to create and manage Subscriptions within your Departments (or within your Enrollment if you\u2019re not using Departments). It\u2019s fairly common to have between 1-3 Accounts per Department. Accounts are created and managed in the Azure EA portal","title":"Note on Accounts &amp; Account Owner Role"},{"location":"lzdesign/eaenrolment/#azure-ea-hierarchy-azure-ad-tenant","text":"o Azure AD(AAD) tenants are defined through Azure Management portal. o Multiple AD tenants can be part of the same EA enterprise enrollment. o Azure AD tenant does not play a role in the enterprise hierarchy. Subscriptions which are grouped under the EA \u201cAccounts\u201d entity can be from the same AD tenant or from different AD tenants.","title":"Azure EA hierarchy &amp; Azure AD Tenant"},{"location":"lzdesign/eaenrolment/#subscriptions","text":"Subscriptions are a unit of management, billing, and scale within Azure. Subscriptions play a critical role when designing for large-scale Azure adoption as well as for future scaling. Subscriptions are created on the Azure Management portal. Design and recommendations for the number of subscriptions, scope of subscriptions and naming for subscription is derived as part of Organization & Management design . Only action required as part of EA enrolment, is to ensure that subscriptions are assigned under the appropriate EA hierarchy explained above (after the subscriptions are created).","title":"Subscriptions"},{"location":"lzdesign/eaenrolment/#design-actions-by-solution-architects","text":"In most cases, client already has an EA agreement and Kyndryl has no play in defining the hierarchy. Nevertheless, Solution architects should review the EA enrolment aspect and recommend best practices to the client: o Identify/Recommend the appropriate EA hierarchy working with client IT teams. o Identify/Recommend the appropriate Subscription Model for an account as part of Organization & Mgmt design . o Ensure EA focal follows recommended subscription naming standards to create the subscriptions. o Above steps, along with Identity & Access Management (described in next section) are the foundational elements of Landing Zone design.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/","text":"Governance & Compliance \u2693\ufe0e Considering the vast requirements and guidelines needed for all aspects of governance, it is categorized into following areas, based on which governance policies shall be defined. Security Baseline Identity Baseline Resource Consistency Deployment Acceleration Cost Management The chart below provides the list of Azure Native tools that are making the governance actionable in each of the above listed areas. The first section below outlines the solution architecture provided by Azure in support of governance. The remaining sections explain the governance and controls that need to be considered and enforced for each discipline and how these tools can be leveraged in providing the optimal governance. Solution Architecture for Governance \u2693\ufe0e The above diagram exemplifies the services and tools that Azure provides for the definition, assignment and enforcement of policies/initiatives. Policies/Initiatives \u2693\ufe0e Initiatives are combinations of policies that achieve a collective goal such as HIPAA industry compliance. Each instance of a policy specifies an action, for example, Deny or Audit, when the conditions specified by a policy match. There are two kinds of policies/initiatives - General policies/initiatives cover constraints such as limiting deployment of resources to specific locations, use of specific VM types and use of a specific tagging convention for resources. Security policies/initiatives cover industry and government mandated compliances as also those custom ones such as firewalls for Internet traffic, encryption for data at rest and so on. As shown in the diagram, instances of both policy/initiative types are created within Azure using the Azure Policy service by the Governance team. Security domain experts will be needed to prepare the security policies/initiatives that the Governance team uses to create within Azure. Role of Azure Resource Manager (ARM) \u2693\ufe0e This section discusses the role of ARM with regard to general policies/initiatives. The ARM handles all requests related to the lifecycle of Azure resources. As and when a request is received from an Operations personnel, it is vetted against all the currently assigned policies/initiatives applicable to the resources in the scope of the request. This is shown by the dotted line to the box labeled Assigned General Policies/Initiatives. Role of Azure Policy \u2693\ufe0e The role of Azure Policy in the creation of instances of all policy/initiative types is already documented above. Azure Policy is also used by the Governance team to assign general policies/initiatives to various collections of resources. In addition, it is used to enforce the assigned policies/initiatives and take the specified action on detection of violations. While ARM enforces policies/initiatives at provisioning time for resources, Azure Policy scans existing resources periodically to ensure continual compliance with the assigned policies/initiatives. Role of Azure Security Center \u2693\ufe0e Security Operations personnel use the Azure Security Center to assign security policies/initiatives to various collections of resources. Azure Security Center then checks for compliance with these policies/initiatives and identifies any violations for followup by the Security Operations team. Security Baseline Governance \u2693\ufe0e Security baselines for Azure help strengthen security through improved tooling, tracking, and security features. They also provide a consistent experience when securing the Azure environment. Security baselines for Azure focus on cloud-centric control areas. These controls are consistent with well-known security benchmarks, such as those described by the Center for Internet Security (CIS). The baselines provide guidance for the control areas listed in the Azure Security Benchmark . To establish a secure baseline for governance, the following factors should be considered. Compliance and risk Data Encryption at rest Data Encryption in flight Infrastructure Security Compliance and risk and Security operations \u2693\ufe0e Azure Security Center continually compares the configuration of the resources with requirements in industry standards, regulations, and benchmarks. The regulatory compliance dashboard provides insights into the compliance posture based on how specific compliance controls and requirements are being met. To know more about policy and Industry Standard controls configuration click here . Azure Security Center improves the overall compliance readiness by performing ongoing assessments, providing rich, actionable insights and reports to simplify your regulatory compliance journey. Azure Security Center identifies the potential threats to deployed resources and services, provides recommendations for improving security hygiene and where possible, provides quick fixes for remediation. It provides a secure score to quantify the extent of an organization\u2019s compliance. Azure Sentinel provides Machine Intelligence driven correlation across a wide set of data sources to detect, respond and recover from security threats. Please refer to the section Security Management Architecture on this page for more details on Azure Security Center and Sentinel. Design Actions by Solution Architects \u2693\ufe0e The Microsoft-authored Azure Security Benchmark is the default set of controls enforced. More details on this benchmark can be obtained here \u00b7Consider applying the below set of guidelines for security and compliance best practices based on common compliance frameworks on top of industry standard controls Azure Security Benchmark CIS Microsoft Azure foundation Benchmark v 1.1.0 These policies will be enforced using the Azure Security Center (standard tier) to understand and reporting the compliance posture relative to industry standards. Data Encryption at Rest \u2693\ufe0e Data encryption at Rest is a common security requirement. While Azure provides native encryption, it may not meet an organization\u2019s stringent security requirements. In which case Azure Key Vault which enables customers to safeguard and control cryptographic keys and other secrets used by cloud apps and services needs to be leveraged. Design Actions by Solution Architects \u2693\ufe0e The following are the design decisions for Key Vault - Use Premium SKUs where hardware-security-module-protected keys are required. Underlying hardware security modules (HSMs) are FIPS 140-2 Level 2 compliant Use a vault per application per environment (Development, Pre-Production and Production). This helps reduce the impact of a breach on other applications and environments. Enable firewall and virtual network service endpoint on the vault to control access to the Key Vault. Use the Azure Monitor Log Analytics workspace to audit key, certificate, and secret usage within each instance of Key Vault. Delegate Key Vault instantiation and privileged access and use Azure Policy to enforce a consistent compliant configuration. Lock down access to your subscription, resource group and Key Vaults (Azure RBAC) Create Access policies for every vault Use least privilege access principal to grant access Azure Defender (Azure Security Center) is enabled for Azure Key Vault Enable Soft delete so data is still available for a period of time after a delete is requested Enable Purge protection Encryption of data in transit \u2693\ufe0e Data must be encrypted when transmitted across networks to protect against eavesdropping of network traffic by unauthorized users. In cases where source and target endpoint devices are within the same protected subnet, data transmission must still be encrypted due to the potential for high negative impact of a data breach. The types of transmission may include client-to-server, server-to-server communication, as well as any data transfer between core systems and third-party systems. East west traffic \u2693\ufe0e This comprises of the traffic between customer applications and services within a VNet and also to and from Azure services. Design Actions by Solution Architects \u2693\ufe0e Secure transfer to storage accounts should be enabled (HTTPS). Web Application should only be accessible over HTTPS. FTPS should be required in your web App and function App. Function App should only be accessible over HTTPS. Enforce SSL connection should be enabled for MySQL database servers. North south traffic traffic between Azure Data centers \u2693\ufe0e For such traffic within the region or between regions, no action is required from customer as MACsec encryption is enabled by default. Design Actions by Solution Architects \u2693\ufe0e In a highly regulated environments if there is a need to encrypt the north south traffic between Azure datacenter/regions Azure VPN Gateways must be used. North South traffic between On-prem and Azure Data centers \u2693\ufe0e It is desirable and on occasion, mandatory to encrypt traffic between Azure Datacenters and customer premises. Design Actions by Solution Architects \u2693\ufe0e Where traffic does not have latency and bandwidth performance requirements, use an Azure VPN gateway to encrypt traffic between your virtual network and on-premises location across a public connection Where traffic has latency and bandwidth performance requirements, Express Route can be used. Express Route is a private link and the network traffic is not encrypted but if the regulatory requirements mandate encryption Express Route Direct that supports MACsec encryption can be used Microsoft gives customers the ability to use Transport Layer Security (TLS) protocol to protect data when it\u2019s traveling between the cloud services and customers. Microsoft Datacenters negotiate a TLS connection with client systems that connect to Azure services. Data Lake data must be always secured in transit by using HTTPS. HTTPS is the only protocol that is supported for the Data Lake Store REST interfaces. Infrastructure Security \u2693\ufe0e Infrastructure refers to the Azure compute, network and storage resources used by customer workloads. Design Actions by Solution Architects \u2693\ufe0e To establish a secure Infrastructure baseline for governance, the following factors should be considered. Security monitoring \u2013 Refer to the section Security Management Architecture on this page Azure Blueprints - Consider developing Azure Blueprints for reusable and rapidly deployable configurations based on Azure Resource Manager templates, policy, security, and more. Details on creation of Azure Blueprints can be viewed here . It is to be noted development of Azure Blueprints needs additional effort. Network security aspects, as listed below, are covered in the Network Topology and Connectivity section here Isolate subnets containing sensitive data from others that do not need access to that data Audit traffic flows to and from the subnets containing sensitive data Prevent direct access to subnets containing sensitive data from the Internet and external Data Centers Leverage Role based Access control (RBAC) to provide minimal access privileges Azure Tools for Security Governance \u2693\ufe0e Azure Resource Manager : Applies access controls to resources and secures virtual networks. Azure Key Vault : Enables encryption of storage at rest. Azure AD : Manages hybrid identity services and applies access controls to resources. Azure Monitor : Enables detection of malicious activities. Azure Security Center : Detects violation of compliance with industry/regulatory standards, preemptively detects vulnerabilities. Azure Policy : Enable DDoS Protection for virtual networks, prevent use of resource types that violate security policies. Identity Baseline Governance \u2693\ufe0e Identity is increasingly considered the primary security perimeter in the cloud, which is a shift from the traditional focus on network security. Identity services provide the core mechanisms supporting access control and organization within IT environments. Design Actions by Solution Architects \u2693\ufe0e Baseline discipline complements the Security Baseline discipline by consistently applying authentication and authorization requirements across cloud adoption efforts. To know about identity security baseline, follow the solution guidance in the Identity and Access Management section here . Logging is vital to keep records of all activities happening in subscriptions. In the event of a security breach or a major legal battle, these audit logs will be key evidence. All security logs must be collected in the central Log Analytics workspace Threat Protection enables Security Center Standard Edition detects a threat in any area of your environment, it generates an alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases an option to trigger a logic app in response. Azure Defender is to be enabled. Integrate Security Center with Sentinel. Refer to the section Security Management Architecture on this page . Resource Consistency Governance \u2693\ufe0e The Resource Consistency governance discipline is concerned with ensuring resources are deployed, updated, and configured consistently and repeatably, and that service disruptions are minimized and remedied in as little time as possible. Design Actions by Solution Architects \u2693\ufe0e When deploying resources, the Cloud Governance team will work with the Operations team to identify business risks and establish a baseline set of policy statements that are intended to mitigate that risk. These policies will be implemented at the outset, then optimized over time based on regular reviews of operations. One example of a business risk is to overprovision resources and incur unnecessary cost. The policy to mitigate this risk would probably detect when a resource was used below 60% and scale down the resource. The operations team could implement monitoring for the risk situation, trigger an alarm when the situation occurs, then have automated remediation resolve the issue. Azure Tooling such as ARM templates and Blueprints could be used to automate the deployment of resources in conformance with the identified policies. Azure Monitor could be used to detect a violation of the policies and also trigger remediation through Azure Logic Apps. Cost management \u2693\ufe0e The Cost Management discipline addresses the needs of customers to govern their costs particularly in balancing performance demands, adoption pacing, and controlling cloud services costs. Design Actions by Solution Architects \u2693\ufe0e Tagging is critical to all governance: Ensure all workloads and resources follow proper naming and tagging conventions and enforce tagging conventions using Azure Policy . This will help your centralized governance teams make wise cost management decisions. Licensing alignment: Proper allocation of Azure Hybrid Benefit and Azure Reserved VM Instances will significantly reduce your per unit cost for assets across your portfolio. These types of licensing decisions are generally established and maintained by central procurement functions. However, decentralized workload teams may want to be consulted on purchase and allocation of the licenses to minimize the cost of their individual workloads. Identify right size opportunities: Review your current resource utilization and performance requirements across the environment. Modify each resource to use the smallest instance or SKU that can support the performance requirements of each resource. Shut down and deprovision unused resources: Unused assets add cost in a cloud environment. Identify and terminate any resources that are adding to costs but are not adding to business value. Horizontal over vertical scale: Using multiple small instances can allow for an easier scaling path that a single larger instance. This allows for scale automation, which creates cost optimization. Azure Tools for Cost Management \u2693\ufe0e Cost Management + Billing : Enables the creation of budgets over varying periods and subscriptions/resource groups. Performs cost analysis providing insights into consumption. Data can be made available to external tooling either via APIs or via export to Storage. Email notifications on budget threshold violations can be configured. Azure Advisor: Leverages resource configuration and usage telemetry to improve the cost effectiveness and performance of the Azure resources. Azure Monitor : Monitors resources and trigger alerts leading to actions such as VM resizing and auto-scaling of VM Scale Sets. Azure Policy : Sets up policies to control resource types used, tagging of resources and budget control. Deployment Automation \u2693\ufe0e The Deployment Acceleration discipline addresses Deployment automation to improve consistency and predicatbility Detection of configuration drift and automated remediation Detection of violation of compliance and automated remediation Automation as a whole will be covered in the next phase of this effort.","title":"6.Governance & Compliance"},{"location":"lzdesign/govcomp/#governance-compliance","text":"Considering the vast requirements and guidelines needed for all aspects of governance, it is categorized into following areas, based on which governance policies shall be defined. Security Baseline Identity Baseline Resource Consistency Deployment Acceleration Cost Management The chart below provides the list of Azure Native tools that are making the governance actionable in each of the above listed areas. The first section below outlines the solution architecture provided by Azure in support of governance. The remaining sections explain the governance and controls that need to be considered and enforced for each discipline and how these tools can be leveraged in providing the optimal governance.","title":"Governance &amp; Compliance"},{"location":"lzdesign/govcomp/#solution-architecture-for-governance","text":"The above diagram exemplifies the services and tools that Azure provides for the definition, assignment and enforcement of policies/initiatives.","title":"Solution Architecture for Governance"},{"location":"lzdesign/govcomp/#policiesinitiatives","text":"Initiatives are combinations of policies that achieve a collective goal such as HIPAA industry compliance. Each instance of a policy specifies an action, for example, Deny or Audit, when the conditions specified by a policy match. There are two kinds of policies/initiatives - General policies/initiatives cover constraints such as limiting deployment of resources to specific locations, use of specific VM types and use of a specific tagging convention for resources. Security policies/initiatives cover industry and government mandated compliances as also those custom ones such as firewalls for Internet traffic, encryption for data at rest and so on. As shown in the diagram, instances of both policy/initiative types are created within Azure using the Azure Policy service by the Governance team. Security domain experts will be needed to prepare the security policies/initiatives that the Governance team uses to create within Azure.","title":"Policies/Initiatives"},{"location":"lzdesign/govcomp/#role-of-azure-resource-manager-arm","text":"This section discusses the role of ARM with regard to general policies/initiatives. The ARM handles all requests related to the lifecycle of Azure resources. As and when a request is received from an Operations personnel, it is vetted against all the currently assigned policies/initiatives applicable to the resources in the scope of the request. This is shown by the dotted line to the box labeled Assigned General Policies/Initiatives.","title":"Role of Azure Resource Manager (ARM)"},{"location":"lzdesign/govcomp/#role-of-azure-policy","text":"The role of Azure Policy in the creation of instances of all policy/initiative types is already documented above. Azure Policy is also used by the Governance team to assign general policies/initiatives to various collections of resources. In addition, it is used to enforce the assigned policies/initiatives and take the specified action on detection of violations. While ARM enforces policies/initiatives at provisioning time for resources, Azure Policy scans existing resources periodically to ensure continual compliance with the assigned policies/initiatives.","title":"Role of Azure Policy"},{"location":"lzdesign/govcomp/#role-of-azure-security-center","text":"Security Operations personnel use the Azure Security Center to assign security policies/initiatives to various collections of resources. Azure Security Center then checks for compliance with these policies/initiatives and identifies any violations for followup by the Security Operations team.","title":"Role of Azure Security Center"},{"location":"lzdesign/govcomp/#security-baseline-governance","text":"Security baselines for Azure help strengthen security through improved tooling, tracking, and security features. They also provide a consistent experience when securing the Azure environment. Security baselines for Azure focus on cloud-centric control areas. These controls are consistent with well-known security benchmarks, such as those described by the Center for Internet Security (CIS). The baselines provide guidance for the control areas listed in the Azure Security Benchmark . To establish a secure baseline for governance, the following factors should be considered. Compliance and risk Data Encryption at rest Data Encryption in flight Infrastructure Security","title":"Security Baseline Governance"},{"location":"lzdesign/govcomp/#compliance-and-risk-and-security-operations","text":"Azure Security Center continually compares the configuration of the resources with requirements in industry standards, regulations, and benchmarks. The regulatory compliance dashboard provides insights into the compliance posture based on how specific compliance controls and requirements are being met. To know more about policy and Industry Standard controls configuration click here . Azure Security Center improves the overall compliance readiness by performing ongoing assessments, providing rich, actionable insights and reports to simplify your regulatory compliance journey. Azure Security Center identifies the potential threats to deployed resources and services, provides recommendations for improving security hygiene and where possible, provides quick fixes for remediation. It provides a secure score to quantify the extent of an organization\u2019s compliance. Azure Sentinel provides Machine Intelligence driven correlation across a wide set of data sources to detect, respond and recover from security threats. Please refer to the section Security Management Architecture on this page for more details on Azure Security Center and Sentinel.","title":"Compliance and risk and Security operations"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects","text":"The Microsoft-authored Azure Security Benchmark is the default set of controls enforced. More details on this benchmark can be obtained here \u00b7Consider applying the below set of guidelines for security and compliance best practices based on common compliance frameworks on top of industry standard controls Azure Security Benchmark CIS Microsoft Azure foundation Benchmark v 1.1.0 These policies will be enforced using the Azure Security Center (standard tier) to understand and reporting the compliance posture relative to industry standards.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#data-encryption-at-rest","text":"Data encryption at Rest is a common security requirement. While Azure provides native encryption, it may not meet an organization\u2019s stringent security requirements. In which case Azure Key Vault which enables customers to safeguard and control cryptographic keys and other secrets used by cloud apps and services needs to be leveraged.","title":"Data Encryption at Rest"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_1","text":"The following are the design decisions for Key Vault - Use Premium SKUs where hardware-security-module-protected keys are required. Underlying hardware security modules (HSMs) are FIPS 140-2 Level 2 compliant Use a vault per application per environment (Development, Pre-Production and Production). This helps reduce the impact of a breach on other applications and environments. Enable firewall and virtual network service endpoint on the vault to control access to the Key Vault. Use the Azure Monitor Log Analytics workspace to audit key, certificate, and secret usage within each instance of Key Vault. Delegate Key Vault instantiation and privileged access and use Azure Policy to enforce a consistent compliant configuration. Lock down access to your subscription, resource group and Key Vaults (Azure RBAC) Create Access policies for every vault Use least privilege access principal to grant access Azure Defender (Azure Security Center) is enabled for Azure Key Vault Enable Soft delete so data is still available for a period of time after a delete is requested Enable Purge protection","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#encryption-of-data-in-transit","text":"Data must be encrypted when transmitted across networks to protect against eavesdropping of network traffic by unauthorized users. In cases where source and target endpoint devices are within the same protected subnet, data transmission must still be encrypted due to the potential for high negative impact of a data breach. The types of transmission may include client-to-server, server-to-server communication, as well as any data transfer between core systems and third-party systems.","title":"Encryption of data in transit"},{"location":"lzdesign/govcomp/#east-west-traffic","text":"This comprises of the traffic between customer applications and services within a VNet and also to and from Azure services.","title":"East west traffic"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_2","text":"Secure transfer to storage accounts should be enabled (HTTPS). Web Application should only be accessible over HTTPS. FTPS should be required in your web App and function App. Function App should only be accessible over HTTPS. Enforce SSL connection should be enabled for MySQL database servers.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#north-south-traffic-traffic-between-azure-data-centers","text":"For such traffic within the region or between regions, no action is required from customer as MACsec encryption is enabled by default.","title":"North south traffic traffic between Azure Data centers"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_3","text":"In a highly regulated environments if there is a need to encrypt the north south traffic between Azure datacenter/regions Azure VPN Gateways must be used.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#north-south-traffic-between-on-prem-and-azure-data-centers","text":"It is desirable and on occasion, mandatory to encrypt traffic between Azure Datacenters and customer premises.","title":"North South traffic between On-prem and Azure Data centers"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_4","text":"Where traffic does not have latency and bandwidth performance requirements, use an Azure VPN gateway to encrypt traffic between your virtual network and on-premises location across a public connection Where traffic has latency and bandwidth performance requirements, Express Route can be used. Express Route is a private link and the network traffic is not encrypted but if the regulatory requirements mandate encryption Express Route Direct that supports MACsec encryption can be used Microsoft gives customers the ability to use Transport Layer Security (TLS) protocol to protect data when it\u2019s traveling between the cloud services and customers. Microsoft Datacenters negotiate a TLS connection with client systems that connect to Azure services. Data Lake data must be always secured in transit by using HTTPS. HTTPS is the only protocol that is supported for the Data Lake Store REST interfaces.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#infrastructure-security","text":"Infrastructure refers to the Azure compute, network and storage resources used by customer workloads.","title":"Infrastructure Security"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_5","text":"To establish a secure Infrastructure baseline for governance, the following factors should be considered. Security monitoring \u2013 Refer to the section Security Management Architecture on this page Azure Blueprints - Consider developing Azure Blueprints for reusable and rapidly deployable configurations based on Azure Resource Manager templates, policy, security, and more. Details on creation of Azure Blueprints can be viewed here . It is to be noted development of Azure Blueprints needs additional effort. Network security aspects, as listed below, are covered in the Network Topology and Connectivity section here Isolate subnets containing sensitive data from others that do not need access to that data Audit traffic flows to and from the subnets containing sensitive data Prevent direct access to subnets containing sensitive data from the Internet and external Data Centers Leverage Role based Access control (RBAC) to provide minimal access privileges","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#azure-tools-for-security-governance","text":"Azure Resource Manager : Applies access controls to resources and secures virtual networks. Azure Key Vault : Enables encryption of storage at rest. Azure AD : Manages hybrid identity services and applies access controls to resources. Azure Monitor : Enables detection of malicious activities. Azure Security Center : Detects violation of compliance with industry/regulatory standards, preemptively detects vulnerabilities. Azure Policy : Enable DDoS Protection for virtual networks, prevent use of resource types that violate security policies.","title":"Azure Tools for Security Governance"},{"location":"lzdesign/govcomp/#identity-baseline-governance","text":"Identity is increasingly considered the primary security perimeter in the cloud, which is a shift from the traditional focus on network security. Identity services provide the core mechanisms supporting access control and organization within IT environments.","title":"Identity Baseline Governance"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_6","text":"Baseline discipline complements the Security Baseline discipline by consistently applying authentication and authorization requirements across cloud adoption efforts. To know about identity security baseline, follow the solution guidance in the Identity and Access Management section here . Logging is vital to keep records of all activities happening in subscriptions. In the event of a security breach or a major legal battle, these audit logs will be key evidence. All security logs must be collected in the central Log Analytics workspace Threat Protection enables Security Center Standard Edition detects a threat in any area of your environment, it generates an alert. These alerts describe details of the affected resources, suggested remediation steps, and in some cases an option to trigger a logic app in response. Azure Defender is to be enabled. Integrate Security Center with Sentinel. Refer to the section Security Management Architecture on this page .","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#resource-consistency-governance","text":"The Resource Consistency governance discipline is concerned with ensuring resources are deployed, updated, and configured consistently and repeatably, and that service disruptions are minimized and remedied in as little time as possible.","title":"Resource Consistency Governance"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_7","text":"When deploying resources, the Cloud Governance team will work with the Operations team to identify business risks and establish a baseline set of policy statements that are intended to mitigate that risk. These policies will be implemented at the outset, then optimized over time based on regular reviews of operations. One example of a business risk is to overprovision resources and incur unnecessary cost. The policy to mitigate this risk would probably detect when a resource was used below 60% and scale down the resource. The operations team could implement monitoring for the risk situation, trigger an alarm when the situation occurs, then have automated remediation resolve the issue. Azure Tooling such as ARM templates and Blueprints could be used to automate the deployment of resources in conformance with the identified policies. Azure Monitor could be used to detect a violation of the policies and also trigger remediation through Azure Logic Apps.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#cost-management","text":"The Cost Management discipline addresses the needs of customers to govern their costs particularly in balancing performance demands, adoption pacing, and controlling cloud services costs.","title":"Cost management"},{"location":"lzdesign/govcomp/#design-actions-by-solution-architects_8","text":"Tagging is critical to all governance: Ensure all workloads and resources follow proper naming and tagging conventions and enforce tagging conventions using Azure Policy . This will help your centralized governance teams make wise cost management decisions. Licensing alignment: Proper allocation of Azure Hybrid Benefit and Azure Reserved VM Instances will significantly reduce your per unit cost for assets across your portfolio. These types of licensing decisions are generally established and maintained by central procurement functions. However, decentralized workload teams may want to be consulted on purchase and allocation of the licenses to minimize the cost of their individual workloads. Identify right size opportunities: Review your current resource utilization and performance requirements across the environment. Modify each resource to use the smallest instance or SKU that can support the performance requirements of each resource. Shut down and deprovision unused resources: Unused assets add cost in a cloud environment. Identify and terminate any resources that are adding to costs but are not adding to business value. Horizontal over vertical scale: Using multiple small instances can allow for an easier scaling path that a single larger instance. This allows for scale automation, which creates cost optimization.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/govcomp/#azure-tools-for-cost-management","text":"Cost Management + Billing : Enables the creation of budgets over varying periods and subscriptions/resource groups. Performs cost analysis providing insights into consumption. Data can be made available to external tooling either via APIs or via export to Storage. Email notifications on budget threshold violations can be configured. Azure Advisor: Leverages resource configuration and usage telemetry to improve the cost effectiveness and performance of the Azure resources. Azure Monitor : Monitors resources and trigger alerts leading to actions such as VM resizing and auto-scaling of VM Scale Sets. Azure Policy : Sets up policies to control resource types used, tagging of resources and budget control.","title":"Azure Tools for Cost Management"},{"location":"lzdesign/govcomp/#deployment-automation","text":"The Deployment Acceleration discipline addresses Deployment automation to improve consistency and predicatbility Detection of configuration drift and automated remediation Detection of violation of compliance and automated remediation Automation as a whole will be covered in the next phase of this effort.","title":"Deployment Automation"},{"location":"lzdesign/iam/","text":"Identity & Access Management \u2693\ufe0e Pre-Requisite Reading \u2693\ufe0e Solution Architect is expected to be familiar with Azure AD basics and Azure RBAC constructs, listed below. Azure Active Directory(AAD) Fundamentals & Features . Azure RBAC, Security Principle, Role Definition, Scope & Role Assignments Kyndryl ITSS standards \u2693\ufe0e Recommendations given in this section \u201caligns\u201d with Kyndryl\u2019s IT Security Standards for Cloud/SaaS User Access Management , wherever feasible. Identity Model \u2693\ufe0e A critical design element in an organization\u2019s cloud adoption plan is choosing the appropriate authentication method for the resources to be hosted in cloud. Authentication is also a critical element in the Landing Zone design. And this starts with choosing the correct identity model for the organization: 1. Cloud only identity : Create a brand-new identity/authentication system in the cloud, applicable to organizations which are \u2018pure cloud\u2019 \u2013 meaning they do not have any on-prem resources. 2. Hybrid Identity : Extend an existing on-premises identity domain into the cloud, applicable to \u2018hybrid\u2019 organizations which have on-prem resources. This solution creates a common user identity for authentication and authorization to all resources, regardless of location. And most organizations fall into this category as they start their cloud adoption journey. o When using Hybrid Identity all User identities are defined in on-premise identity systems which are then synched to Azure AD. Users are then assigned roles and permissions using Azure RBAC. For \u2018Pure Cloud\u2019 organizations the decision is easy since the only option is to use Azure Active Directory (AAD) for Identity and Access management. However, for organizations that need to use Hybrid identity, there are different hybrid authentication methods available to choose from. Hybrid Authentication Methods \u2693\ufe0e To achieve hybrid identity with Azure AD (AAD), one of three authentication methods explained below can be used: \u2022 Password hash synchronization (PHS). \u2022 Pass-through authentication (PTA). \u2022 Federation (AD FS) \u2013 In this authentication method Azure AD will hand off the authentication process to a separate trusted authentication system, such as AD FS or a third-party federation system, to validate the user's sign-in. All above authentication methods also provide single sign-on capabilities. In addition, Azure Active Directory Seamless Single Sign-On (Azure AD Seamless SSO) automatically signs users in when they are on their corporate devices connected to their corporate network. When enabled, users don't need to type in their passwords to sign in to Azure AD, and usually, don\u2019t even type in their usernames. This feature provides users easy access to cloud-based applications without needing any additional on-premises components. Decision Tree to choose a Hybrid Authentication Method \u2693\ufe0e As part of landing zone design, choose the appropriate hybrid authentication method for the client, using this decision tree In addition to the decision tree, CAF provides common scenarios & recommendations to choose the appropriate hybrid identity option for a client. Additional Azure AD services when choosing Hybrid Identity \u2693\ufe0e There are scenarios where additional Azure AD services and components may be required, explained below: \u2022 Identify on-Prem applications that rely on domain services such as domain join, group policy, use older protocols like lightweight directory access protocol (LDAP), Kerberos/NTLM authentication. These applications require Azure AD Domain Services to run in Azure. Alternately, the same can be achieved by using a Self-Managed AD Domain Services model, whereby AD domain controllers are deployed on Azure VMs, benefit being that additional customization such as extending the schema or creating forest trusts are possible. Self-Managed AD DS is recommended only when such customizations are absolutely needed. \u2022 Identify on-prem applications that require authentication and single sign-on using Azure AD. Azure AD Application Proxy service provides this function as well as remote access to on-premise applications either using external or internal URLs. \u2022 There may be requirements for client organizations to manage and secure external users, including customers & partners. This is addressed by Azure AD External Identities , which includes B2B & B2C collaborations. This applies to both cloud-only & hybrid identities \u2022 Pricing : Note that some of the above functions may require different pricing editions, which need to be considered as part of the selection process. When to create \"Identity Subscription\" \u2693\ufe0e The only scenario where a separate subscription for identity \"may\" be needed is when deploying Self-Managed AD Domain Services instead of \"Azure AD Domain Services\", whereby AD domain controllers are deployed on VMs in Azure. It is recommended to have the AD domain controllers in a unique subscription in order to separate roles and responsibilities for managing the AD DCs versus other entities. Enabling Hybrid Identity \u2693\ufe0e Above guidance focuses only choosing the correct identity methods. In order to enable and use hybrid identity, Azure AD connect is needed to sync between on-premises identity system and Azure AD. Note: Configuration of Azure AD connect is out-of-scope for this design guidance. Access Management \u2693\ufe0e In addition to choosing the correct identity method, organizations should define appropriate access controls to access Azure resources by following a least privileged approach to operational access. This is facilitated in Azure through RBAC and roles. Note that there are 2 different roles in Azure: 1) Azure AD Roles which manages access to administer Azure AD, including creation of users and groups. o Only important Azure AD Roles are shown in the above link. o There are additional built-in Azure AD Roles 2) Azure Roles to manage access to Azure resources such as compute and storage. Azure Roles can be classified into three groups: o Fundamental Roles . o several built-in roles &. o Optional custom roles Role definitions for Landing Zones \u2693\ufe0e As part of landing zone creation, it is important to define access controls for the various Landing Zone elements. Table below lists the required personas, responsibilities to be performed by these personas, and the corresponding Azure roles to be used. \u2022 These are the minimum set of roles that should be defined for any account irrespective of the LZ baseline/reference architectures . \u2022 Scope of these roles can be applied at subscription or management groups or at resource group level depending on the scenario. Scope level details will be provided in a separate section. Persona (=Groups in Azure AD) Responsibilities Azure Roles (Built-In or Custom) Scope (Set of resources this access applies to) Azure Platform Admin (Group) Mgmt Group & Subscription Lifecycle Mgmt \u2022 Owner NOTE: There should be more than one person associated with this role, and not more than three All Subscriptions Identity & Access Admin Assign permissions to Users/Groups/Service Principals \u2022 User Access Administrator, AAD Role: \u201cUser Administrator\u201d All Subscriptions Patching & Compliance Compute \u2022 Virtual Machine Contributor All Subscriptions Security Admin Horizontal security responsibility across the entire Azure estate and the Azure Key Vault purge policy Security Admin,Sentinel Contributor,Sentinel Automation Contributor All Subscriptions Storage Admin Management of Storage Accounts Storage Account Contributor All Subscriptions Network Admin Connectivity management: Virtual networks, UDRs, NSGs, NVAs, VPN, Azure ExpressRoute, and others \u2022 Network Contributor All Subscriptions Backup/Restore Admin Account wide management of Backup and Restore \u2022 Backup Contributor All Subscriptions VM Admin Account wide management of Backup and Restore \u2022 Virtual Machine Contributor All Subscriptions SysOps/ Distributed Ops/AppOps Operational view of platform and application elements \u2022 Reader \u2022 Management Subscriptions, optionally add other application subscriptions Automation & DevOps Admin (Group), Contains User and Service Principals Use Automation for Day 1 provisioning and use automation for Day 2 operations \u2022 Contributor All Subscriptions App Owners Application Owners, to create and manage workloads \u2022 Contributor \u2022 App specific subscriptions Mock Group for PoCs \u2022 Depending on PoC \u2022 Depends on PoC Standard Best Practices to be implemented for IAM \u2693\ufe0e Identity & Access management goes beyond identifying the appropriate Azure services for authentication and access controls. Additional controls like Conditional Access, Multi-factor authentication(MFA), Privileged Identity Management (PIM) should also be considered, as defined in Azure IAM security best practices for Landing Zones There are mandatory controls that need to be implemented for every account as part of identity management: a. Multi-Factor Authentication (MFA) should be enabled by default through risk based conditional access policies. b. For auditing purposes, user sign-in should be monitored and logs retained for at least 365 days or longer if an account needs so. \u2022 This can be achieved by enabling Azure AD Audit Logs & Sign-in Logs to be sent to various destinations for storing. At the minimum, logs should be sent to a storage account & a log workspace monitored by security operations. Primary & Secondary Controls for IAM \u2693\ufe0e Kyndryl\u2019s Global IAM process has defined mandatory Primary & Secondary control processes that need to be followed for every account. Primary Controls : Azure AD has capabilities to integrate with HR applications or (Human Capital Management) systems like Workday, Success Factors or any HR application. Custom approval workflows could also be defined by integrating with Azure AD with client's approval systems. These integrations have not been validated by Kyndryl yet. Hence alternate primary controls should be implemented using existing account processes or implementing custom processes. It is also possible to use Azure native PIM function for user ID approval process, which does not need any external integrations. Secondary Controls : It is possible to establish secondary controls using Azure native services Access Reviews and Privileged Identity Management . Recommendation for enforcing Primary & Secondary controls: Currently, there are no specific guidelines on how this will be enforced. Investigations are in progress and will be updated accordingly. Primary & Secondary Controls for IAM \u2013 Machine Identities \u2693\ufe0e It is important to establish secure methods for machine-to-machine communication, similar to the controls listed above for human personas. Machines here mean VMs connecting to databases, applications, websites, container workloads, IoT, and so on. In Azure, machine to machine authentication is established using Managed Identities. Primary & Secondary controls should be established for Managed Identities too. Currently, there are no specific guidelines on how this will be enforced. Investigations are in progress and will be updated accordingly. Design Actions by Solution Architects \u2693\ufe0e o Define Identity Model for the account. o Use the decision tree above to define the appropriate Hybrid authentication method. o Review personas & roles with client and add any account specific roles and role definitions. o Review with account team options for enabling Primary & Secondary controls. o Ensure Azure Active Directory(AAD) P2 licensing is included in the solution - to address primary & secondary controls","title":"2.Identity & Access"},{"location":"lzdesign/iam/#identity-access-management","text":"","title":"Identity &amp; Access Management"},{"location":"lzdesign/iam/#pre-requisite-reading","text":"Solution Architect is expected to be familiar with Azure AD basics and Azure RBAC constructs, listed below. Azure Active Directory(AAD) Fundamentals & Features . Azure RBAC, Security Principle, Role Definition, Scope & Role Assignments","title":"Pre-Requisite Reading"},{"location":"lzdesign/iam/#kyndryl-itss-standards","text":"Recommendations given in this section \u201caligns\u201d with Kyndryl\u2019s IT Security Standards for Cloud/SaaS User Access Management , wherever feasible.","title":"Kyndryl ITSS standards"},{"location":"lzdesign/iam/#identity-model","text":"A critical design element in an organization\u2019s cloud adoption plan is choosing the appropriate authentication method for the resources to be hosted in cloud. Authentication is also a critical element in the Landing Zone design. And this starts with choosing the correct identity model for the organization: 1. Cloud only identity : Create a brand-new identity/authentication system in the cloud, applicable to organizations which are \u2018pure cloud\u2019 \u2013 meaning they do not have any on-prem resources. 2. Hybrid Identity : Extend an existing on-premises identity domain into the cloud, applicable to \u2018hybrid\u2019 organizations which have on-prem resources. This solution creates a common user identity for authentication and authorization to all resources, regardless of location. And most organizations fall into this category as they start their cloud adoption journey. o When using Hybrid Identity all User identities are defined in on-premise identity systems which are then synched to Azure AD. Users are then assigned roles and permissions using Azure RBAC. For \u2018Pure Cloud\u2019 organizations the decision is easy since the only option is to use Azure Active Directory (AAD) for Identity and Access management. However, for organizations that need to use Hybrid identity, there are different hybrid authentication methods available to choose from.","title":"Identity Model"},{"location":"lzdesign/iam/#hybrid-authentication-methods","text":"To achieve hybrid identity with Azure AD (AAD), one of three authentication methods explained below can be used: \u2022 Password hash synchronization (PHS). \u2022 Pass-through authentication (PTA). \u2022 Federation (AD FS) \u2013 In this authentication method Azure AD will hand off the authentication process to a separate trusted authentication system, such as AD FS or a third-party federation system, to validate the user's sign-in. All above authentication methods also provide single sign-on capabilities. In addition, Azure Active Directory Seamless Single Sign-On (Azure AD Seamless SSO) automatically signs users in when they are on their corporate devices connected to their corporate network. When enabled, users don't need to type in their passwords to sign in to Azure AD, and usually, don\u2019t even type in their usernames. This feature provides users easy access to cloud-based applications without needing any additional on-premises components.","title":"Hybrid Authentication Methods"},{"location":"lzdesign/iam/#decision-tree-to-choose-a-hybrid-authentication-method","text":"As part of landing zone design, choose the appropriate hybrid authentication method for the client, using this decision tree In addition to the decision tree, CAF provides common scenarios & recommendations to choose the appropriate hybrid identity option for a client.","title":"Decision Tree to choose a Hybrid Authentication Method"},{"location":"lzdesign/iam/#additional-azure-ad-services-when-choosing-hybrid-identity","text":"There are scenarios where additional Azure AD services and components may be required, explained below: \u2022 Identify on-Prem applications that rely on domain services such as domain join, group policy, use older protocols like lightweight directory access protocol (LDAP), Kerberos/NTLM authentication. These applications require Azure AD Domain Services to run in Azure. Alternately, the same can be achieved by using a Self-Managed AD Domain Services model, whereby AD domain controllers are deployed on Azure VMs, benefit being that additional customization such as extending the schema or creating forest trusts are possible. Self-Managed AD DS is recommended only when such customizations are absolutely needed. \u2022 Identify on-prem applications that require authentication and single sign-on using Azure AD. Azure AD Application Proxy service provides this function as well as remote access to on-premise applications either using external or internal URLs. \u2022 There may be requirements for client organizations to manage and secure external users, including customers & partners. This is addressed by Azure AD External Identities , which includes B2B & B2C collaborations. This applies to both cloud-only & hybrid identities \u2022 Pricing : Note that some of the above functions may require different pricing editions, which need to be considered as part of the selection process.","title":"Additional Azure AD services when choosing Hybrid Identity"},{"location":"lzdesign/iam/#when-to-create-identity-subscription","text":"The only scenario where a separate subscription for identity \"may\" be needed is when deploying Self-Managed AD Domain Services instead of \"Azure AD Domain Services\", whereby AD domain controllers are deployed on VMs in Azure. It is recommended to have the AD domain controllers in a unique subscription in order to separate roles and responsibilities for managing the AD DCs versus other entities.","title":"When to create \"Identity Subscription\""},{"location":"lzdesign/iam/#enabling-hybrid-identity","text":"Above guidance focuses only choosing the correct identity methods. In order to enable and use hybrid identity, Azure AD connect is needed to sync between on-premises identity system and Azure AD. Note: Configuration of Azure AD connect is out-of-scope for this design guidance.","title":"Enabling Hybrid Identity"},{"location":"lzdesign/iam/#access-management","text":"In addition to choosing the correct identity method, organizations should define appropriate access controls to access Azure resources by following a least privileged approach to operational access. This is facilitated in Azure through RBAC and roles. Note that there are 2 different roles in Azure: 1) Azure AD Roles which manages access to administer Azure AD, including creation of users and groups. o Only important Azure AD Roles are shown in the above link. o There are additional built-in Azure AD Roles 2) Azure Roles to manage access to Azure resources such as compute and storage. Azure Roles can be classified into three groups: o Fundamental Roles . o several built-in roles &. o Optional custom roles","title":"Access Management"},{"location":"lzdesign/iam/#role-definitions-for-landing-zones","text":"As part of landing zone creation, it is important to define access controls for the various Landing Zone elements. Table below lists the required personas, responsibilities to be performed by these personas, and the corresponding Azure roles to be used. \u2022 These are the minimum set of roles that should be defined for any account irrespective of the LZ baseline/reference architectures . \u2022 Scope of these roles can be applied at subscription or management groups or at resource group level depending on the scenario. Scope level details will be provided in a separate section. Persona (=Groups in Azure AD) Responsibilities Azure Roles (Built-In or Custom) Scope (Set of resources this access applies to) Azure Platform Admin (Group) Mgmt Group & Subscription Lifecycle Mgmt \u2022 Owner NOTE: There should be more than one person associated with this role, and not more than three All Subscriptions Identity & Access Admin Assign permissions to Users/Groups/Service Principals \u2022 User Access Administrator, AAD Role: \u201cUser Administrator\u201d All Subscriptions Patching & Compliance Compute \u2022 Virtual Machine Contributor All Subscriptions Security Admin Horizontal security responsibility across the entire Azure estate and the Azure Key Vault purge policy Security Admin,Sentinel Contributor,Sentinel Automation Contributor All Subscriptions Storage Admin Management of Storage Accounts Storage Account Contributor All Subscriptions Network Admin Connectivity management: Virtual networks, UDRs, NSGs, NVAs, VPN, Azure ExpressRoute, and others \u2022 Network Contributor All Subscriptions Backup/Restore Admin Account wide management of Backup and Restore \u2022 Backup Contributor All Subscriptions VM Admin Account wide management of Backup and Restore \u2022 Virtual Machine Contributor All Subscriptions SysOps/ Distributed Ops/AppOps Operational view of platform and application elements \u2022 Reader \u2022 Management Subscriptions, optionally add other application subscriptions Automation & DevOps Admin (Group), Contains User and Service Principals Use Automation for Day 1 provisioning and use automation for Day 2 operations \u2022 Contributor All Subscriptions App Owners Application Owners, to create and manage workloads \u2022 Contributor \u2022 App specific subscriptions Mock Group for PoCs \u2022 Depending on PoC \u2022 Depends on PoC","title":"Role definitions for Landing Zones"},{"location":"lzdesign/iam/#standard-best-practices-to-be-implemented-for-iam","text":"Identity & Access management goes beyond identifying the appropriate Azure services for authentication and access controls. Additional controls like Conditional Access, Multi-factor authentication(MFA), Privileged Identity Management (PIM) should also be considered, as defined in Azure IAM security best practices for Landing Zones There are mandatory controls that need to be implemented for every account as part of identity management: a. Multi-Factor Authentication (MFA) should be enabled by default through risk based conditional access policies. b. For auditing purposes, user sign-in should be monitored and logs retained for at least 365 days or longer if an account needs so. \u2022 This can be achieved by enabling Azure AD Audit Logs & Sign-in Logs to be sent to various destinations for storing. At the minimum, logs should be sent to a storage account & a log workspace monitored by security operations.","title":"Standard Best Practices to be implemented for IAM"},{"location":"lzdesign/iam/#primary-secondary-controls-for-iam","text":"Kyndryl\u2019s Global IAM process has defined mandatory Primary & Secondary control processes that need to be followed for every account. Primary Controls : Azure AD has capabilities to integrate with HR applications or (Human Capital Management) systems like Workday, Success Factors or any HR application. Custom approval workflows could also be defined by integrating with Azure AD with client's approval systems. These integrations have not been validated by Kyndryl yet. Hence alternate primary controls should be implemented using existing account processes or implementing custom processes. It is also possible to use Azure native PIM function for user ID approval process, which does not need any external integrations. Secondary Controls : It is possible to establish secondary controls using Azure native services Access Reviews and Privileged Identity Management . Recommendation for enforcing Primary & Secondary controls: Currently, there are no specific guidelines on how this will be enforced. Investigations are in progress and will be updated accordingly.","title":"Primary &amp; Secondary Controls for IAM"},{"location":"lzdesign/iam/#primary-secondary-controls-for-iam-machine-identities","text":"It is important to establish secure methods for machine-to-machine communication, similar to the controls listed above for human personas. Machines here mean VMs connecting to databases, applications, websites, container workloads, IoT, and so on. In Azure, machine to machine authentication is established using Managed Identities. Primary & Secondary controls should be established for Managed Identities too. Currently, there are no specific guidelines on how this will be enforced. Investigations are in progress and will be updated accordingly.","title":"Primary &amp; Secondary Controls for IAM \u2013 Machine Identities"},{"location":"lzdesign/iam/#design-actions-by-solution-architects","text":"o Define Identity Model for the account. o Use the decision tree above to define the appropriate Hybrid authentication method. o Review personas & roles with client and add any account specific roles and role definitions. o Review with account team options for enabling Primary & Secondary controls. o Ensure Azure Active Directory(AAD) P2 licensing is included in the solution - to address primary & secondary controls","title":"Design Actions by Solution Architects"},{"location":"lzdesign/lzdesignapproach/","text":"Landing Zone Design Approach \u2693\ufe0e An Azure landing zone results from a multi-subscription Azure environment that addresses scale, security governance, networking, and identity. Azure landing zones are pre-provisioned through code and enable application migration, modernization, and innovation at enterprise-scale in Azure. These zones consider all platform resources that are required to support the customer's application portfolio and do not differentiate between infrastructure as a service or platform as a service. Azure CAF provides two approaches for implementing landing zones. Start small and expand -- The intent is to support low-risk workloads and with little or no focus on the Govern and Manage methodologies. Enterprise-scale -- The intent is to support mission-critical workloads and with comprehensive application of the Govern and Manage methodologies. This document will focus on the enterprise-scale approach. The rest of this section covers, A component/services view of a Landing Zone, identifying all the parts that together compose a landing zone. A brief description of the enterprise-scale architectural approach to the construction of landing zones. Critical design areas to help translate requirements to Azure constructs and capabilities that in turn constitute a landing zone. Enterprise-scale reference architectures for a landing zone based on three different network topologies. Solution guidance in terms of the assets and steps involved to create a landing zone. Design principles that guide the technical decisions across critical design areas. Landing Zone -- Component/Services View \u2693\ufe0e The view in the above diagram shows multiple applications hosted in the landing zone using shared services for identity and access management, policies, network, management, and monitoring. Each application leveraged platform resources such as VMs, Storage, Databases and has application-specific identity and access management, policies, keys, monitoring and management agents. Enterprise-Scale Architecture \u2693\ufe0e The enterprise-scale architecture provides prescriptive guidance coupled with best practices for the Azure control plane. It follows design principles across the critical design areas for an organization\\'s Azure environment. The enterprise-scale architecture is modular by design. It allows starting with a foundational landing zone control plane that support application portfolios, no matter whether the applications are being migrated or are newly developed and deployed to Azure. The architecture can scale alongside business requirements regardless of scale point. Critical Design Areas \u2693\ufe0e The core of enterprise-scale architecture, as defined by CAF, contains a critical design path comprised of eight fundamental design areas with heavily interrelated and dependent design decisions. Overview of the eight critical design areas are listed below: No. Design Areas Objective 1 Enterprise Enrolment For enterprise customers with an Azure commitment, proper tenant creation and enrollment is an important early step 2 Identity & Access Management Identity and access management is a primary security boundary in the public cloud. It\\'s the foundation for any secure and fully compliant architecture 3 Network topology and connectivity Networking and connectivity decisions are an equally important foundational aspect of any cloud architecture 4 Resource organization As cloud adoption scales, considerations for subscription design and management group hierarchy have an impact on governance, operations management, and adoption patterns 5 Governance disciplines Automate auditing and enforcement of security governance and compliance policies 6 Operations baseline For stable, ongoing operations in the cloud, an operations baseline is required to provide visibility, operations compliance, and protect and recover capabilities 7 Business continuity and disaster recovery (BCDR) Resiliency is key for smooth functioning of applications. BCDR is an important component of resiliency. BCDR involves protection of data via backups and protection of applications from outages via disaster recovery 8 Deployment options Align the best tools and templates to deploy your landing zones and supporting resources These fundamental design areas can be categorized as Foundational & Core elements from a solutioning perspective, as shown below. This document provides design guidance for each of the above eight critical design areas , in separate sections. Each section provides guidance on design considerations, standard and optional recommendations to be followed. Reference Architectures \u2693\ufe0e Three reference architectures are described below based on a particular network topology. Each extends the Component/Services view described previously into a deployment model showing the logical location of the various platform services. Enterprise-scale architecture -- Hub-spoke with Virtual Network Gateways for Small Enterprises \u2693\ufe0e This reference architecture provides a design path and initial technical state for Small Enterprises to start with foundational landing zones that support their application portfolios. It is meant for organizations that do not have a large IT team and do not require fine grained administration delegation models. Hence, Management, Connectivity and Identity resources are consolidated in a single Platform Subscription. The connectivity resource group within the Platform subscription leverages the hub and spoke topology with Virtual Network gateways. In the event connectivity to customer premises is not needed, the connectivity resource group within the Platform subscription can be dispensed with. If the business requirements change over time, the architecture allows for creating additional subscriptions and placing them into the suitable management group and assigning Azure policies. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-scale for small enterprises documentation . Enterprise-scale architecture -- Hub-spoke with Virtual Network Gateways for Medium and Large Enterprises \u2693\ufe0e This reference architecture is well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure by implementing a network architecture based on the traditional hub and spoke network topology leveraging VPN/Express Route gateways. It is meant for organizations that have large, decentralised IT teams. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-Scale with hub and spoke architecture documentation . Enterprise-scale architecture -- Hub-spoke with Virtual WAN for Medium and Large Enterprises \u2693\ufe0e This reference architecture is also well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure, where, in addition to the needs of large, decentralised teams, a global transit network is required, including hybrid connectivity to on-premises datacentres and branch offices via ExpressRoute and VPN. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-Scale with Azure VWAN documentation . Other Architectures \u2693\ufe0e Azure CAF provides a set of implementation options that can be leveraged for rapid creation of a landing zone. The options provided support combinations of the various options to add on functionality and services as needed. The options provided include the three reference architectures described above. A User Guide exists for the implementation of the \"Hub-spoke architecture with Virtual Network Gateways\" architecture. Landing Zone Guidance Assets \u2693\ufe0e The enterprise-scale approach to construct landing zones includes three sets of assets to support cloud solution teams: Design guidelines derived from evaluation of requirements against critical design areas that drive the design of the enterprise-scale landing zone. Reference architectures that demonstrate design areas and best practices. Azure Resource Manager templates that enable rapid deployment of the reference architectures.(in a later release) Design Principles \u2693\ufe0e The principles in the table below serve as a compass for subsequent design decisions across critical technical domains. Subscription democratization - Subscriptions should be used as a unit of management and scale. Policy-driven governance -- Use Azure Policy to provide guardrails and ensure continued compliance of platform and applications. Single control and management plane - unified and consistent control plane across all Azure resources and provisioning channels Application-centric and archetype-neutral -- No differentiation between applications (old/new, IaaS/PaaS) Recommendations -- Use preview services and service roadmaps to overcome technical blockers.","title":"Design Approach"},{"location":"lzdesign/lzdesignapproach/#landing-zone-design-approach","text":"An Azure landing zone results from a multi-subscription Azure environment that addresses scale, security governance, networking, and identity. Azure landing zones are pre-provisioned through code and enable application migration, modernization, and innovation at enterprise-scale in Azure. These zones consider all platform resources that are required to support the customer's application portfolio and do not differentiate between infrastructure as a service or platform as a service. Azure CAF provides two approaches for implementing landing zones. Start small and expand -- The intent is to support low-risk workloads and with little or no focus on the Govern and Manage methodologies. Enterprise-scale -- The intent is to support mission-critical workloads and with comprehensive application of the Govern and Manage methodologies. This document will focus on the enterprise-scale approach. The rest of this section covers, A component/services view of a Landing Zone, identifying all the parts that together compose a landing zone. A brief description of the enterprise-scale architectural approach to the construction of landing zones. Critical design areas to help translate requirements to Azure constructs and capabilities that in turn constitute a landing zone. Enterprise-scale reference architectures for a landing zone based on three different network topologies. Solution guidance in terms of the assets and steps involved to create a landing zone. Design principles that guide the technical decisions across critical design areas.","title":"Landing Zone Design Approach"},{"location":"lzdesign/lzdesignapproach/#landing-zone-componentservices-view","text":"The view in the above diagram shows multiple applications hosted in the landing zone using shared services for identity and access management, policies, network, management, and monitoring. Each application leveraged platform resources such as VMs, Storage, Databases and has application-specific identity and access management, policies, keys, monitoring and management agents.","title":"Landing Zone -- Component/Services View"},{"location":"lzdesign/lzdesignapproach/#enterprise-scale-architecture","text":"The enterprise-scale architecture provides prescriptive guidance coupled with best practices for the Azure control plane. It follows design principles across the critical design areas for an organization\\'s Azure environment. The enterprise-scale architecture is modular by design. It allows starting with a foundational landing zone control plane that support application portfolios, no matter whether the applications are being migrated or are newly developed and deployed to Azure. The architecture can scale alongside business requirements regardless of scale point.","title":"Enterprise-Scale Architecture"},{"location":"lzdesign/lzdesignapproach/#critical-design-areas","text":"The core of enterprise-scale architecture, as defined by CAF, contains a critical design path comprised of eight fundamental design areas with heavily interrelated and dependent design decisions. Overview of the eight critical design areas are listed below: No. Design Areas Objective 1 Enterprise Enrolment For enterprise customers with an Azure commitment, proper tenant creation and enrollment is an important early step 2 Identity & Access Management Identity and access management is a primary security boundary in the public cloud. It\\'s the foundation for any secure and fully compliant architecture 3 Network topology and connectivity Networking and connectivity decisions are an equally important foundational aspect of any cloud architecture 4 Resource organization As cloud adoption scales, considerations for subscription design and management group hierarchy have an impact on governance, operations management, and adoption patterns 5 Governance disciplines Automate auditing and enforcement of security governance and compliance policies 6 Operations baseline For stable, ongoing operations in the cloud, an operations baseline is required to provide visibility, operations compliance, and protect and recover capabilities 7 Business continuity and disaster recovery (BCDR) Resiliency is key for smooth functioning of applications. BCDR is an important component of resiliency. BCDR involves protection of data via backups and protection of applications from outages via disaster recovery 8 Deployment options Align the best tools and templates to deploy your landing zones and supporting resources These fundamental design areas can be categorized as Foundational & Core elements from a solutioning perspective, as shown below. This document provides design guidance for each of the above eight critical design areas , in separate sections. Each section provides guidance on design considerations, standard and optional recommendations to be followed.","title":"Critical Design Areas"},{"location":"lzdesign/lzdesignapproach/#reference-architectures","text":"Three reference architectures are described below based on a particular network topology. Each extends the Component/Services view described previously into a deployment model showing the logical location of the various platform services.","title":"Reference Architectures"},{"location":"lzdesign/lzdesignapproach/#enterprise-scale-architecture-hub-spoke-with-virtual-network-gateways-for-small-enterprises","text":"This reference architecture provides a design path and initial technical state for Small Enterprises to start with foundational landing zones that support their application portfolios. It is meant for organizations that do not have a large IT team and do not require fine grained administration delegation models. Hence, Management, Connectivity and Identity resources are consolidated in a single Platform Subscription. The connectivity resource group within the Platform subscription leverages the hub and spoke topology with Virtual Network gateways. In the event connectivity to customer premises is not needed, the connectivity resource group within the Platform subscription can be dispensed with. If the business requirements change over time, the architecture allows for creating additional subscriptions and placing them into the suitable management group and assigning Azure policies. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-scale for small enterprises documentation .","title":"Enterprise-scale architecture -- Hub-spoke with Virtual Network Gateways for Small Enterprises"},{"location":"lzdesign/lzdesignapproach/#enterprise-scale-architecture-hub-spoke-with-virtual-network-gateways-for-medium-and-large-enterprises","text":"This reference architecture is well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure by implementing a network architecture based on the traditional hub and spoke network topology leveraging VPN/Express Route gateways. It is meant for organizations that have large, decentralised IT teams. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-Scale with hub and spoke architecture documentation .","title":"Enterprise-scale architecture -- Hub-spoke with Virtual Network Gateways for Medium and Large Enterprises"},{"location":"lzdesign/lzdesignapproach/#enterprise-scale-architecture-hub-spoke-with-virtual-wan-for-medium-and-large-enterprises","text":"This reference architecture is also well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure, where, in addition to the needs of large, decentralised teams, a global transit network is required, including hybrid connectivity to on-premises datacentres and branch offices via ExpressRoute and VPN. A detailed description of the various resources deployed in the above reference architecture can be obtained from Deploy Enterprise-Scale with Azure VWAN documentation .","title":"Enterprise-scale architecture -- Hub-spoke with Virtual WAN for Medium and Large Enterprises"},{"location":"lzdesign/lzdesignapproach/#other-architectures","text":"Azure CAF provides a set of implementation options that can be leveraged for rapid creation of a landing zone. The options provided support combinations of the various options to add on functionality and services as needed. The options provided include the three reference architectures described above. A User Guide exists for the implementation of the \"Hub-spoke architecture with Virtual Network Gateways\" architecture.","title":"Other Architectures"},{"location":"lzdesign/lzdesignapproach/#landing-zone-guidance-assets","text":"The enterprise-scale approach to construct landing zones includes three sets of assets to support cloud solution teams: Design guidelines derived from evaluation of requirements against critical design areas that drive the design of the enterprise-scale landing zone. Reference architectures that demonstrate design areas and best practices. Azure Resource Manager templates that enable rapid deployment of the reference architectures.(in a later release)","title":"Landing Zone Guidance Assets"},{"location":"lzdesign/lzdesignapproach/#design-principles","text":"The principles in the table below serve as a compass for subsequent design decisions across critical technical domains. Subscription democratization - Subscriptions should be used as a unit of management and scale. Policy-driven governance -- Use Azure Policy to provide guardrails and ensure continued compliance of platform and applications. Single control and management plane - unified and consistent control plane across all Azure resources and provisioning channels Application-centric and archetype-neutral -- No differentiation between applications (old/new, IaaS/PaaS) Recommendations -- Use preview services and service roadmaps to overcome technical blockers.","title":"Design Principles"},{"location":"lzdesign/lzdesignoverview/","text":"Landing Zone Design Overview \u2693\ufe0e Purpose of this document is to guide Kyndryl solution architects to design Azure Landing Zone for Kyndryl managed accounts. This document will provide guidance on critical aspects of Landing Zone design and standards that need to be followed. Guidance in this document is based upon IBM Cloud Adoption Design (CAD) and Microsoft\u2019s Cloud Adoption Framework (CAF). The Cloud Adoption Framework is a full lifecycle framework, supporting customers throughout each phase of adoption. Guidance in this document focuses on the \u201cReady\u201d, \u201cGovern\u201d and \u201cManage\u201d phases. Landing Zone Design Scope \u2693\ufe0e This version of the document will explain in detail the options and recommendations to design a landing zone design for a client, with focus on the below areas: a) Landing zone design for hosting the platform services in support of the workloads and not the workloads themselves. This constitutes the Azure control and management plane for the landing zone. b) General recommendations/best practices to be implemented. Specific details within each of the areas \u2013 for example: Policies, Role Definitions, Role Assignments, Scope, Monitoring Metrics are NOT included in this version. These will be added in future sprints. c) Implementation (how to) details are NOT included in this version. They will be added in future sprints. d) The goal of this document is to provide a template to capture all the design elements, which could then be used as the source for implementing Landing Zone through automation. This template will be provided in a future release. Account Solution Document Template \u2693\ufe0e Solution Architects should develop a solution doc for each account, which explains Landing Zone design, based on the guidelines and standards defined in this guide. This is an audit requirement as well as a compliance requirement that is part of Azure MSP certification A Solution Design template is provided which shall be used by the Solution Architects.","title":"Overview & Scope"},{"location":"lzdesign/lzdesignoverview/#landing-zone-design-overview","text":"Purpose of this document is to guide Kyndryl solution architects to design Azure Landing Zone for Kyndryl managed accounts. This document will provide guidance on critical aspects of Landing Zone design and standards that need to be followed. Guidance in this document is based upon IBM Cloud Adoption Design (CAD) and Microsoft\u2019s Cloud Adoption Framework (CAF). The Cloud Adoption Framework is a full lifecycle framework, supporting customers throughout each phase of adoption. Guidance in this document focuses on the \u201cReady\u201d, \u201cGovern\u201d and \u201cManage\u201d phases.","title":"Landing Zone Design Overview"},{"location":"lzdesign/lzdesignoverview/#landing-zone-design-scope","text":"This version of the document will explain in detail the options and recommendations to design a landing zone design for a client, with focus on the below areas: a) Landing zone design for hosting the platform services in support of the workloads and not the workloads themselves. This constitutes the Azure control and management plane for the landing zone. b) General recommendations/best practices to be implemented. Specific details within each of the areas \u2013 for example: Policies, Role Definitions, Role Assignments, Scope, Monitoring Metrics are NOT included in this version. These will be added in future sprints. c) Implementation (how to) details are NOT included in this version. They will be added in future sprints. d) The goal of this document is to provide a template to capture all the design elements, which could then be used as the source for implementing Landing Zone through automation. This template will be provided in a future release.","title":"Landing Zone Design Scope"},{"location":"lzdesign/lzdesignoverview/#account-solution-document-template","text":"Solution Architects should develop a solution doc for each account, which explains Landing Zone design, based on the guidelines and standards defined in this guide. This is an audit requirement as well as a compliance requirement that is part of Azure MSP certification A Solution Design template is provided which shall be used by the Solution Architects.","title":"Account Solution Document Template"},{"location":"lzdesign/monitoringmgmt/","text":"Monitoring & Management \u2693\ufe0e Centralized monitoring and management of all IT and Application infrastructure, along with associated middleware and security elements is important across on-premises, hybrid and multi-cloud environments. Microsoft\u2019s cloud adoption framework (CAF) prescribes monitoring across Infrastructure, Applications, Security, and all cloud services utilized by a client and service provider organization. These include Monitoring health, performance and security of IT resources Gathering and analyzing logs across all services and applications Gathering and analyzing metrics across all services and applications Extract meaningful information from the analysis, identify actionable incidents and remediate the same reactively and pro-actively This document starts with a description of Azure functions and components that support monitoring in addition to an overview of Azure monitoring architecture and Security Management Architecture. It then proceeds to discuss Kyndryl offerings that integrate Azure monitoring into Service Management solutions. It concludes with a discussion on design considerations and recommendations for monitoring. Overview of Azure services for Monitoring & Management \u2693\ufe0e Azure provides a comprehensive set of tools and services to monitor across infrastructure, security, cloud services and application stacks. Both metrics data and logs are gathered across Identity and Access Management, Azure resources, Subscriptions, Operating systems, Applications that can be integrated to other tools either through Logic Apps or APIs Based on evaluation of the data against certain conditions, alerts can be triggered and also, autoscaling of resources such as Virtual Machine Scale Sets can be performed Tools for analyzing both metrics data and logs are available Visualization tools for presentation of either raw data or filtered data resulting from queries are available Tools to create insights from the analysis of data across Virtual Machines, Containers and Applications are available. For cloud-native Security Information and Event Management (SIEM), Azure Sentinel should be leveraged which helps in analyzing large volumes of data across an enterprise with built-in AI. Azure Monitoring Architecture \u2693\ufe0e This section presents an architectural overview of the monitoring solution in Azure. The intent is to give the readers a high-level understanding of the various components involved in Monitoring and Logging across the Azure platform and services and how these components complement one another in providing a comprehensive solution. The diagram below illustrates the architecture. The centerpiece of the Monitoring solution is the Azure Monitor. This consists of the Metrics component and the Logs component. Metrics are numerical values. The Metrics component gathers the metrics into a fast, lightweight time series database across all those components that generate them. Metrics can be viewed in less than 60 seconds post their occurrence. The intent is to be able to react in near real time to any issue based on an analysis of these metrics. The Logs component gathers both metrics and logs across all those components that generate them and can store them in different stores or targets. This is a slower process and intended more for analysis of data gathered over a period. Those components that do not rely on agents send the metrics and the logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub. For those that depend on agents, Log Analytics is the only possible destination. The following sections cover in detail, the sources of metrics and logs, the target destinations they can be directed to, the visualization and analytics that can be performed on the data and the raising of alerts based on the detection of certain conditions in the data. Sources of data \u2693\ufe0e This section briefly covers all those Azure resources that generate metrics and logs. It is to be emphasize that where metrics are generated, they are sent to both the fast time series database and the slow targets. Azure Active Directory (AAD) \u2693\ufe0e AAD creates a variety of logs (audit, sign-in, provisioning). AAD can be configured to direct these logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub. Subscription \u2693\ufe0e Each subscription generates an Activity log that encompasses actions related to the lifecycle of Azure resources. The Activity log also contains information generated by the Service Health tool that includes Resource alerts, Health issues/advisories/alerts, Planned Maintenance and Security advisories. A subscription can be configured to direct the activity log to one or more of destinations such as Log Analytics, Storage Account or Event Hub. ARM Resources \u2693\ufe0e Each resource generates metrics as numerical values that are unique to itself. Each resource also generates logs as both numerical and string values that are unique to itself. Note that the metrics and logs here relate to the ARM resources as known to the virtualization layer hosting them. A resource can be configured to direct its metrics and logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub. Guest OS \u2693\ufe0e For resources such as VMs that run Guest Operating Systems, additional metrics and logs are generated through agents running on those resources. These resources need not be just ARM resources. They could be VMs running for example on either customer premises or in other clouds. The metrics and logs from these resources can only be directed to Log Analytics. Applications \u2693\ufe0e Applications can be monitored either through codeless attach procedures (J2EE/.NET runtime monitoring is an example) or through compile-time instrumentation. The metrics and logs from these resources can only be directed to Log Analytics. The resulting metrics and logs can be analyzed by the App Insights tool to diagnose issues. Miscellaneous sources \u2693\ufe0e There are certain solutions such as Hadoop and Kubernetes clusters that generate, store, and process their metrics and logs differently. It is also possible to generate metrics and logs for custom resources through the ARM REST API. The metrics and logs from these resources can only be directed to Log Analytics. Destinations for data \u2693\ufe0e This section briefly describes the three main possible target destinations for the metrics and logs generated by various Azure resources. Again, metrics are always sent to the time series database in addition to the targets that logs are sent to. Storage Account \u2693\ufe0e One potential destination for the generated metrics and logs is a storage account. The benefit of using a storage account is the low cost that enables long term storage or archival. Event Hub \u2693\ufe0e A second potential destination for the generated metrics and logs is an Event Hub. The benefit of this is the ability to send the data to multiple third-party solutions that can provide more sophisticated analysis of the data. Log Analytics \u2693\ufe0e This is a workspace that is part of the Azure Monitor Logs tool. A workspace is a combination of storage space in the form of tables and processing capabilities that allow queries to be run against the data to extract insights. In general, there is a cost to both ingestion of the data as well as the storage. By default, the data is retained for 2 years but can be configured to tradeoff cost against retention. It is also possible to cap the rate at which data is ingested. For those data sources that can direct their metrics and logs only to Log Analytics and not to either a Storage Account or an Event Hub, it is possible to export data on a per table basis from Log Analytics to either a Storage Account or an Event Hub. It is possible to filter the data using Logic Apps, for example, as part of the export process. An Event Hub is fed the data in near real-time while a Storage Account is updated hourly. Processing of data \u2693\ufe0e This section covers the various tools that exist or can be created to process the data from metrics and logs for visualization and analytics. Insights \u2693\ufe0e These are tools to aggregate, correlate and display the data gathered for resources such as VMs, Storage, AKS, Key Vaults, Databases and Applications. They are either custom developed or workbook based. If the latter, the display can be customized. Typically, data from the metrics timeseries database, Log Analytics and activity logs are processed by the Insights. Dashboards \u2693\ufe0e These are tools that provide a single pane of glass across multiple resources and their metrics/logs. They are integrated into the Azure Portal and support auto refresh. They can be shared with specific users through Azure RBAC. Just like Insights, data from the metrics timeseries database, Log Analytics and activity logs can be aggregated by Dashboards. Workbooks \u2693\ufe0e These are like Dashboards except that their purpose is to analyze the data instead of displaying it. Unlike Dashboards, these do not support Azure RBAC. Prometheus and Grafana \u2693\ufe0e It is possible to load metric data into Prometheus from Log Analytics and create dashboards for the data using Grafana. Power BI \u2693\ufe0e This tool can be used to run powerful queries against the data in Log Analytics. Kusto queries \u2693\ufe0e It is possible to create and run custom queries in the Kusto Query Language against data in Log Analytics. Alerting \u2693\ufe0e Events recorded in Activity logs, metrics from the time series Metrics database and results of queries on the Log Analytics can all be processed by alert rules and an alert raised if certain conditions are met. Alerts can be viewed in a dashboard provided by the Azure Portal. Static alerts are those that are raised when a static threshold value is violated. Dynamic alerts are those for which artificial intelligence techniques are used to determine abnormal threshold values. Typically, alerts are processed by action rules which process alerts in the context of a scope (say a particular set of resources) and of a specified priority. If certain conditions are met, the action rule can trigger the execution of an action group which is simply a set of actions. These could be sending an SMS, sending an email, executing some automation, or raising an ITSM incident ticket. It is also possible to configure the action rule to suppress the action group for certain situations such as, say a planned maintenance window. Azure Traffic Analytics \u2693\ufe0e Traffic Analytics is a cloud-based solution that provides visibility into user and application activity in cloud networks. Traffic analytics analyzes Network Watcher network security group (NSG) flow logs to provide insights into traffic flow in a customer's Azure environment. Traffic analytics can: Visualize network activity across Azure subscriptions and identify hot spots. Identify security threats to, and secure the network, with information such as open-ports, applications attempting internet access, and virtual machines (VM) connecting to rogue networks. Understand traffic flow patterns across Azure regions and the internet to optimize network deployment for performance and capacity. Pinpoint network misconfigurations leading to failed connections in the network. The following diagram illustrates the components involved in the solution involving Traffic Analytics. Network Watcher is needed to turn on the generation of raw flow logs for an NSG and configuring a Storage Account to hold the raw flow logs. It also set up Traffic Analytics and associates it with a Logic Analytics workspace which holds the aggregated and indexed data that Traffic Analytics generates from the raw flow logs. Finally the Traffic Analytics solution provides various queries to help diagnose network issues. Security Management Architecture \u2693\ufe0e This section presents an architectural overview of the security solution in Azure. The intent is to give the readers a high-level understanding of the various components involved in the enablement of security at the various layers of Azure and how these components complement one another in providing a comprehensive solution. The diagram below illustrates the architecture. Log Analytics Workspace \u2693\ufe0e This is a centralized workspace that gathers all the various logs across the Azure environment for a customer. Along with the Azure AD logs, subscription activity logs, VM logs and possibly PaaS logs other Azure and non-Azure data sources can also be leveraged via data connectors. The proposed architecture shows the workspace as being shared between two other components \u2013 Azure Security Center (ASC) and Azure Sentinel. While it is possible to deploy a separate workspace for each of these components, a shared workspace is proposed to avoid duplicate data and the associated cost. Azure Policy \u2693\ufe0e This component provides bundles of policies, called initiatives, that specify the controls a customer\u2019s environment and workloads in Azure need to adhere to. These initiatives are communicated to another component, the Azure Security Center, that then monitors for violation of the associated policies. These initiatives often include policies in support of industry and regulatory compliances such as CIS Benchmarks, PCI DSS and HIPAA. Azure Security Center (ASC) \u2693\ufe0e ASC is a unified infrastructure security management system that \u2013 \u00b7 Proactively assesses a customer\u2019s environment and workloads against the enabled policies \u00b7 Identifies potential security risks and recommends a course of action to mitigate \u00b7 In certain cases, it provides remediation in the form of quick fixes \u00b7 Supports workflow automation through Logic Apps that can perform notification as well as remediation ASC obtains a large part of the data it needs to monitor the environment and workload via Azure backend telemetry. It only relies on the Log Analytics workspace for VM logs. Azure Defender \u2693\ufe0e This component extends the capability of ASC to protect customer workloads. Defender combined with ASC Base Edition gives the ASC Standard Edition. It provides the following advanced solutions \u2013 \u00b7 VM vulnerability assessment via the Qualys tool \u00b7 Just-in-Time VM access \u00b7 Adaptive Application Controls leverage Machine Learning to determine a list of safe software on a VM and enable Defender to detect suspicious software \u00b7 Container image scanning to detect vulnerabilities \u00b7 Adaptive network hardening uses Machine Learning to analyze network traffic patterns and determine further tightening of the NSG rules \u00b7 SQL vulnerability assessment analyses SQL Server configurations to detect vulnerabilities \u00b7 File integrity monitoring detects and reports suspicious changes to files on a VM \u00b7 Network map that graphically displays network topologies to inform on traffic flows and any security risk such as lack of NSGs for subnets Azure Sentinel \u2693\ufe0e This component provides Security Incident and Event Management (SIEM) and Security Orchestration Automated Response (SOAR) function for Azure environments. In addition to the logs from all Azure services and resources, the events and alerts it obtains from ASC via the shared Log Analyzer workspace, it also provides data connectors so many non-Azure components can also input data into the Log Analyzer workspace used by Sentinel. Sentinel basically correlates all the data it receives using Machine Intelligence to detect security threats and generates alerts in response. It also supports tools that can be used to recover from these security threats. Its notable features are \u2013 \u00b7 Many pre-built rules based on Kusto queries of the data in the Log Anaytics workspace to detect certain conditions and raise alerts \u00b7 Many pre-built playbooks to automate the performance of tasks such as blocking a user, blocking an IP address upon detection of certain conditions \u00b7 Many pre-built workbooks to perform various types of computation \u00b7 Hunting capability to find occurrences of certain conditions in the received data \u00b7 Many pre-built notebooks that enhance the Machine Learning capabilities \u00b7 Entity behaviour analysis to identify anomalous behaviour patterns indicative of a security attack \u00b7 Ability to enhance a database of threats with new ones Log Analytics Workspace \u2693\ufe0e While this component has been described briefly in different contexts, it is core to all of monitoring as the diagram below illustrates. The purpose of this section is to provide more details on this component to serve as a basis for laying out design considerations and recommendations. Deployment Models \u2693\ufe0e In general, three deployment models are possible for this component - Centralized : Single central workspace with differentiated access per team. Easy to manage, search across resources, and cross-correlate logs. Workspace size can grow significantly. Additional administrative overhead to maintain access control to different users. Cost of transporting data when the workspace spans multiple regions needs to be considered. Decentralized : Multiple workspaces across regions, subscriptions and resource groups. Access control to resources is possible but cross-correlation across multiple logs will be difficult. Hybrid : Complex, expensive, and hard-to-maintain configuration with gaps in logs coverage. Access Control for Resources \u2693\ufe0e Log Analytics workspaces now support a new resource-context log model wherein every log record emitted by an Azure resource is automatically associated with this resource. Such logs when forwarded to a central workspace enable scoping and Azure RBAC based on the resources. Depending on the context, a user may have either a workspace-context mode or a resource-context mode. For example, when examining metrics for all VMs, the workspace-context mode is assumed and when examining metrics for a particular VM, the resource-context mode is assumed. The Access control mode is a setting on each workspace that defines how permissions are determined for the workspace. Log Analytics workspaces can require workspace permissions only or use either workspace or resource permissions. The latter option enables fine grain access control through Azure RBAC the downside being the need to set the resource-level permissions for users. When a workspace has only workspace permissions there is no RBAC-based access control and the behavior is as follows a user with workspace-context mode can access all the tables in the workspace that it has been granted access to a user with resource-context mode can access only the data for those resources it has been granted access to When a workspace has both workspace and resource permissions, RBAC-based access control takes effect and the behavior is as follows a user with workspace-context mode can be given access based on the workspace permissions a user with resource-context mode can be given access based on the resource permissions For a more details on the deployment models and access control for Log Analytics workspace, please refer to the information at Designing your Azure Monitor Logs deployment . Update Management \u2693\ufe0e Azure provides the Update Management service for Windows VMs as well as Linux VMs for some popular distributions. Windows VMs are generally configured to use Microsoft Update while Linux VMs are generally configured to use the public repositories specific to the distributions. These are the sources for information on availability of patches. Both Windows and Linux VMs report their patch status as well as available patches via the Log Analytics agent to a Log Analytics workspace. The information can be used by Update Management to schedule select patches to groups of VMs at a certain time. An Automation Account is a service that supports automation of tasks, updates being one, through runbooks. Update Management schedules updates using runbooks in an Automation Account. Resource Locks \u2693\ufe0e A Resource Lock is not technically a policy, but the purpose is similar in the fact that it will prevent accidental modification and/or deletion, of subscription, resources groups or resources. There are two types of locks: o CanNotDelete means authorized users can still read and modify a resource, but they can\u2019t delete the resource. o ReadOnly means authorized users can read a resource, but they can\u2019t delete or update the resource. Applying this lock is like restricting all authorized users to the permissions granted by the Reader role. When you apply a lock at a parent scope, all resources within that scope inherit the same lock. Even resources you add later inherit the lock from the parent. The most restrictive lock in the inheritance takes precedence. Unlike role-based access control, you use management locks to apply a restriction across all users and roles. Applying Read-only can lead to unexpected results because some operations that seem like read operations require additional actions. For example, placing a ReadOnly lock on a storage account prevents all users from listing the keys. The list keys operation is handled through a POST request because the returned keys are available for write operations. For another example, placing a ReadOnly lock on an App Service resource prevents Visual Studio Server Explorer from displaying files for the resource because that interaction requires write access. Centralized Cloud Platform Management \u2693\ufe0e For all Kyndryl managed accounts, centralized monitoring and management platform is recommended. It requires the use of Azure native tools as well as Kyndryl\u2019s recommended ITOM (IT Operations Management) and ITSM (IT Service Management) tools. The high-level architecture of centralized monitoring and management solution is as below The solution components are described below \u2013 \u00b7 Monitoring Layer : It provides the centralized monitoring capabilities for the LZ, and the applications hosted in it. Azure Monitor, Log Analytics Workspace, Azure Application insights provide native capability to monitor the LZ infrastructure. Additional Enterprise monitoring tools like ScienceLogic, Datadog are recommended to monitor the components needed for the application. E.g if an Oracle or DB2 database running on a VM. The subsystems running on a VM cannot be monitored by Native monitoring tools. \u00b7 Event Aggregation Layer : The various monitoring and audit events generated at the monitoring layer needs to be aggregated at event management layer to avoid duplicate tickets and noise in the management system. This layer also triggers the event automations to auto remediate the incidents. \u00b7 Automation Layer : It provides automation capability for Day-1 provisioning tasks and Day-2 operational tasks. Discovery service will discover the managed resources and sync the CMDB. \u00b7 Service Management Layer : This layer provides the IT Service management capability for IPC (Incident, Change and Problem). This is the primary system to manage the cloud environment. The automations for service requests, change requests can be triggered from this layer. It also provides the self-service catalog for the end users to provision the cloud resources in the target LZ. \u00b7 Automation Framework : It is recommended to have a common automation framework like Ansible based CACF framework. Cloud native automations can also be used to automate repetitive tasks and the processes. NOTE - It is to be noted that at this time, it is unclear how the the three upper layers in the stack above - Service Management, Automation Layer and Event Aggregation Layer will be designed and implemented. A number of options are being examined and the integration of the Azure platform with these management layers will be clearer once the choice is made as to which option to go forward with. Centralized Monitoring & Management by MCMS \u2693\ufe0e MultiCloud Management Services (MCMS) offering provides monitoring and management services for Client\u2019s Azure LZ. It is a shared service offering. The tools and processes needed to manage client\u2019s Azure environment will be hosted on Kyndryl MCMS Tools Pod and Kyndryl MCMS MSP subscription . The reference architecture is as shown below. This Kyndryl MSP subscription will contain: a) Transit-Hub vNet This Transit-Hub vNet is connected to Kyndryl\u2019s management pod using IPSec Tunnel b) Transit-Client vNet For each client subscription there will be a Transit vNet Transit-Client vNet contains Bastion host for Kyndryl support teams to connect to target services In each client subscription Transit-Client vNet also contains Proxy &/or Relay servers to pass traffic from/to Azure native tools to Kyndryl tools in the management pod. Each Transit-Client vNet will connect to respective client\u2019s Hub vNet either through vNet peering or site-to-site VPN . vNet peering will be default whereas site-to-site VPN will be used for regulated accounts and if a client security requirement demands this. IP address range for Transit-client vNet can overlap with that of another Transit-client vNet. This is because Transit-client VNets do not communicate amongst themselves When Transit-client vNets communicate to Transit-Hub vNets, NAT is implemented at the Transit-Hub to differentiate each of the Transit-Client vNets IP address range for Transit-client vNets cannot overlap with client Hub vNet, and hence has to be discussed with respective clients to provide a x.x.x.x/26 IP address range Azure Lighthouse \u2693\ufe0e To provide centralized cloud management services for multiple clients, MCMS needs a cloud native service called Lighthouse. It enables cross- and multi-tenant management, allowing for higher automation, scalability, and enhanced governance across resources and tenants. With Azure Lighthouse, service providers can deliver managed services using comprehensive and robust management tooling built into the Azure platform. \u00b7 It provides built-in options for centralized access across organizations (tenants & subscriptions) \u00b7 Azure Lighthouse will be included as part of Kyndryl\u2019s centralized Cloud management services. \u00b7 Specific guidance on enabling this will be included in future releases. Centralized Monitoring & Management Environment \u2013 ISPwW & traditional SO \u2693\ufe0e \u00b7 For non-MCMS managed accounts, management PoD should be hosted in ISPwW or in traditional Strategic Outsourcing (SO) environments. \u00b7 Similar model used for MCMS can be applied on ISPwW & SO accounts too \u00b7 Guidance for this will be added in the next release. Solution Guidance \u2693\ufe0e This sub-section provides guidance at a high level on the key design considerations and best practices for designing a monitoring and management platform for Azure resources. Design Considerations \u2693\ufe0e Following are the main design considerations for monitoring and management - The appropriate deployment model for Log Analytics workspace needs to be chosen based on customer requirements Address the need for detection and notification of abnormal events at the IaaS level Address the need for detection and notification of abnormal events at the SQL Server that is running on either Managed Instances or Iaas VMs Address the need for detection and notification of abnormal events at the third-party middleware and database level Assess and identify security risks and provide for detection, response, and recovery from threats Potential integration with on-premises security information and event management (SIEM) systems such as ServiceNow, ArcSight, or the Onapsis security platform Monitor network flows for potential issues Support the timely deployment of patches to VMs to minimize security risks. Azure data retention thresholds and archiving requirements: The default retention period for Azure Monitor Logs is 30 days, with a maximum of two years The default retention period for Azure AD reports (premium) is 30 days The default retention period for the Azure diagnostic service is 90 days. Operational requirements - Resource locks to protect editing and deleting resource. Design Actions by Solution Architects. \u2693\ufe0e In alignment with recent Microsoft recommendations, define a single Log Analytics workspace per region to simplify administration, avoid additional cost from log replication across workspaces and enable correlation across a wide set of inputs. Where multiple regions are used, create one Log Analytics workspace in each region to avoid the cost from data transfer across regions Add integration with MCMS or Kyndryl SO delivery for Event correlation, Event Automation & ITSM integrations, which will cover the following: a. Configure alerts for standardized metrics and associated thresholds for IaaS resources using the Azure Monitor metrics database. Alerts are conveyed to MCMS for incident ticket creation and potential automated remediation b. Configure alerts for VM and SQL metrics for SQL Server using both the Azure Monitor metrics database and the Azure Monitor Log Analytics workspace. Alerts are conveyed to MCMS for incident creation and potential automated remediation c. Deploy third party monitoring solutions as needed, such as Science Logic for middleware and third-party databases and to also configure alerts. Alerts are conveyed to MCMS for incident creation and potential automated remediation Include Azure Security Center and Azure Sentinel for assessing compliance and identifying security risks and for detection, response and recovery from threats. Alerts raised by Sentinel are conveyed to MCMS/SO delivery tools for incident creation and potential automated remediation. Integration of Azure Sentinel with on-premises SIEM should be limited to communicating alerts Include Azure Policy for configuring access control and compliance reporting. Azure Policy provides the ability to enforce organization-wide settings to ensure consistent policy adherence and fast violation detection Use Network Watcher to proactively monitor traffic flows via Network Watcher NSG flow logs v2 . Traffic Analytics analyzes NSG flow logs to gather deep insights about IP traffic within a virtual network and provides critical information for effective management and monitoring. It can support queries that can then trigger alerts Use Update Management in Azure Automation as a long-term patching mechanism for both Windows and Linux VMs. Enforcing Update Management configurations via Azure Policy ensures that all VMs are included in the patch management regimen and provides application teams with the ability to manage patch deployment for their VMs. Integrate Azure Update Management into service flow implemented as part of MCMS tooling to support triggering of updates and their approvals. For Kyndrly accounts ensure the MCMS/SO delivery service flow is synchronized with Risk based Continuous Patch Tracking (RCP). RCP provides visibility and enforcement capabilities to the central compliance team. Export logs to Azure Storage if log retention requirements exceed two years. Use immutable storage with a write-once, read-many policy to make data non-erasable and non-modifiable for a user-specified interval. Operational requirements: Define resource locks to prevent accidental deletion of critical shared services. Additional Notes \u2693\ufe0e It is to be noted that MCMS integration with Azure is in progress. So all features and functions mentioned in the context of MCMS need to be validated with the MCMS Offering team.","title":"5.Monitoring & Mgmt"},{"location":"lzdesign/monitoringmgmt/#monitoring-management","text":"Centralized monitoring and management of all IT and Application infrastructure, along with associated middleware and security elements is important across on-premises, hybrid and multi-cloud environments. Microsoft\u2019s cloud adoption framework (CAF) prescribes monitoring across Infrastructure, Applications, Security, and all cloud services utilized by a client and service provider organization. These include Monitoring health, performance and security of IT resources Gathering and analyzing logs across all services and applications Gathering and analyzing metrics across all services and applications Extract meaningful information from the analysis, identify actionable incidents and remediate the same reactively and pro-actively This document starts with a description of Azure functions and components that support monitoring in addition to an overview of Azure monitoring architecture and Security Management Architecture. It then proceeds to discuss Kyndryl offerings that integrate Azure monitoring into Service Management solutions. It concludes with a discussion on design considerations and recommendations for monitoring.","title":"Monitoring &amp; Management"},{"location":"lzdesign/monitoringmgmt/#overview-of-azure-services-for-monitoring-management","text":"Azure provides a comprehensive set of tools and services to monitor across infrastructure, security, cloud services and application stacks. Both metrics data and logs are gathered across Identity and Access Management, Azure resources, Subscriptions, Operating systems, Applications that can be integrated to other tools either through Logic Apps or APIs Based on evaluation of the data against certain conditions, alerts can be triggered and also, autoscaling of resources such as Virtual Machine Scale Sets can be performed Tools for analyzing both metrics data and logs are available Visualization tools for presentation of either raw data or filtered data resulting from queries are available Tools to create insights from the analysis of data across Virtual Machines, Containers and Applications are available. For cloud-native Security Information and Event Management (SIEM), Azure Sentinel should be leveraged which helps in analyzing large volumes of data across an enterprise with built-in AI.","title":"Overview of Azure services for Monitoring &amp; Management"},{"location":"lzdesign/monitoringmgmt/#azure-monitoring-architecture","text":"This section presents an architectural overview of the monitoring solution in Azure. The intent is to give the readers a high-level understanding of the various components involved in Monitoring and Logging across the Azure platform and services and how these components complement one another in providing a comprehensive solution. The diagram below illustrates the architecture. The centerpiece of the Monitoring solution is the Azure Monitor. This consists of the Metrics component and the Logs component. Metrics are numerical values. The Metrics component gathers the metrics into a fast, lightweight time series database across all those components that generate them. Metrics can be viewed in less than 60 seconds post their occurrence. The intent is to be able to react in near real time to any issue based on an analysis of these metrics. The Logs component gathers both metrics and logs across all those components that generate them and can store them in different stores or targets. This is a slower process and intended more for analysis of data gathered over a period. Those components that do not rely on agents send the metrics and the logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub. For those that depend on agents, Log Analytics is the only possible destination. The following sections cover in detail, the sources of metrics and logs, the target destinations they can be directed to, the visualization and analytics that can be performed on the data and the raising of alerts based on the detection of certain conditions in the data.","title":"Azure Monitoring Architecture"},{"location":"lzdesign/monitoringmgmt/#sources-of-data","text":"This section briefly covers all those Azure resources that generate metrics and logs. It is to be emphasize that where metrics are generated, they are sent to both the fast time series database and the slow targets.","title":"Sources of data"},{"location":"lzdesign/monitoringmgmt/#azure-active-directory-aad","text":"AAD creates a variety of logs (audit, sign-in, provisioning). AAD can be configured to direct these logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub.","title":"Azure Active Directory (AAD)"},{"location":"lzdesign/monitoringmgmt/#subscription","text":"Each subscription generates an Activity log that encompasses actions related to the lifecycle of Azure resources. The Activity log also contains information generated by the Service Health tool that includes Resource alerts, Health issues/advisories/alerts, Planned Maintenance and Security advisories. A subscription can be configured to direct the activity log to one or more of destinations such as Log Analytics, Storage Account or Event Hub.","title":"Subscription"},{"location":"lzdesign/monitoringmgmt/#arm-resources","text":"Each resource generates metrics as numerical values that are unique to itself. Each resource also generates logs as both numerical and string values that are unique to itself. Note that the metrics and logs here relate to the ARM resources as known to the virtualization layer hosting them. A resource can be configured to direct its metrics and logs to one or more of destinations such as Log Analytics, Storage Account or Event Hub.","title":"ARM Resources"},{"location":"lzdesign/monitoringmgmt/#guest-os","text":"For resources such as VMs that run Guest Operating Systems, additional metrics and logs are generated through agents running on those resources. These resources need not be just ARM resources. They could be VMs running for example on either customer premises or in other clouds. The metrics and logs from these resources can only be directed to Log Analytics.","title":"Guest OS"},{"location":"lzdesign/monitoringmgmt/#applications","text":"Applications can be monitored either through codeless attach procedures (J2EE/.NET runtime monitoring is an example) or through compile-time instrumentation. The metrics and logs from these resources can only be directed to Log Analytics. The resulting metrics and logs can be analyzed by the App Insights tool to diagnose issues.","title":"Applications"},{"location":"lzdesign/monitoringmgmt/#miscellaneous-sources","text":"There are certain solutions such as Hadoop and Kubernetes clusters that generate, store, and process their metrics and logs differently. It is also possible to generate metrics and logs for custom resources through the ARM REST API. The metrics and logs from these resources can only be directed to Log Analytics.","title":"Miscellaneous sources"},{"location":"lzdesign/monitoringmgmt/#destinations-for-data","text":"This section briefly describes the three main possible target destinations for the metrics and logs generated by various Azure resources. Again, metrics are always sent to the time series database in addition to the targets that logs are sent to.","title":"Destinations for data"},{"location":"lzdesign/monitoringmgmt/#storage-account","text":"One potential destination for the generated metrics and logs is a storage account. The benefit of using a storage account is the low cost that enables long term storage or archival.","title":"Storage Account"},{"location":"lzdesign/monitoringmgmt/#event-hub","text":"A second potential destination for the generated metrics and logs is an Event Hub. The benefit of this is the ability to send the data to multiple third-party solutions that can provide more sophisticated analysis of the data.","title":"Event Hub"},{"location":"lzdesign/monitoringmgmt/#log-analytics","text":"This is a workspace that is part of the Azure Monitor Logs tool. A workspace is a combination of storage space in the form of tables and processing capabilities that allow queries to be run against the data to extract insights. In general, there is a cost to both ingestion of the data as well as the storage. By default, the data is retained for 2 years but can be configured to tradeoff cost against retention. It is also possible to cap the rate at which data is ingested. For those data sources that can direct their metrics and logs only to Log Analytics and not to either a Storage Account or an Event Hub, it is possible to export data on a per table basis from Log Analytics to either a Storage Account or an Event Hub. It is possible to filter the data using Logic Apps, for example, as part of the export process. An Event Hub is fed the data in near real-time while a Storage Account is updated hourly.","title":"Log Analytics"},{"location":"lzdesign/monitoringmgmt/#processing-of-data","text":"This section covers the various tools that exist or can be created to process the data from metrics and logs for visualization and analytics.","title":"Processing of data"},{"location":"lzdesign/monitoringmgmt/#insights","text":"These are tools to aggregate, correlate and display the data gathered for resources such as VMs, Storage, AKS, Key Vaults, Databases and Applications. They are either custom developed or workbook based. If the latter, the display can be customized. Typically, data from the metrics timeseries database, Log Analytics and activity logs are processed by the Insights.","title":"Insights"},{"location":"lzdesign/monitoringmgmt/#dashboards","text":"These are tools that provide a single pane of glass across multiple resources and their metrics/logs. They are integrated into the Azure Portal and support auto refresh. They can be shared with specific users through Azure RBAC. Just like Insights, data from the metrics timeseries database, Log Analytics and activity logs can be aggregated by Dashboards.","title":"Dashboards"},{"location":"lzdesign/monitoringmgmt/#workbooks","text":"These are like Dashboards except that their purpose is to analyze the data instead of displaying it. Unlike Dashboards, these do not support Azure RBAC.","title":"Workbooks"},{"location":"lzdesign/monitoringmgmt/#prometheus-and-grafana","text":"It is possible to load metric data into Prometheus from Log Analytics and create dashboards for the data using Grafana.","title":"Prometheus and Grafana"},{"location":"lzdesign/monitoringmgmt/#power-bi","text":"This tool can be used to run powerful queries against the data in Log Analytics.","title":"Power BI"},{"location":"lzdesign/monitoringmgmt/#kusto-queries","text":"It is possible to create and run custom queries in the Kusto Query Language against data in Log Analytics.","title":"Kusto queries"},{"location":"lzdesign/monitoringmgmt/#alerting","text":"Events recorded in Activity logs, metrics from the time series Metrics database and results of queries on the Log Analytics can all be processed by alert rules and an alert raised if certain conditions are met. Alerts can be viewed in a dashboard provided by the Azure Portal. Static alerts are those that are raised when a static threshold value is violated. Dynamic alerts are those for which artificial intelligence techniques are used to determine abnormal threshold values. Typically, alerts are processed by action rules which process alerts in the context of a scope (say a particular set of resources) and of a specified priority. If certain conditions are met, the action rule can trigger the execution of an action group which is simply a set of actions. These could be sending an SMS, sending an email, executing some automation, or raising an ITSM incident ticket. It is also possible to configure the action rule to suppress the action group for certain situations such as, say a planned maintenance window.","title":"Alerting"},{"location":"lzdesign/monitoringmgmt/#azure-traffic-analytics","text":"Traffic Analytics is a cloud-based solution that provides visibility into user and application activity in cloud networks. Traffic analytics analyzes Network Watcher network security group (NSG) flow logs to provide insights into traffic flow in a customer's Azure environment. Traffic analytics can: Visualize network activity across Azure subscriptions and identify hot spots. Identify security threats to, and secure the network, with information such as open-ports, applications attempting internet access, and virtual machines (VM) connecting to rogue networks. Understand traffic flow patterns across Azure regions and the internet to optimize network deployment for performance and capacity. Pinpoint network misconfigurations leading to failed connections in the network. The following diagram illustrates the components involved in the solution involving Traffic Analytics. Network Watcher is needed to turn on the generation of raw flow logs for an NSG and configuring a Storage Account to hold the raw flow logs. It also set up Traffic Analytics and associates it with a Logic Analytics workspace which holds the aggregated and indexed data that Traffic Analytics generates from the raw flow logs. Finally the Traffic Analytics solution provides various queries to help diagnose network issues.","title":"Azure Traffic Analytics"},{"location":"lzdesign/monitoringmgmt/#security-management-architecture","text":"This section presents an architectural overview of the security solution in Azure. The intent is to give the readers a high-level understanding of the various components involved in the enablement of security at the various layers of Azure and how these components complement one another in providing a comprehensive solution. The diagram below illustrates the architecture.","title":"Security Management Architecture"},{"location":"lzdesign/monitoringmgmt/#log-analytics-workspace","text":"This is a centralized workspace that gathers all the various logs across the Azure environment for a customer. Along with the Azure AD logs, subscription activity logs, VM logs and possibly PaaS logs other Azure and non-Azure data sources can also be leveraged via data connectors. The proposed architecture shows the workspace as being shared between two other components \u2013 Azure Security Center (ASC) and Azure Sentinel. While it is possible to deploy a separate workspace for each of these components, a shared workspace is proposed to avoid duplicate data and the associated cost.","title":"Log Analytics Workspace"},{"location":"lzdesign/monitoringmgmt/#azure-policy","text":"This component provides bundles of policies, called initiatives, that specify the controls a customer\u2019s environment and workloads in Azure need to adhere to. These initiatives are communicated to another component, the Azure Security Center, that then monitors for violation of the associated policies. These initiatives often include policies in support of industry and regulatory compliances such as CIS Benchmarks, PCI DSS and HIPAA.","title":"Azure Policy"},{"location":"lzdesign/monitoringmgmt/#azure-security-center-asc","text":"ASC is a unified infrastructure security management system that \u2013 \u00b7 Proactively assesses a customer\u2019s environment and workloads against the enabled policies \u00b7 Identifies potential security risks and recommends a course of action to mitigate \u00b7 In certain cases, it provides remediation in the form of quick fixes \u00b7 Supports workflow automation through Logic Apps that can perform notification as well as remediation ASC obtains a large part of the data it needs to monitor the environment and workload via Azure backend telemetry. It only relies on the Log Analytics workspace for VM logs.","title":"Azure Security Center (ASC)"},{"location":"lzdesign/monitoringmgmt/#azure-defender","text":"This component extends the capability of ASC to protect customer workloads. Defender combined with ASC Base Edition gives the ASC Standard Edition. It provides the following advanced solutions \u2013 \u00b7 VM vulnerability assessment via the Qualys tool \u00b7 Just-in-Time VM access \u00b7 Adaptive Application Controls leverage Machine Learning to determine a list of safe software on a VM and enable Defender to detect suspicious software \u00b7 Container image scanning to detect vulnerabilities \u00b7 Adaptive network hardening uses Machine Learning to analyze network traffic patterns and determine further tightening of the NSG rules \u00b7 SQL vulnerability assessment analyses SQL Server configurations to detect vulnerabilities \u00b7 File integrity monitoring detects and reports suspicious changes to files on a VM \u00b7 Network map that graphically displays network topologies to inform on traffic flows and any security risk such as lack of NSGs for subnets","title":"Azure Defender"},{"location":"lzdesign/monitoringmgmt/#azure-sentinel","text":"This component provides Security Incident and Event Management (SIEM) and Security Orchestration Automated Response (SOAR) function for Azure environments. In addition to the logs from all Azure services and resources, the events and alerts it obtains from ASC via the shared Log Analyzer workspace, it also provides data connectors so many non-Azure components can also input data into the Log Analyzer workspace used by Sentinel. Sentinel basically correlates all the data it receives using Machine Intelligence to detect security threats and generates alerts in response. It also supports tools that can be used to recover from these security threats. Its notable features are \u2013 \u00b7 Many pre-built rules based on Kusto queries of the data in the Log Anaytics workspace to detect certain conditions and raise alerts \u00b7 Many pre-built playbooks to automate the performance of tasks such as blocking a user, blocking an IP address upon detection of certain conditions \u00b7 Many pre-built workbooks to perform various types of computation \u00b7 Hunting capability to find occurrences of certain conditions in the received data \u00b7 Many pre-built notebooks that enhance the Machine Learning capabilities \u00b7 Entity behaviour analysis to identify anomalous behaviour patterns indicative of a security attack \u00b7 Ability to enhance a database of threats with new ones","title":"Azure Sentinel"},{"location":"lzdesign/monitoringmgmt/#log-analytics-workspace_1","text":"While this component has been described briefly in different contexts, it is core to all of monitoring as the diagram below illustrates. The purpose of this section is to provide more details on this component to serve as a basis for laying out design considerations and recommendations.","title":"Log Analytics Workspace"},{"location":"lzdesign/monitoringmgmt/#deployment-models","text":"In general, three deployment models are possible for this component - Centralized : Single central workspace with differentiated access per team. Easy to manage, search across resources, and cross-correlate logs. Workspace size can grow significantly. Additional administrative overhead to maintain access control to different users. Cost of transporting data when the workspace spans multiple regions needs to be considered. Decentralized : Multiple workspaces across regions, subscriptions and resource groups. Access control to resources is possible but cross-correlation across multiple logs will be difficult. Hybrid : Complex, expensive, and hard-to-maintain configuration with gaps in logs coverage.","title":"Deployment Models"},{"location":"lzdesign/monitoringmgmt/#access-control-for-resources","text":"Log Analytics workspaces now support a new resource-context log model wherein every log record emitted by an Azure resource is automatically associated with this resource. Such logs when forwarded to a central workspace enable scoping and Azure RBAC based on the resources. Depending on the context, a user may have either a workspace-context mode or a resource-context mode. For example, when examining metrics for all VMs, the workspace-context mode is assumed and when examining metrics for a particular VM, the resource-context mode is assumed. The Access control mode is a setting on each workspace that defines how permissions are determined for the workspace. Log Analytics workspaces can require workspace permissions only or use either workspace or resource permissions. The latter option enables fine grain access control through Azure RBAC the downside being the need to set the resource-level permissions for users. When a workspace has only workspace permissions there is no RBAC-based access control and the behavior is as follows a user with workspace-context mode can access all the tables in the workspace that it has been granted access to a user with resource-context mode can access only the data for those resources it has been granted access to When a workspace has both workspace and resource permissions, RBAC-based access control takes effect and the behavior is as follows a user with workspace-context mode can be given access based on the workspace permissions a user with resource-context mode can be given access based on the resource permissions For a more details on the deployment models and access control for Log Analytics workspace, please refer to the information at Designing your Azure Monitor Logs deployment .","title":"Access Control for Resources"},{"location":"lzdesign/monitoringmgmt/#update-management","text":"Azure provides the Update Management service for Windows VMs as well as Linux VMs for some popular distributions. Windows VMs are generally configured to use Microsoft Update while Linux VMs are generally configured to use the public repositories specific to the distributions. These are the sources for information on availability of patches. Both Windows and Linux VMs report their patch status as well as available patches via the Log Analytics agent to a Log Analytics workspace. The information can be used by Update Management to schedule select patches to groups of VMs at a certain time. An Automation Account is a service that supports automation of tasks, updates being one, through runbooks. Update Management schedules updates using runbooks in an Automation Account.","title":"Update Management"},{"location":"lzdesign/monitoringmgmt/#resource-locks","text":"A Resource Lock is not technically a policy, but the purpose is similar in the fact that it will prevent accidental modification and/or deletion, of subscription, resources groups or resources. There are two types of locks: o CanNotDelete means authorized users can still read and modify a resource, but they can\u2019t delete the resource. o ReadOnly means authorized users can read a resource, but they can\u2019t delete or update the resource. Applying this lock is like restricting all authorized users to the permissions granted by the Reader role. When you apply a lock at a parent scope, all resources within that scope inherit the same lock. Even resources you add later inherit the lock from the parent. The most restrictive lock in the inheritance takes precedence. Unlike role-based access control, you use management locks to apply a restriction across all users and roles. Applying Read-only can lead to unexpected results because some operations that seem like read operations require additional actions. For example, placing a ReadOnly lock on a storage account prevents all users from listing the keys. The list keys operation is handled through a POST request because the returned keys are available for write operations. For another example, placing a ReadOnly lock on an App Service resource prevents Visual Studio Server Explorer from displaying files for the resource because that interaction requires write access.","title":"Resource Locks"},{"location":"lzdesign/monitoringmgmt/#centralized-cloud-platform-management","text":"For all Kyndryl managed accounts, centralized monitoring and management platform is recommended. It requires the use of Azure native tools as well as Kyndryl\u2019s recommended ITOM (IT Operations Management) and ITSM (IT Service Management) tools. The high-level architecture of centralized monitoring and management solution is as below The solution components are described below \u2013 \u00b7 Monitoring Layer : It provides the centralized monitoring capabilities for the LZ, and the applications hosted in it. Azure Monitor, Log Analytics Workspace, Azure Application insights provide native capability to monitor the LZ infrastructure. Additional Enterprise monitoring tools like ScienceLogic, Datadog are recommended to monitor the components needed for the application. E.g if an Oracle or DB2 database running on a VM. The subsystems running on a VM cannot be monitored by Native monitoring tools. \u00b7 Event Aggregation Layer : The various monitoring and audit events generated at the monitoring layer needs to be aggregated at event management layer to avoid duplicate tickets and noise in the management system. This layer also triggers the event automations to auto remediate the incidents. \u00b7 Automation Layer : It provides automation capability for Day-1 provisioning tasks and Day-2 operational tasks. Discovery service will discover the managed resources and sync the CMDB. \u00b7 Service Management Layer : This layer provides the IT Service management capability for IPC (Incident, Change and Problem). This is the primary system to manage the cloud environment. The automations for service requests, change requests can be triggered from this layer. It also provides the self-service catalog for the end users to provision the cloud resources in the target LZ. \u00b7 Automation Framework : It is recommended to have a common automation framework like Ansible based CACF framework. Cloud native automations can also be used to automate repetitive tasks and the processes. NOTE - It is to be noted that at this time, it is unclear how the the three upper layers in the stack above - Service Management, Automation Layer and Event Aggregation Layer will be designed and implemented. A number of options are being examined and the integration of the Azure platform with these management layers will be clearer once the choice is made as to which option to go forward with.","title":"Centralized Cloud Platform Management"},{"location":"lzdesign/monitoringmgmt/#centralized-monitoring-management-by-mcms","text":"MultiCloud Management Services (MCMS) offering provides monitoring and management services for Client\u2019s Azure LZ. It is a shared service offering. The tools and processes needed to manage client\u2019s Azure environment will be hosted on Kyndryl MCMS Tools Pod and Kyndryl MCMS MSP subscription . The reference architecture is as shown below. This Kyndryl MSP subscription will contain: a) Transit-Hub vNet This Transit-Hub vNet is connected to Kyndryl\u2019s management pod using IPSec Tunnel b) Transit-Client vNet For each client subscription there will be a Transit vNet Transit-Client vNet contains Bastion host for Kyndryl support teams to connect to target services In each client subscription Transit-Client vNet also contains Proxy &/or Relay servers to pass traffic from/to Azure native tools to Kyndryl tools in the management pod. Each Transit-Client vNet will connect to respective client\u2019s Hub vNet either through vNet peering or site-to-site VPN . vNet peering will be default whereas site-to-site VPN will be used for regulated accounts and if a client security requirement demands this. IP address range for Transit-client vNet can overlap with that of another Transit-client vNet. This is because Transit-client VNets do not communicate amongst themselves When Transit-client vNets communicate to Transit-Hub vNets, NAT is implemented at the Transit-Hub to differentiate each of the Transit-Client vNets IP address range for Transit-client vNets cannot overlap with client Hub vNet, and hence has to be discussed with respective clients to provide a x.x.x.x/26 IP address range","title":"Centralized Monitoring &amp; Management by MCMS"},{"location":"lzdesign/monitoringmgmt/#azure-lighthouse","text":"To provide centralized cloud management services for multiple clients, MCMS needs a cloud native service called Lighthouse. It enables cross- and multi-tenant management, allowing for higher automation, scalability, and enhanced governance across resources and tenants. With Azure Lighthouse, service providers can deliver managed services using comprehensive and robust management tooling built into the Azure platform. \u00b7 It provides built-in options for centralized access across organizations (tenants & subscriptions) \u00b7 Azure Lighthouse will be included as part of Kyndryl\u2019s centralized Cloud management services. \u00b7 Specific guidance on enabling this will be included in future releases.","title":"Azure Lighthouse"},{"location":"lzdesign/monitoringmgmt/#centralized-monitoring-management-environment-ispww-traditional-so","text":"\u00b7 For non-MCMS managed accounts, management PoD should be hosted in ISPwW or in traditional Strategic Outsourcing (SO) environments. \u00b7 Similar model used for MCMS can be applied on ISPwW & SO accounts too \u00b7 Guidance for this will be added in the next release.","title":"Centralized Monitoring &amp; Management Environment \u2013 ISPwW &amp; traditional SO"},{"location":"lzdesign/monitoringmgmt/#solution-guidance","text":"This sub-section provides guidance at a high level on the key design considerations and best practices for designing a monitoring and management platform for Azure resources.","title":"Solution Guidance"},{"location":"lzdesign/monitoringmgmt/#design-considerations","text":"Following are the main design considerations for monitoring and management - The appropriate deployment model for Log Analytics workspace needs to be chosen based on customer requirements Address the need for detection and notification of abnormal events at the IaaS level Address the need for detection and notification of abnormal events at the SQL Server that is running on either Managed Instances or Iaas VMs Address the need for detection and notification of abnormal events at the third-party middleware and database level Assess and identify security risks and provide for detection, response, and recovery from threats Potential integration with on-premises security information and event management (SIEM) systems such as ServiceNow, ArcSight, or the Onapsis security platform Monitor network flows for potential issues Support the timely deployment of patches to VMs to minimize security risks. Azure data retention thresholds and archiving requirements: The default retention period for Azure Monitor Logs is 30 days, with a maximum of two years The default retention period for Azure AD reports (premium) is 30 days The default retention period for the Azure diagnostic service is 90 days. Operational requirements - Resource locks to protect editing and deleting resource.","title":"Design Considerations"},{"location":"lzdesign/monitoringmgmt/#design-actions-by-solution-architects","text":"In alignment with recent Microsoft recommendations, define a single Log Analytics workspace per region to simplify administration, avoid additional cost from log replication across workspaces and enable correlation across a wide set of inputs. Where multiple regions are used, create one Log Analytics workspace in each region to avoid the cost from data transfer across regions Add integration with MCMS or Kyndryl SO delivery for Event correlation, Event Automation & ITSM integrations, which will cover the following: a. Configure alerts for standardized metrics and associated thresholds for IaaS resources using the Azure Monitor metrics database. Alerts are conveyed to MCMS for incident ticket creation and potential automated remediation b. Configure alerts for VM and SQL metrics for SQL Server using both the Azure Monitor metrics database and the Azure Monitor Log Analytics workspace. Alerts are conveyed to MCMS for incident creation and potential automated remediation c. Deploy third party monitoring solutions as needed, such as Science Logic for middleware and third-party databases and to also configure alerts. Alerts are conveyed to MCMS for incident creation and potential automated remediation Include Azure Security Center and Azure Sentinel for assessing compliance and identifying security risks and for detection, response and recovery from threats. Alerts raised by Sentinel are conveyed to MCMS/SO delivery tools for incident creation and potential automated remediation. Integration of Azure Sentinel with on-premises SIEM should be limited to communicating alerts Include Azure Policy for configuring access control and compliance reporting. Azure Policy provides the ability to enforce organization-wide settings to ensure consistent policy adherence and fast violation detection Use Network Watcher to proactively monitor traffic flows via Network Watcher NSG flow logs v2 . Traffic Analytics analyzes NSG flow logs to gather deep insights about IP traffic within a virtual network and provides critical information for effective management and monitoring. It can support queries that can then trigger alerts Use Update Management in Azure Automation as a long-term patching mechanism for both Windows and Linux VMs. Enforcing Update Management configurations via Azure Policy ensures that all VMs are included in the patch management regimen and provides application teams with the ability to manage patch deployment for their VMs. Integrate Azure Update Management into service flow implemented as part of MCMS tooling to support triggering of updates and their approvals. For Kyndrly accounts ensure the MCMS/SO delivery service flow is synchronized with Risk based Continuous Patch Tracking (RCP). RCP provides visibility and enforcement capabilities to the central compliance team. Export logs to Azure Storage if log retention requirements exceed two years. Use immutable storage with a write-once, read-many policy to make data non-erasable and non-modifiable for a user-specified interval. Operational requirements: Define resource locks to prevent accidental deletion of critical shared services.","title":"Design Actions by Solution Architects."},{"location":"lzdesign/monitoringmgmt/#additional-notes","text":"It is to be noted that MCMS integration with Azure is in progress. So all features and functions mentioned in the context of MCMS need to be validated with the MCMS Offering team.","title":"Additional Notes"},{"location":"lzdesign/nwtopology/","text":"Network Topology and Connectivity \u2693\ufe0e This section covers: - An overview of the various network components and functions available in Microsoft Azure. - A brief discussion of two commonly used network topologies. - Solution guidance, based on key design considerations and best practices surrounding networking and connectivity to, from, and within Microsoft Azure. Network Components and Functions in Microsoft Azure -- An Overview \u2693\ufe0e Solution Architects are expected to be familiar with the topics listed at the following link as a prerequisite for the rest of the section -Microsoft Azure Networking Fundamentals Azure Virtual Network (VNet) \u2693\ufe0e The VNet is the fundamental building block in Azure cloud that enables communications for various resources such as Virtual Machines (VMs) and Containers. A VNet exists within a subscription and within an Azure region, that is, it cannot span subscriptions or regions. It provides a boundary and isolation for the resources hosted within it. A VNet consists of one or more IP ranges, typically from the private IP ranges specified in RFC 1918. IPv6 address ranges can also be assigned. Azure implements VNets using Software Defined Network (SDN) technologies. This precludes the use of broadcast, multicast and different types of tunnelling. More details on VNets can be obtained from Virtual Network documentation . Subnets and IP addressing \u2693\ufe0e Each IP address range within a VNet can be used to create subnets. In each subnet that is created, the first four and the last IP addresses are reserved. The first IP address is the network address, the second is used for the default gateway for that subnet, the third and fourth are used for Domain Name Services (DNS) and the last is the broadcast address. A subnet spans the availability zones (AZs) within a region so VMs in different AZs can be on the same subnet. Azure manages the IP address pool for each subnet. Typically, IP addresses are assigned to network interfaces via DHCP. Every time a VM that a network interface is attached to is deallocated and revived, the IP address can change. It is possible to assign a static IP address to a network interface. It is also possible to associate a public IP address with a network interface. This does not mean the public IP address is configured on the network interface. Instead, Azure performs Network Address Translation (NAT) to deliver the traffic destined for the public IP address to the private IP address configured on the network interface. It is possible to assign IPv6 address ranges to subnets but IPv4 is mandatory, given Azure management traffic uses IPv4. Just as for IPv4, public IPv6 addresses can be assigned to network interfaces. More details on subnets can be obtained from the Subnet documentation . More details on IP addresses can be obtained from the navigation links on https://docs.microsoft.com/en-us/azure/virtual-network/public-ip-addresses . Internet Connectivity \u2693\ufe0e Outgoing traffic to the Internet uses public IP addresses that Azure implements. Network and Port Address Translation (NAT/PAT) is used to ensure that corresponding incoming traffic is permitted. As mentioned before, if a public IP address is assigned to a network interface, 1:1 NAT is used instead. Incoming traffic from the Internet can leverage a public IP address assigned to a network interface. However, the recommended approach is to use public IP addresses assigned to Load Balancers. When Load Balancers are used, the outgoing traffic will use the public IP addresses assigned to the Load Balancers. Load Balancers are covered in greater detail in other sections of this document. More details on Internet Connectivity using NAT can be obtained from NAT documentation . NAT Gateway \u2693\ufe0e A NAT Gateway resource specifies several public IP addresses (individual addresses, a pool through a prefix or a combination of the two). It is associated with a VNet. For each subnet the NAT Gateway resource to be used for outbound Internet traffic can then be specified. More details on NAT Gateway resources can be obtained from NAT gateway documentation . Connectivity between VNets \u2693\ufe0e The best possible approach for traffic between VNets is to leverage the Azure backbone via peering. VNets in multiple regions, subscriptions and Azure AD tenants can be peered. Network traffic between peered virtual networks is private. No public Internet, gateways, or encryption is required in the communication between the virtual networks. Resources in either virtual network can directly connect with resources in the peered virtual network. The network latency between virtual machines in peered virtual networks in the same region is the same as the latency within a single virtual network. The network throughput is based on the bandwidth that\\'s allowed for the virtual machine, proportionate to its size. There isn\\'t any additional restriction on bandwidth within the peering. More details on VNet peering can be obtained from Virtual Network peering documentation . Azure DNS \u2693\ufe0e This is a hosting service for DNS domains that provides name resolution by using Azure infrastructure. The service can be operated through the Azure Portal, Azure PowerShell cmdlets and the cross-platform Azure CLI. The service is offered via a special IP address (168.63.129.16) that is not routable outside Azure. Azure Private DNS \u2693\ufe0e By default, when a VM is created, it is given a unique name in the zone internal.cloudapp.net and auto-registered with Azure DNS. This may not acceptable to most clients, and clients would prefer something like hostname.contoso.com or hostname.azure.contoso.com . This is achieved using Private DNS & Private DNS Zones . Azure Private DNS provides a reliable, secure DNS service to manage and resolve domain names in a virtual network without the need to add a custom DNS solution. By using private DNS zones, you can use your own custom domain names(hostname.azure.contoso.com) rather than the Azure-provided names available today(internal.cloudapp.net) After creating a Private DNS zone, vNET should be linked to the Private DNS zone. When creating a link between a private DNS zone and a virtual network, you have the option to enable autoregistration. With this setting enabled, the virtual network becomes a registration virtual network for the private DNS zone. A DNS record gets automatically created for any virtual machines you deploy in the virtual network. Note that the auto-registration to the default domain of internal.cloudapp.net always happens in addition to that in the Private DNS zone if any. If the VM is deleted, both auto-registrations are deleted. For the custom auto-registration, when the hostname is changed, the associated DNS record is updated. But for the default auto-registration, the record does not change. One private DNS zone can have multiple resolution virtual networks and a virtual network can have multiple resolution zones associated to it. Azure DNS supports A, AAAA, CNAME, MX, PTR, SOA, SRV, and TXT records. Note that only one custom zone can be linked with a VNet. However, multiple VNets can link to the same custom zone. Name resolution across multiple zones can be performed from within a VNet. It is possible to implement a custom DNS server for the custom zones. If a VM pointing to such a custom DNS server needs to resolve a name in one of the Azure internal zones (such as internal.cloudapp.net), the custom DNS server can be made to forward to Azure DNS server (that is, forward to 168.63.129.16). However, if the custom DNS server is hosted outsize Azure (say the customer premises), it will need to forward to a DNS resolver within the VNet that will then forward to Azure DNS. Azure Public DNS \u2693\ufe0e For Public DNS Azure DNS provides the name server. User is expected to register its domain and use IP addresses provided by Azure as the name servers. All this can be performed from the Azure Portal, Azure Powershell cmdlets and the cross-platform Azure CLI. Once the domain registration process is completed, A records can be added to the Azure DNS. Note that in the case of split-brain DNS where the same zone is defined in both public and private spaces, resolution is performed using the private space first. More details on Azure DNS can be obtained from Azure DNS documentation . Network Security Groups (NSGs) \u2693\ufe0e NSGs are groups of rules that enable filtering of traffic to and from Azure resources within a VNet. The resources include VMs, Azure Kubernetes Services, Azure Container Services, Azure Functions, API Management, Web Apps and so on. Each rule has a name, priority, IP address/IP address range/service tag/application security group, protocol, direction, port range and action (Allow or Deny). The rules are evaluated by priority (lower numbers have higher priority) using the 5-tuple information (source and destination addresses, source and destination ports, protocol) to either allow or deny the traffic. A flow record is established for each connection allowing stateful behaviour. Default rules exist in each security group that allow all traffic within the VNet, all outbound traffic to the Internet, all inbound traffic coming via an Azure Load Balancer, and deny all other incoming and outgoing traffic to and from the VNet. While these rules cannot be deleted, they can be overridden by new rules of higher priority. NSGs are associated with VNet subnets and the network interfaces of VMs. For incoming traffic, the NSGs at the subnet level are evaluated first followed by those at the network interface level. For outgoing traffic, the reverse is true. It is important to know that if an NSG that denies all incoming and outgoing traffic is associated with a subnet, all communications between VMs in that subnet are denied. Typical recommendation is to associate NSGs with either a subnet or the network interface of a VM, but not both. More details on NSGs can be obtained from NSG documentation . Application Security Groups (ASGs) \u2693\ufe0e ASGs are groups of VMs that can either be the source or destination in an NSG rule. The group is created from the network interfaces on a set of VMs that belong in the same VNet. More details on ASGs can be obtained from ASG documentation . Azure Load Balancer \u2693\ufe0e Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. The Load Balancer distributes inbound flows that arrive at the Load Balancer\\'s front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines or instances in a virtual machine scale set. Session affinity can be achieved by configuring the load-balancing rules appropriately. Public Load Balancers are used to load balance internet traffic to VMs. A Private or Internal Load Balancer is used to load balance traffic within a VNet or from an on-premises network in a hybrid scenario. The Azure Load Balancer is available as either a Basic or a Standard Load Balancer. The former is limited in features and capacity. A VM in the backend sees incoming traffic with the source IP address of the originator. The response from the VM is routed via the Load Balancer so the IP address of the VM that is used as the source for the response can be replaced by the IP address for the front end of the Load Balancer. This traffic is distinguished from the outgoing connections a VM may initiate. This point is important in the case of a Public Load Balancer. By default, these outgoing connection requests to the Internet are SNATed by NAT gateways. However, if a standard Public Load Balancer is used, there is an option to use the public IP address of the Load Balancer to SNAT the outgoing connections to the Internet from the backend VMs. It is important to note that the Azure Load Balancer is implemented using Software Defined Networking techniques and a distributed set of network elements. Azure Standard Load Balancer helps load-balance all protocol flows on all ports simultaneously when used an Internal Load Balancer via High Availability Ports. High availability (HA) ports are a type of load balancing rule that provides an easy way to load-balance all flows that arrive on all ports of an Internal Standard Load Balancer. The HA ports load-balancing rules help with critical scenarios, such as high availability and scale for network virtual appliances (NVAs) inside virtual networks. The feature can also help when many ports must be load-balanced. The Azure Standard Load Balancer supports multiple IP addresses at the front-end. This allows the use of one instance of the Standard Load Balancer to support multiple workloads that may be reusing the same ports. In a region with Availability Zones, a Standard Load Balancer can be zone redundant. The frontend IP may be used to reach all (non-impacted) backend pool members no matter the zone. One or more availability zones can fail but the data path survives if at least one zone in the region remains healthy. The frontend\\'s IP address is served simultaneously by multiple independent infrastructure deployments in multiple availability zones. Any retries or reestablishment will succeed in other zones not affected by the zone failure. More details on Azure Load Balancer can be obtained from Azure Load Balancer documentation . Azure Firewall \u2693\ufe0e Azure Firewall is a managed, cloud-based network security service that protects Azure Virtual Network resources. It supports firewall rules at L3, L4 and L7 layers of the OSI stack. It\\'s a fully stateful firewall as a service with built-in high availability and unrestricted cloud scalability. Azure Firewall can be configured during deployment to span multiple Availability Zones for increased availability. Threat intelligence-based filtering can be enabled on the firewall to alert and deny traffic from/to known malicious IP addresses and domains. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed. SNAT for outgoing and DNAT for incoming traffic along with filtering is supported. Multiple public IP addresses are supported. All Internet traffic can be routed via a network virtual appliance or on-premises edge firewall (forced tunneling) before going to the Internet. The service is fully integrated with Azure Monitor for logging and analytics. More details on Azure Firewall can be obtained from Azure Firewall documentation . Azure Bastion Service \u2693\ufe0e Azure Bastion is a service that allows a user to connect to a VM using a browser and the Azure Portal. This service is a fully platform-managed PaaS service that is provisioned inside a VNet and in its own subnet that is named AzureBastionSubnet. It provides secure and seamless RDP/SSH connectivity to all the VMs in the VNet directly from the Azure portal over TLS. The VMs do not need a public IP address, agent, or special client software to support connectivity via the Azure portal and the Azure Bastion service. The Azure Bastion service is hardened internally and kept up to date to provide secure RDP/SSH connectivity and protect against zero-day exploits. NSGs can be configured to permit remote connections (RDP/SSH) only via the Azure Bastion service. This way the VMs are protected from port scanning by rogue and malicious users outside the VNet. More details on Azure Bastion service can be obtained from Azure Bastion service documentation . Azure DDoS Protection \u2693\ufe0e This function is available as two SKUs. Every resource in Azure is protected with no additional cost by Azure\\'s infrastructure DDoS (Basic) Protection through always-on traffic monitoring and real-time mitigation. DDoS Protection Basic requires no user configuration or application changes. DDoS Protection Basic helps protect all Azure services, including PaaS services like Azure DNS. Azure DDoS Protection Standard , combined with application design best practices, provides enhanced DDoS mitigation features to defend against DDoS attacks. It is automatically tuned to help protect specific Azure resources in a virtual network. Protection is simple to enable on any new or existing virtual network, and it requires no application or resource changes. It has several advantages over the basic service, including logging, alerting, and telemetry. More details on Azure DDoS Protection can be obtained from Azure DDoS Protection Standard documentation . Azure Network Watcher \u2693\ufe0e Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is designed to monitor and repair the network health of IaaS (Infrastructure-as-a-Service) products such as Virtual Machines, Virtual Networks, Application Gateways, and Load Balancers. It is not intended for and will not work for PaaS monitoring or Web analytics. When a VNet is created or updated, Network Watcher will be enabled automatically in the VNet's region. If needed, it can also be enabled in any Azure region manually. The capabilities provided by Network Watcher are described briefly below. The Connection Monitor checks the communication between a VM and an endpoint and reports regularly on reachability, latency, and changes in network topology between the VM and the endpoint. Network Performance Monitor helps monitor network performance between various points in the network infrastructure. It detects network issues like traffic blackholing, routing errors, and issues that conventional network monitoring methods can't detect. The Topology capability generates a visual diagram of the resources in a virtual network, and the relationships between the resources. The IP Flow Verify capability discovers and reports on security rules that may be inhibiting traffic between two IP endpoints. The Next Hop capability reports on the next hop used to route traffic from one IP address to another thus enabling users to fix routing issues. The Connection Troubleshoot capability is like Connection Monitor except that it runs when explicitly invoked as against periodically. The Capture capability can capture packets to and from a VM. Filtering options and controls such as time and size can be configured. Captured data can be stored in Azure Storage and analyzed using a variety of tools. The VPN Diagnostics capability diagnoses the health of the gateway and its connections and provides detailed information on the cause for any issues. The Security Group View capability shows the security rules applicable to a network interface and is a combination of the rules applicable at both the subnet the network interface is in and the network interface itself. The Network Subscription Limit capability reports on the limits and the actual utilization of network resources for a subscription and region. The NSG Flow Log capability enables logging of the source and destination IP address, port, protocol, and whether traffic was allowed or denied by an NSG. The Diagnostic Logs capability provides a single interface to enable and disable network resource diagnostic logs for any existing network resource that generates a diagnostic log. These logs can be viewed using tools such as Microsoft Power BI and Azure Monitor logs. Network Watcher capability reports on latencies between Azure regions and across Internet service providers. More details on Azure Network Watcher can be obtained from Azure Network Watcher documentation . Azure Application Gateway \u2693\ufe0e Azure Application Gateway is a web traffic load balancer for web applications. It makes routing decisions based on additional attributes of an HTTP request, for example URI path or host headers. It is therefore called an application layer or OSI layer 7 load balancer. More details on the Azure Application Gateway can be obtained from Azure Application Gateway documentation . Azure Service Endpoints \u2693\ufe0e Azure PaaS services are accessible using pubic IP addresses. This feature allows access to Azure PaaS Services (for example, Azure Storage and SQL Database) from a VNet without having to use the path commonly used by traffic directed to the Internet. The Azure PaaS service retains its public IP address. A route is established over the Azure backbone between the VNet and the Azure PaaS service. Firewall rules can be created for the Azure PaaS service allowing dedicated access to this VNet. However, connectivity to Azure IaaS services from customer premises using service endpoints is not possible. A service endpoint can be configured as part of a subnet configuration. More details can be obtained from Virtual Network service endpoints . Azure Private Link \u2693\ufe0e Azure Private Link (Private Endpoint) allows you to access Azure PaaS services over Private IP address within the VNet. It gets a new private IP on your VNet. When you send traffic to PaaS resource, it will always ensure traffic stays within your VNet. This feature enables access to Azure PaaS Services (for example, Azure Storage and SQL Database) and /or customer-owned/partner services hosted in Azure over a private IP address within the VNet. The traffic between the service and the consumer of the service does not go over the Internet. Configuration of the service consists of two steps. First, a private link service needs to be created that enables private connections to an existing service. Next, a private endpoint needs to be created that enables access to the existing service via the private link service. Azure Private Link has integration with Azure Monitor. More details can be obtained from Azure Private Link documentation . Dedicated Azure services in VNets (VNet Integration) Injection \u2693\ufe0e This feature enables deployment of Azure PaaS services (for example Azure SQL) and Azure hosted customer-owned/partner services within a subnet of VNet. The service is then accessible within the VNet, across VNets via VNet peering as well as from customer premises. A PaaS service can be injected into a VNet as part of the instantiation of the service. More details on VNet integration injection can be obtained from Integrate Azure services . VPN Gateway \u2693\ufe0e A VPN gateway is a type of virtual network gateway that is used to send encrypted traffic between a VNet and an on-premises location over the public Internet. Each VNet can have only one VPN gateway. However, multiple connections can be created to the same VPN gateway with all connections sharing the available gateway bandwidth. The encryption over public Internet limits the performance in terms of bandwidth and latency. A VPN gateway is implemented as a pair of VMs in a dedicated subnet called the gateway subnet. After a VPN gateway is created, an IPsec/IKE VPN tunnel connection between that VPN gateway and another VPN gateway (VNet-to-VNet), or a cross-premises IPsec/IKE VPN tunnel connection between the VPN gateway and an on-premises VPN device (Site-to-Site) can be provisioned. It is also possible to create a Point-to-Site VPN connection (VPN over OpenVPN, IKEv2, or SSTP), which allows connection to the virtual network from a remote location, such as from home. In general, the VPN Gateway is meant to be used for Development, Test and Laboratory scenarios Small-scale production workloads More details on VPN gateways can be obtained from VPN Gateway documentation . ExpressRoute Gateway \u2693\ufe0e An ExpressRoute gateway is a type of virtual network gateway that is used to send traffic between a VNet and an on-premises location over a private link. Each VNet can have only one ExpressRoute gateway. However, multiple ExpressRoute circuits can be created to the same ExpressRoute gateway with all circuits sharing the same available gateway bandwidth. Similarly, multiple VNets can be linked to an ExpressRoute circuit. The private link can either be a point to point physical link such as a fiberoptic cable carrying Ethernet or an any to any fabric such as Multi-Protocol Label Switching (MPLS). This link is established in a Meet-Me location between Microsoft routers and Customer routers, the latter connecting to the Customer on-premises network. The Microsoft routers connect to the Azure backbone. The Microsoft routers, the Customer routers any network fabric between them provided by a Service Provider all constitute an ExpressRoute circuit. The ExpressRoute gateway is implemented as a pair of VMs, each supporting route advertisement and data traffic routing for the ExpressRoute circuit. When an ExpressRoute gateway is configured, it establishes an IP path over the backbone, the Microsoft routers and Customer routers to the on-premises network. ExpressRoute circuits provide two peering paths between on-premises and Microsoft Azure. Microsoft peering so Azure cloud services are accessible from on-premises. Private peering so VNets are accessible from on-premises. By default, peering through a location in a particular region allows on-premises access to the Azure cloud services across all the regions in that geopolitical region. ExpressRoute Premium can be enabled to extend connectivity across geopolitical boundaries. ExpressRoute Global Reach enables different on-premises sites to connect to different peering locations and use the Azure backbone to transfer data between sites. Connections over ExpressRoute circuits offer more reliability, faster speeds, and lower latencies. They are ideal for Data backup and replication Hybrid production workloads spanning on-premise and Azure cloud More details on ExpressRoute gateways can be obtained from Azure ExpressRoute documentation . Virtual WAN \u2693\ufe0e Virtual WAN is a network service that combines the functions of routing, firewall, encryption, VPN gateway and ExpressRoute gateway. Unlike other solutions for hybrid connectivity, the Virtual WAN network service is managed by Microsoft. In this document, it is covered from the Virtual WAN network topology point view and not as a standalone element. It provides services such as Branch connectivity (via connectivity automation from Virtual WAN Partner devices such as SD-WAN or VPN CPE) Site-to-site VPN connectivity Remote user VPN (Point-to-site) connectivity Private (ExpressRoute) connectivity Intra-cloud connectivity (transitive connectivity for virtual networks) Routing between two branches one connected via VPN the other via ExpressRoute More details on Virtual WAN can be obtained from Azure Virtual WAN documentation . User Defined Routes (UDR) \u2693\ufe0e Azure creates default routes for each subnet in a VNet that allows routing of traffic within the subnets of that VNet as well as to the Internet. In addition, when VNet peering is enabled, routes for all the subnets in the remote VNet are added for each subnet within the local VNet. Also, all the routes advertised by on-premises routers to a virtual network gateway in a VNet are setup for all the subnets in all the VNets that are peered with the VNet hosting the virtual network gateway. It is also possible for the user to set up custom routes for a subnet. Take the case where a Network Virtual Appliance (NVA) that is responsible for routing is installed in a VNet. A custom route for a subnet in a VNet peered with the VNet hosting the NVA can be created by a user. Another situation is where routing between different subnet pairs needs to be setup within a VNet. More details on UDR can be obtained from Virtual Network traffic routing documentation . Azure Firewall Manager \u2693\ufe0e Azure Firewall Manager is a security management service that provides central security policy and route management for cloud-based security perimeters. Firewall Manager can provide security management for two network architecture types: Secure virtual hub - An Azure Virtual WAN hub with associated security and routing policies. Supports Azure Firewall and third-party Security as a Service (SECaaS) providers. Hub virtual network -- A standard virtual network with associated security policies. Supports Azure Firewall only. More details on Azure Firewall Manager can be obtained from Azure Firewall Manager documentation . Azure Networking Use Cases \u2693\ufe0e Microsoft Azure supports many networking scenarios. The below table summarized the use cases and associated Azure services. Network Topologies \u2693\ufe0e The information presented thus far pertains to the network components and functions in Azure. This sub-section covers the network topologies commonly used to connect virtual networks in Azure and networks on-premises for any-to-any traffic flows. Hub-Spoke Network Topology using Virtual Network Gateways \u2693\ufe0e This architecture is an extension of the VPN and ExpressRoute Gateways proposed in the preceding sections. It combines the VNet-VNet connectivity with the use of virtual gateways to support the connectivity between customer premises and one or more VNets in Azure hosting customer workloads. One VNet acts as the hub that the customer premises connect to and workload VNets are peered with. The benefits of this architecture are the ability to share network services across multiple workload environments. separation of the centralized management of the network services from the management of the workload environments By default, each workload VNet can communicate with the customer premises through the Hub VNet but not with other workload VNets. It is however possible to configure the Hub and workload VNets to forward traffic between the workload VNets. While either the VPN gateway or the ExpressRoute gateway can be used, the bandwidth available will be limited by the capabilities of these gateways. A better alternative is to use the Azure Firewall or other Network Virtual Appliance in the Hub VNet that can function as a router between the workload VNets. The virtual gateway is used solely for traffic between the Hub VNet and the customer premises. Note that while the diagram shows VPN and ExpressRoute connections to a virtual network gateway, a VPN gateway supports only S-2-S and P-2-S VPN connections, and an ExpressRoute gateway supports only ExpressRoute connections. The main use cases for this network topology are - Workloads deployed in different environments, such as development, testing, and production, that require shared services such as DNS, IDS, NTP, or AD DS. Shared services are placed in the hub virtual network, while each environment is deployed to a spoke to maintain isolation. Enterprises that require central control over security aspects, such as a firewall in the hub as a DMZ, and segregated management for the workloads in each spoke. The VPN Gateway can be enabled for data transfers between parts of a workload distributed across either VNets or across on-premises and VNets. Note that such use is not recommended for production environments. The ExpressRoute Gateway can be enabled for data transfers between parts of a workload distributed across either VNets or across on-premises and VNets. It can be used for Production workloads as long as the number of VNets and on-premise sites is limited. More details on Hub-Spoke Network Topology can be obtained from Hub-spoke network topology in Azure documentation . Hub-Spoke Network Topology with Azure Virtual WAN \u2693\ufe0e This architecture is like the previously discussed Hub-Spoke architecture except that in this case the Hub VNet (called Virtual Hub) uses network components described in the previous section on Azure Virtual WAN. As seen in the above figure, each region can deploy a Virtual Hub. The Virtual Hubs in multiple regions are connected in a full mesh over the Azure Backbone. Thus, workload VNets in Region 2 are reachable from both the customer premises connected to Region 1 and the workload VNets in Region 1. Microsoft manages the Virtual Hub infrastructure. This means that the lifecycle of the Virtual Hub Router, VPN Gateway, ExpressRoute Gateway, Azure Firewall and any other network elements incorporated into the Azure Virtual WAN are all managed by Microsoft. The sizing of the network elements incorporated into the Azure Virtual WAN enables higher bandwidth for data traffic. Use of the Azure Virtual WAN entails manual configuration. Microsoft partners provide their own Customer Premise Equipment (CPE) NVA that support automatic configuration and optimization in concert with their SD-WAN solutions and that can be installed in the Virtual Hub. The main use cases for this network topology are -- Connectivity among workloads distributed across VNets and on-premise sites with central control and access to shared services. An enterprise requires central control over security aspects, such as a firewall, and requires segregated management for the workloads in each spoke. More details on Hub-Spoke Network Topology with Azure Virtual WAN can be obtained from Hub-spoke network topology with Azure Virtual WAN documentation . Design Actions by Solution Architects \u2693\ufe0e This sub-section provides guidance at a high level on the key design considerations and best practices for creating a network architecture using the above described network components, functions, and topologies. 1.Plan for IP Addressing \u2693\ufe0e It is necessary to determine the IP address ranges to be assigned to on-premise networks, if any, and the VNets in Azure. One key design consideration is to ensure none of the assigned address ranges overlap. Another consideration is to ensure optimal use of the address ranges, particularly in the case of IPv4 addressing. A detailed discussion of the design considerations and recommendations can be obtained from Plan for IP Addressing documentation . 2. Design DNS for on-premise and Azure resources \u2693\ufe0e DNS can be configured to either leverage on-premise DNS infrastructure or to leverage Azure cloud. The earlier section on DNS provides a brief description of DNS support in Azure and links to more detailed information. One design consideration is that the maximum number of private DNS zones to which a virtual network can link with auto-registration is one. Another design consideration is that the maximum number of private DNS zones to which a virtual network can link is 1,000 all without auto-registration enabled. A detailed discussion of the design considerations and recommendations can be obtained from DNS for on-premises and Azure resources documentation . 3.Private Link and DNS Integration \u2693\ufe0e The Private Link feature as well as private DNS have been discussed in previous sub-sections. Private DNS allows a VNet to be associated with a custom zone. However, when the customer wishes to associate a private DNS A record with such customer defined PaaS service, there could be challenges in terms of accessibility given the implementation approach of the private DNS. Details of how such custom PaaS services can be provided A records in the private DNS can be obtained from Private Link and DNS integration at scale documentation . 4.Connectivity to Azure \u2693\ufe0e In terms of data traffic, ExpressRoute is the preferred approach to connecting on-premises to Azure. ExpressRoute has several characteristics such as peering for Microsoft cloud SaaS/PaaS services and Global Reach that need to be considered along with limitations such as needing a peering location. Redundancy can be achieved through dual circuits across different peering locations and use of BGP. A detailed discussion of the design considerations and recommendations can be obtained from Connectivity to Azure documentation . 5.Define an Azure network topology \u2693\ufe0e Network topology is a critical element of the solution design because it defines how applications can communicate with each other. Two network topologies have been presented in the previous section. The Hub-Spoke network topology with Virtual Network gateways is to be mainly used where there are only a few on-premise sites to connect to Azure. The Hub-Spoke network topology with Virtual WAN is to be mainly used where there are many regional and branch locations to be connected to Azure. Other decision criteria can be obtained from Define an Azure network topology documentation . 6.Network Design with Virtual WAN \u2693\ufe0e Virtual WAN is a managed service with unique features such as any-to-any connectivity across VNets, regional sites and branch sites, routing between VPN connected and ExpressRoute connected sites, Firewall, and encryption. But there are certain limitations such as inability to support NVAs and UDRs. The detailed design considerations and recommendations for Virtual WAN topology can be obtained from Virtual WAN network topology (Microsoft-managed) documentation . 7.Network Design with Virtual Network gateways \u2693\ufe0e Virtual Network gateways have certain features and limitations that need to be considered during network design. While a hup-spoke topology with Virtual Network gateways provides flexibility in terms of NVAs and UDR, the limitations in terms of bandwidth and connections need to be considered as well. The detailed design considerations and recommendations for Virtual Network gateways can be obtained from Traditional Azure networking topology documentation . 8.Connectivity to Azure PaaS services \u2693\ufe0e Often, there is a requirement to avoid access to Azure PaaS and customer defined PaaS services through public IP addresses. The detailed design considerations and recommendations for use of techniques such as VNet injection and Private Link can be obtained from Connectivity to Azure PaaS services documentation . 9.Plan for Inbound and Outbound Connectivity \u2693\ufe0e Internet connectivity, Azure Firewall and Application Gateway were discussed briefly in preceding sub-sections. Azure Firewall and Application Gateway are fully managed services so there are no operational or management costs. NVAs can be used instead if special functionality that these Azure services do not provide are needed. The detailed design considerations and recommendations for the use of these components to comprehensively secure Internet traffic can be obtained from Plan for inbound and outbound internet connectivity documentation . 10.Plan for Application Delivery \u2693\ufe0e The Azure Load Balancer and Application Gateway were discussed briefly in previous sub-sections. Azure Application Gateway allows the secure delivery of HTTP/S applications at a regional level while Azure Front Door allows the secure delivery of highly available HTTP/S applications across Azure regions. The detailed design considerations and recommendations for the use of these components to deliver internal and external facing applications in a secure, highly scalable and highly available manner can be obtained from Plan for application delivery documentation . 11.Plan for Landing Zone Network Segmentation \u2693\ufe0e NSGs were discussed briefly in a previous sub-section. NSGs help protect traffic across subnets, as well as east/west traffic across the platform (traffic between landing zones) while application security groups at the subnet-level NSGs help protect multitier VMs within the landing zone. The detailed design considerations and recommendations for the use of NSGs and other controls to drive a network zero-trust implementation can be obtained from Plan for landing zone network segmentation documentation . 12.Plan for Network Encryption \u2693\ufe0e While two network topologies have been briefly discussed in earlier sub-sections, encryption of traffic between on-premises and Azure has not been covered. While a VPN gateway provides encryption of data passing through the IPSEC tunnel, an ExpressRoute gateway does not provide any encryption by default. The design considerations and recommendations for providing encryption with ExpressRoute gateways and the Virtual WAN gateways can be obtained from Plan for Network Encryption documentation . 13.Plan for Traffic Inspection \u2693\ufe0e In many industries, organizations require that traffic in Azure is mirrored to a network packet collector for deep inspection and analysis. This requirement typically focuses on inbound and outbound internet traffic. Network Watcher can capture packets although for a limited window of 5 hours. Third party tools are recommended for deep packet inspection. The detailed design considerations and recommendations for mirroring or tapping traffic within Azure Virtual Network can be obtained from Plan for Traffic Inspection documentation .","title":"4.Network"},{"location":"lzdesign/nwtopology/#network-topology-and-connectivity","text":"This section covers: - An overview of the various network components and functions available in Microsoft Azure. - A brief discussion of two commonly used network topologies. - Solution guidance, based on key design considerations and best practices surrounding networking and connectivity to, from, and within Microsoft Azure.","title":"Network Topology and Connectivity"},{"location":"lzdesign/nwtopology/#network-components-and-functions-in-microsoft-azure-an-overview","text":"Solution Architects are expected to be familiar with the topics listed at the following link as a prerequisite for the rest of the section -Microsoft Azure Networking Fundamentals","title":"Network Components and Functions in Microsoft Azure -- An Overview"},{"location":"lzdesign/nwtopology/#azure-virtual-network-vnet","text":"The VNet is the fundamental building block in Azure cloud that enables communications for various resources such as Virtual Machines (VMs) and Containers. A VNet exists within a subscription and within an Azure region, that is, it cannot span subscriptions or regions. It provides a boundary and isolation for the resources hosted within it. A VNet consists of one or more IP ranges, typically from the private IP ranges specified in RFC 1918. IPv6 address ranges can also be assigned. Azure implements VNets using Software Defined Network (SDN) technologies. This precludes the use of broadcast, multicast and different types of tunnelling. More details on VNets can be obtained from Virtual Network documentation .","title":"Azure Virtual Network (VNet)"},{"location":"lzdesign/nwtopology/#subnets-and-ip-addressing","text":"Each IP address range within a VNet can be used to create subnets. In each subnet that is created, the first four and the last IP addresses are reserved. The first IP address is the network address, the second is used for the default gateway for that subnet, the third and fourth are used for Domain Name Services (DNS) and the last is the broadcast address. A subnet spans the availability zones (AZs) within a region so VMs in different AZs can be on the same subnet. Azure manages the IP address pool for each subnet. Typically, IP addresses are assigned to network interfaces via DHCP. Every time a VM that a network interface is attached to is deallocated and revived, the IP address can change. It is possible to assign a static IP address to a network interface. It is also possible to associate a public IP address with a network interface. This does not mean the public IP address is configured on the network interface. Instead, Azure performs Network Address Translation (NAT) to deliver the traffic destined for the public IP address to the private IP address configured on the network interface. It is possible to assign IPv6 address ranges to subnets but IPv4 is mandatory, given Azure management traffic uses IPv4. Just as for IPv4, public IPv6 addresses can be assigned to network interfaces. More details on subnets can be obtained from the Subnet documentation . More details on IP addresses can be obtained from the navigation links on https://docs.microsoft.com/en-us/azure/virtual-network/public-ip-addresses .","title":"Subnets and IP addressing"},{"location":"lzdesign/nwtopology/#internet-connectivity","text":"Outgoing traffic to the Internet uses public IP addresses that Azure implements. Network and Port Address Translation (NAT/PAT) is used to ensure that corresponding incoming traffic is permitted. As mentioned before, if a public IP address is assigned to a network interface, 1:1 NAT is used instead. Incoming traffic from the Internet can leverage a public IP address assigned to a network interface. However, the recommended approach is to use public IP addresses assigned to Load Balancers. When Load Balancers are used, the outgoing traffic will use the public IP addresses assigned to the Load Balancers. Load Balancers are covered in greater detail in other sections of this document. More details on Internet Connectivity using NAT can be obtained from NAT documentation .","title":"Internet Connectivity"},{"location":"lzdesign/nwtopology/#nat-gateway","text":"A NAT Gateway resource specifies several public IP addresses (individual addresses, a pool through a prefix or a combination of the two). It is associated with a VNet. For each subnet the NAT Gateway resource to be used for outbound Internet traffic can then be specified. More details on NAT Gateway resources can be obtained from NAT gateway documentation .","title":"NAT Gateway"},{"location":"lzdesign/nwtopology/#connectivity-between-vnets","text":"The best possible approach for traffic between VNets is to leverage the Azure backbone via peering. VNets in multiple regions, subscriptions and Azure AD tenants can be peered. Network traffic between peered virtual networks is private. No public Internet, gateways, or encryption is required in the communication between the virtual networks. Resources in either virtual network can directly connect with resources in the peered virtual network. The network latency between virtual machines in peered virtual networks in the same region is the same as the latency within a single virtual network. The network throughput is based on the bandwidth that\\'s allowed for the virtual machine, proportionate to its size. There isn\\'t any additional restriction on bandwidth within the peering. More details on VNet peering can be obtained from Virtual Network peering documentation .","title":"Connectivity between VNets"},{"location":"lzdesign/nwtopology/#azure-dns","text":"This is a hosting service for DNS domains that provides name resolution by using Azure infrastructure. The service can be operated through the Azure Portal, Azure PowerShell cmdlets and the cross-platform Azure CLI. The service is offered via a special IP address (168.63.129.16) that is not routable outside Azure.","title":"Azure DNS"},{"location":"lzdesign/nwtopology/#azure-private-dns","text":"By default, when a VM is created, it is given a unique name in the zone internal.cloudapp.net and auto-registered with Azure DNS. This may not acceptable to most clients, and clients would prefer something like hostname.contoso.com or hostname.azure.contoso.com . This is achieved using Private DNS & Private DNS Zones . Azure Private DNS provides a reliable, secure DNS service to manage and resolve domain names in a virtual network without the need to add a custom DNS solution. By using private DNS zones, you can use your own custom domain names(hostname.azure.contoso.com) rather than the Azure-provided names available today(internal.cloudapp.net) After creating a Private DNS zone, vNET should be linked to the Private DNS zone. When creating a link between a private DNS zone and a virtual network, you have the option to enable autoregistration. With this setting enabled, the virtual network becomes a registration virtual network for the private DNS zone. A DNS record gets automatically created for any virtual machines you deploy in the virtual network. Note that the auto-registration to the default domain of internal.cloudapp.net always happens in addition to that in the Private DNS zone if any. If the VM is deleted, both auto-registrations are deleted. For the custom auto-registration, when the hostname is changed, the associated DNS record is updated. But for the default auto-registration, the record does not change. One private DNS zone can have multiple resolution virtual networks and a virtual network can have multiple resolution zones associated to it. Azure DNS supports A, AAAA, CNAME, MX, PTR, SOA, SRV, and TXT records. Note that only one custom zone can be linked with a VNet. However, multiple VNets can link to the same custom zone. Name resolution across multiple zones can be performed from within a VNet. It is possible to implement a custom DNS server for the custom zones. If a VM pointing to such a custom DNS server needs to resolve a name in one of the Azure internal zones (such as internal.cloudapp.net), the custom DNS server can be made to forward to Azure DNS server (that is, forward to 168.63.129.16). However, if the custom DNS server is hosted outsize Azure (say the customer premises), it will need to forward to a DNS resolver within the VNet that will then forward to Azure DNS.","title":"Azure Private DNS"},{"location":"lzdesign/nwtopology/#azure-public-dns","text":"For Public DNS Azure DNS provides the name server. User is expected to register its domain and use IP addresses provided by Azure as the name servers. All this can be performed from the Azure Portal, Azure Powershell cmdlets and the cross-platform Azure CLI. Once the domain registration process is completed, A records can be added to the Azure DNS. Note that in the case of split-brain DNS where the same zone is defined in both public and private spaces, resolution is performed using the private space first. More details on Azure DNS can be obtained from Azure DNS documentation .","title":"Azure Public DNS"},{"location":"lzdesign/nwtopology/#network-security-groups-nsgs","text":"NSGs are groups of rules that enable filtering of traffic to and from Azure resources within a VNet. The resources include VMs, Azure Kubernetes Services, Azure Container Services, Azure Functions, API Management, Web Apps and so on. Each rule has a name, priority, IP address/IP address range/service tag/application security group, protocol, direction, port range and action (Allow or Deny). The rules are evaluated by priority (lower numbers have higher priority) using the 5-tuple information (source and destination addresses, source and destination ports, protocol) to either allow or deny the traffic. A flow record is established for each connection allowing stateful behaviour. Default rules exist in each security group that allow all traffic within the VNet, all outbound traffic to the Internet, all inbound traffic coming via an Azure Load Balancer, and deny all other incoming and outgoing traffic to and from the VNet. While these rules cannot be deleted, they can be overridden by new rules of higher priority. NSGs are associated with VNet subnets and the network interfaces of VMs. For incoming traffic, the NSGs at the subnet level are evaluated first followed by those at the network interface level. For outgoing traffic, the reverse is true. It is important to know that if an NSG that denies all incoming and outgoing traffic is associated with a subnet, all communications between VMs in that subnet are denied. Typical recommendation is to associate NSGs with either a subnet or the network interface of a VM, but not both. More details on NSGs can be obtained from NSG documentation .","title":"Network Security Groups (NSGs)"},{"location":"lzdesign/nwtopology/#application-security-groups-asgs","text":"ASGs are groups of VMs that can either be the source or destination in an NSG rule. The group is created from the network interfaces on a set of VMs that belong in the same VNet. More details on ASGs can be obtained from ASG documentation .","title":"Application Security Groups (ASGs)"},{"location":"lzdesign/nwtopology/#azure-load-balancer","text":"Azure Load Balancer operates at layer 4 of the Open Systems Interconnection (OSI) model. The Load Balancer distributes inbound flows that arrive at the Load Balancer\\'s front end to backend pool instances. These flows are according to configured load-balancing rules and health probes. The backend pool instances can be Azure Virtual Machines or instances in a virtual machine scale set. Session affinity can be achieved by configuring the load-balancing rules appropriately. Public Load Balancers are used to load balance internet traffic to VMs. A Private or Internal Load Balancer is used to load balance traffic within a VNet or from an on-premises network in a hybrid scenario. The Azure Load Balancer is available as either a Basic or a Standard Load Balancer. The former is limited in features and capacity. A VM in the backend sees incoming traffic with the source IP address of the originator. The response from the VM is routed via the Load Balancer so the IP address of the VM that is used as the source for the response can be replaced by the IP address for the front end of the Load Balancer. This traffic is distinguished from the outgoing connections a VM may initiate. This point is important in the case of a Public Load Balancer. By default, these outgoing connection requests to the Internet are SNATed by NAT gateways. However, if a standard Public Load Balancer is used, there is an option to use the public IP address of the Load Balancer to SNAT the outgoing connections to the Internet from the backend VMs. It is important to note that the Azure Load Balancer is implemented using Software Defined Networking techniques and a distributed set of network elements. Azure Standard Load Balancer helps load-balance all protocol flows on all ports simultaneously when used an Internal Load Balancer via High Availability Ports. High availability (HA) ports are a type of load balancing rule that provides an easy way to load-balance all flows that arrive on all ports of an Internal Standard Load Balancer. The HA ports load-balancing rules help with critical scenarios, such as high availability and scale for network virtual appliances (NVAs) inside virtual networks. The feature can also help when many ports must be load-balanced. The Azure Standard Load Balancer supports multiple IP addresses at the front-end. This allows the use of one instance of the Standard Load Balancer to support multiple workloads that may be reusing the same ports. In a region with Availability Zones, a Standard Load Balancer can be zone redundant. The frontend IP may be used to reach all (non-impacted) backend pool members no matter the zone. One or more availability zones can fail but the data path survives if at least one zone in the region remains healthy. The frontend\\'s IP address is served simultaneously by multiple independent infrastructure deployments in multiple availability zones. Any retries or reestablishment will succeed in other zones not affected by the zone failure. More details on Azure Load Balancer can be obtained from Azure Load Balancer documentation .","title":"Azure Load Balancer"},{"location":"lzdesign/nwtopology/#azure-firewall","text":"Azure Firewall is a managed, cloud-based network security service that protects Azure Virtual Network resources. It supports firewall rules at L3, L4 and L7 layers of the OSI stack. It\\'s a fully stateful firewall as a service with built-in high availability and unrestricted cloud scalability. Azure Firewall can be configured during deployment to span multiple Availability Zones for increased availability. Threat intelligence-based filtering can be enabled on the firewall to alert and deny traffic from/to known malicious IP addresses and domains. The IP addresses and domains are sourced from the Microsoft Threat Intelligence feed. SNAT for outgoing and DNAT for incoming traffic along with filtering is supported. Multiple public IP addresses are supported. All Internet traffic can be routed via a network virtual appliance or on-premises edge firewall (forced tunneling) before going to the Internet. The service is fully integrated with Azure Monitor for logging and analytics. More details on Azure Firewall can be obtained from Azure Firewall documentation .","title":"Azure Firewall"},{"location":"lzdesign/nwtopology/#azure-bastion-service","text":"Azure Bastion is a service that allows a user to connect to a VM using a browser and the Azure Portal. This service is a fully platform-managed PaaS service that is provisioned inside a VNet and in its own subnet that is named AzureBastionSubnet. It provides secure and seamless RDP/SSH connectivity to all the VMs in the VNet directly from the Azure portal over TLS. The VMs do not need a public IP address, agent, or special client software to support connectivity via the Azure portal and the Azure Bastion service. The Azure Bastion service is hardened internally and kept up to date to provide secure RDP/SSH connectivity and protect against zero-day exploits. NSGs can be configured to permit remote connections (RDP/SSH) only via the Azure Bastion service. This way the VMs are protected from port scanning by rogue and malicious users outside the VNet. More details on Azure Bastion service can be obtained from Azure Bastion service documentation .","title":"Azure Bastion Service"},{"location":"lzdesign/nwtopology/#azure-ddos-protection","text":"This function is available as two SKUs. Every resource in Azure is protected with no additional cost by Azure\\'s infrastructure DDoS (Basic) Protection through always-on traffic monitoring and real-time mitigation. DDoS Protection Basic requires no user configuration or application changes. DDoS Protection Basic helps protect all Azure services, including PaaS services like Azure DNS. Azure DDoS Protection Standard , combined with application design best practices, provides enhanced DDoS mitigation features to defend against DDoS attacks. It is automatically tuned to help protect specific Azure resources in a virtual network. Protection is simple to enable on any new or existing virtual network, and it requires no application or resource changes. It has several advantages over the basic service, including logging, alerting, and telemetry. More details on Azure DDoS Protection can be obtained from Azure DDoS Protection Standard documentation .","title":"Azure DDoS Protection"},{"location":"lzdesign/nwtopology/#azure-network-watcher","text":"Azure Network Watcher provides tools to monitor, diagnose, view metrics, and enable or disable logs for resources in an Azure virtual network. Network Watcher is designed to monitor and repair the network health of IaaS (Infrastructure-as-a-Service) products such as Virtual Machines, Virtual Networks, Application Gateways, and Load Balancers. It is not intended for and will not work for PaaS monitoring or Web analytics. When a VNet is created or updated, Network Watcher will be enabled automatically in the VNet's region. If needed, it can also be enabled in any Azure region manually. The capabilities provided by Network Watcher are described briefly below. The Connection Monitor checks the communication between a VM and an endpoint and reports regularly on reachability, latency, and changes in network topology between the VM and the endpoint. Network Performance Monitor helps monitor network performance between various points in the network infrastructure. It detects network issues like traffic blackholing, routing errors, and issues that conventional network monitoring methods can't detect. The Topology capability generates a visual diagram of the resources in a virtual network, and the relationships between the resources. The IP Flow Verify capability discovers and reports on security rules that may be inhibiting traffic between two IP endpoints. The Next Hop capability reports on the next hop used to route traffic from one IP address to another thus enabling users to fix routing issues. The Connection Troubleshoot capability is like Connection Monitor except that it runs when explicitly invoked as against periodically. The Capture capability can capture packets to and from a VM. Filtering options and controls such as time and size can be configured. Captured data can be stored in Azure Storage and analyzed using a variety of tools. The VPN Diagnostics capability diagnoses the health of the gateway and its connections and provides detailed information on the cause for any issues. The Security Group View capability shows the security rules applicable to a network interface and is a combination of the rules applicable at both the subnet the network interface is in and the network interface itself. The Network Subscription Limit capability reports on the limits and the actual utilization of network resources for a subscription and region. The NSG Flow Log capability enables logging of the source and destination IP address, port, protocol, and whether traffic was allowed or denied by an NSG. The Diagnostic Logs capability provides a single interface to enable and disable network resource diagnostic logs for any existing network resource that generates a diagnostic log. These logs can be viewed using tools such as Microsoft Power BI and Azure Monitor logs. Network Watcher capability reports on latencies between Azure regions and across Internet service providers. More details on Azure Network Watcher can be obtained from Azure Network Watcher documentation .","title":"Azure Network Watcher"},{"location":"lzdesign/nwtopology/#azure-application-gateway","text":"Azure Application Gateway is a web traffic load balancer for web applications. It makes routing decisions based on additional attributes of an HTTP request, for example URI path or host headers. It is therefore called an application layer or OSI layer 7 load balancer. More details on the Azure Application Gateway can be obtained from Azure Application Gateway documentation .","title":"Azure Application Gateway"},{"location":"lzdesign/nwtopology/#azure-service-endpoints","text":"Azure PaaS services are accessible using pubic IP addresses. This feature allows access to Azure PaaS Services (for example, Azure Storage and SQL Database) from a VNet without having to use the path commonly used by traffic directed to the Internet. The Azure PaaS service retains its public IP address. A route is established over the Azure backbone between the VNet and the Azure PaaS service. Firewall rules can be created for the Azure PaaS service allowing dedicated access to this VNet. However, connectivity to Azure IaaS services from customer premises using service endpoints is not possible. A service endpoint can be configured as part of a subnet configuration. More details can be obtained from Virtual Network service endpoints .","title":"Azure Service Endpoints"},{"location":"lzdesign/nwtopology/#azure-private-link","text":"Azure Private Link (Private Endpoint) allows you to access Azure PaaS services over Private IP address within the VNet. It gets a new private IP on your VNet. When you send traffic to PaaS resource, it will always ensure traffic stays within your VNet. This feature enables access to Azure PaaS Services (for example, Azure Storage and SQL Database) and /or customer-owned/partner services hosted in Azure over a private IP address within the VNet. The traffic between the service and the consumer of the service does not go over the Internet. Configuration of the service consists of two steps. First, a private link service needs to be created that enables private connections to an existing service. Next, a private endpoint needs to be created that enables access to the existing service via the private link service. Azure Private Link has integration with Azure Monitor. More details can be obtained from Azure Private Link documentation .","title":"Azure Private Link"},{"location":"lzdesign/nwtopology/#dedicated-azure-services-in-vnets-vnet-integration-injection","text":"This feature enables deployment of Azure PaaS services (for example Azure SQL) and Azure hosted customer-owned/partner services within a subnet of VNet. The service is then accessible within the VNet, across VNets via VNet peering as well as from customer premises. A PaaS service can be injected into a VNet as part of the instantiation of the service. More details on VNet integration injection can be obtained from Integrate Azure services .","title":"Dedicated Azure services in VNets (VNet Integration) Injection"},{"location":"lzdesign/nwtopology/#vpn-gateway","text":"A VPN gateway is a type of virtual network gateway that is used to send encrypted traffic between a VNet and an on-premises location over the public Internet. Each VNet can have only one VPN gateway. However, multiple connections can be created to the same VPN gateway with all connections sharing the available gateway bandwidth. The encryption over public Internet limits the performance in terms of bandwidth and latency. A VPN gateway is implemented as a pair of VMs in a dedicated subnet called the gateway subnet. After a VPN gateway is created, an IPsec/IKE VPN tunnel connection between that VPN gateway and another VPN gateway (VNet-to-VNet), or a cross-premises IPsec/IKE VPN tunnel connection between the VPN gateway and an on-premises VPN device (Site-to-Site) can be provisioned. It is also possible to create a Point-to-Site VPN connection (VPN over OpenVPN, IKEv2, or SSTP), which allows connection to the virtual network from a remote location, such as from home. In general, the VPN Gateway is meant to be used for Development, Test and Laboratory scenarios Small-scale production workloads More details on VPN gateways can be obtained from VPN Gateway documentation .","title":"VPN Gateway"},{"location":"lzdesign/nwtopology/#expressroute-gateway","text":"An ExpressRoute gateway is a type of virtual network gateway that is used to send traffic between a VNet and an on-premises location over a private link. Each VNet can have only one ExpressRoute gateway. However, multiple ExpressRoute circuits can be created to the same ExpressRoute gateway with all circuits sharing the same available gateway bandwidth. Similarly, multiple VNets can be linked to an ExpressRoute circuit. The private link can either be a point to point physical link such as a fiberoptic cable carrying Ethernet or an any to any fabric such as Multi-Protocol Label Switching (MPLS). This link is established in a Meet-Me location between Microsoft routers and Customer routers, the latter connecting to the Customer on-premises network. The Microsoft routers connect to the Azure backbone. The Microsoft routers, the Customer routers any network fabric between them provided by a Service Provider all constitute an ExpressRoute circuit. The ExpressRoute gateway is implemented as a pair of VMs, each supporting route advertisement and data traffic routing for the ExpressRoute circuit. When an ExpressRoute gateway is configured, it establishes an IP path over the backbone, the Microsoft routers and Customer routers to the on-premises network. ExpressRoute circuits provide two peering paths between on-premises and Microsoft Azure. Microsoft peering so Azure cloud services are accessible from on-premises. Private peering so VNets are accessible from on-premises. By default, peering through a location in a particular region allows on-premises access to the Azure cloud services across all the regions in that geopolitical region. ExpressRoute Premium can be enabled to extend connectivity across geopolitical boundaries. ExpressRoute Global Reach enables different on-premises sites to connect to different peering locations and use the Azure backbone to transfer data between sites. Connections over ExpressRoute circuits offer more reliability, faster speeds, and lower latencies. They are ideal for Data backup and replication Hybrid production workloads spanning on-premise and Azure cloud More details on ExpressRoute gateways can be obtained from Azure ExpressRoute documentation .","title":"ExpressRoute Gateway"},{"location":"lzdesign/nwtopology/#virtual-wan","text":"Virtual WAN is a network service that combines the functions of routing, firewall, encryption, VPN gateway and ExpressRoute gateway. Unlike other solutions for hybrid connectivity, the Virtual WAN network service is managed by Microsoft. In this document, it is covered from the Virtual WAN network topology point view and not as a standalone element. It provides services such as Branch connectivity (via connectivity automation from Virtual WAN Partner devices such as SD-WAN or VPN CPE) Site-to-site VPN connectivity Remote user VPN (Point-to-site) connectivity Private (ExpressRoute) connectivity Intra-cloud connectivity (transitive connectivity for virtual networks) Routing between two branches one connected via VPN the other via ExpressRoute More details on Virtual WAN can be obtained from Azure Virtual WAN documentation .","title":"Virtual WAN"},{"location":"lzdesign/nwtopology/#user-defined-routes-udr","text":"Azure creates default routes for each subnet in a VNet that allows routing of traffic within the subnets of that VNet as well as to the Internet. In addition, when VNet peering is enabled, routes for all the subnets in the remote VNet are added for each subnet within the local VNet. Also, all the routes advertised by on-premises routers to a virtual network gateway in a VNet are setup for all the subnets in all the VNets that are peered with the VNet hosting the virtual network gateway. It is also possible for the user to set up custom routes for a subnet. Take the case where a Network Virtual Appliance (NVA) that is responsible for routing is installed in a VNet. A custom route for a subnet in a VNet peered with the VNet hosting the NVA can be created by a user. Another situation is where routing between different subnet pairs needs to be setup within a VNet. More details on UDR can be obtained from Virtual Network traffic routing documentation .","title":"User Defined Routes (UDR)"},{"location":"lzdesign/nwtopology/#azure-firewall-manager","text":"Azure Firewall Manager is a security management service that provides central security policy and route management for cloud-based security perimeters. Firewall Manager can provide security management for two network architecture types: Secure virtual hub - An Azure Virtual WAN hub with associated security and routing policies. Supports Azure Firewall and third-party Security as a Service (SECaaS) providers. Hub virtual network -- A standard virtual network with associated security policies. Supports Azure Firewall only. More details on Azure Firewall Manager can be obtained from Azure Firewall Manager documentation .","title":"Azure Firewall Manager"},{"location":"lzdesign/nwtopology/#azure-networking-use-cases","text":"Microsoft Azure supports many networking scenarios. The below table summarized the use cases and associated Azure services.","title":"Azure Networking Use Cases"},{"location":"lzdesign/nwtopology/#network-topologies","text":"The information presented thus far pertains to the network components and functions in Azure. This sub-section covers the network topologies commonly used to connect virtual networks in Azure and networks on-premises for any-to-any traffic flows.","title":"Network Topologies"},{"location":"lzdesign/nwtopology/#hub-spoke-network-topology-using-virtual-network-gateways","text":"This architecture is an extension of the VPN and ExpressRoute Gateways proposed in the preceding sections. It combines the VNet-VNet connectivity with the use of virtual gateways to support the connectivity between customer premises and one or more VNets in Azure hosting customer workloads. One VNet acts as the hub that the customer premises connect to and workload VNets are peered with. The benefits of this architecture are the ability to share network services across multiple workload environments. separation of the centralized management of the network services from the management of the workload environments By default, each workload VNet can communicate with the customer premises through the Hub VNet but not with other workload VNets. It is however possible to configure the Hub and workload VNets to forward traffic between the workload VNets. While either the VPN gateway or the ExpressRoute gateway can be used, the bandwidth available will be limited by the capabilities of these gateways. A better alternative is to use the Azure Firewall or other Network Virtual Appliance in the Hub VNet that can function as a router between the workload VNets. The virtual gateway is used solely for traffic between the Hub VNet and the customer premises. Note that while the diagram shows VPN and ExpressRoute connections to a virtual network gateway, a VPN gateway supports only S-2-S and P-2-S VPN connections, and an ExpressRoute gateway supports only ExpressRoute connections. The main use cases for this network topology are - Workloads deployed in different environments, such as development, testing, and production, that require shared services such as DNS, IDS, NTP, or AD DS. Shared services are placed in the hub virtual network, while each environment is deployed to a spoke to maintain isolation. Enterprises that require central control over security aspects, such as a firewall in the hub as a DMZ, and segregated management for the workloads in each spoke. The VPN Gateway can be enabled for data transfers between parts of a workload distributed across either VNets or across on-premises and VNets. Note that such use is not recommended for production environments. The ExpressRoute Gateway can be enabled for data transfers between parts of a workload distributed across either VNets or across on-premises and VNets. It can be used for Production workloads as long as the number of VNets and on-premise sites is limited. More details on Hub-Spoke Network Topology can be obtained from Hub-spoke network topology in Azure documentation .","title":"Hub-Spoke Network Topology using Virtual Network Gateways"},{"location":"lzdesign/nwtopology/#hub-spoke-network-topology-with-azure-virtual-wan","text":"This architecture is like the previously discussed Hub-Spoke architecture except that in this case the Hub VNet (called Virtual Hub) uses network components described in the previous section on Azure Virtual WAN. As seen in the above figure, each region can deploy a Virtual Hub. The Virtual Hubs in multiple regions are connected in a full mesh over the Azure Backbone. Thus, workload VNets in Region 2 are reachable from both the customer premises connected to Region 1 and the workload VNets in Region 1. Microsoft manages the Virtual Hub infrastructure. This means that the lifecycle of the Virtual Hub Router, VPN Gateway, ExpressRoute Gateway, Azure Firewall and any other network elements incorporated into the Azure Virtual WAN are all managed by Microsoft. The sizing of the network elements incorporated into the Azure Virtual WAN enables higher bandwidth for data traffic. Use of the Azure Virtual WAN entails manual configuration. Microsoft partners provide their own Customer Premise Equipment (CPE) NVA that support automatic configuration and optimization in concert with their SD-WAN solutions and that can be installed in the Virtual Hub. The main use cases for this network topology are -- Connectivity among workloads distributed across VNets and on-premise sites with central control and access to shared services. An enterprise requires central control over security aspects, such as a firewall, and requires segregated management for the workloads in each spoke. More details on Hub-Spoke Network Topology with Azure Virtual WAN can be obtained from Hub-spoke network topology with Azure Virtual WAN documentation .","title":"Hub-Spoke Network Topology with Azure Virtual WAN"},{"location":"lzdesign/nwtopology/#design-actions-by-solution-architects","text":"This sub-section provides guidance at a high level on the key design considerations and best practices for creating a network architecture using the above described network components, functions, and topologies.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/nwtopology/#1plan-for-ip-addressing","text":"It is necessary to determine the IP address ranges to be assigned to on-premise networks, if any, and the VNets in Azure. One key design consideration is to ensure none of the assigned address ranges overlap. Another consideration is to ensure optimal use of the address ranges, particularly in the case of IPv4 addressing. A detailed discussion of the design considerations and recommendations can be obtained from Plan for IP Addressing documentation .","title":"1.Plan for IP Addressing"},{"location":"lzdesign/nwtopology/#2-design-dns-for-on-premise-and-azure-resources","text":"DNS can be configured to either leverage on-premise DNS infrastructure or to leverage Azure cloud. The earlier section on DNS provides a brief description of DNS support in Azure and links to more detailed information. One design consideration is that the maximum number of private DNS zones to which a virtual network can link with auto-registration is one. Another design consideration is that the maximum number of private DNS zones to which a virtual network can link is 1,000 all without auto-registration enabled. A detailed discussion of the design considerations and recommendations can be obtained from DNS for on-premises and Azure resources documentation .","title":"2. Design DNS for on-premise and Azure resources"},{"location":"lzdesign/nwtopology/#3private-link-and-dns-integration","text":"The Private Link feature as well as private DNS have been discussed in previous sub-sections. Private DNS allows a VNet to be associated with a custom zone. However, when the customer wishes to associate a private DNS A record with such customer defined PaaS service, there could be challenges in terms of accessibility given the implementation approach of the private DNS. Details of how such custom PaaS services can be provided A records in the private DNS can be obtained from Private Link and DNS integration at scale documentation .","title":"3.Private Link and DNS Integration"},{"location":"lzdesign/nwtopology/#4connectivity-to-azure","text":"In terms of data traffic, ExpressRoute is the preferred approach to connecting on-premises to Azure. ExpressRoute has several characteristics such as peering for Microsoft cloud SaaS/PaaS services and Global Reach that need to be considered along with limitations such as needing a peering location. Redundancy can be achieved through dual circuits across different peering locations and use of BGP. A detailed discussion of the design considerations and recommendations can be obtained from Connectivity to Azure documentation .","title":"4.Connectivity to Azure"},{"location":"lzdesign/nwtopology/#5define-an-azure-network-topology","text":"Network topology is a critical element of the solution design because it defines how applications can communicate with each other. Two network topologies have been presented in the previous section. The Hub-Spoke network topology with Virtual Network gateways is to be mainly used where there are only a few on-premise sites to connect to Azure. The Hub-Spoke network topology with Virtual WAN is to be mainly used where there are many regional and branch locations to be connected to Azure. Other decision criteria can be obtained from Define an Azure network topology documentation .","title":"5.Define an Azure network topology"},{"location":"lzdesign/nwtopology/#6network-design-with-virtual-wan","text":"Virtual WAN is a managed service with unique features such as any-to-any connectivity across VNets, regional sites and branch sites, routing between VPN connected and ExpressRoute connected sites, Firewall, and encryption. But there are certain limitations such as inability to support NVAs and UDRs. The detailed design considerations and recommendations for Virtual WAN topology can be obtained from Virtual WAN network topology (Microsoft-managed) documentation .","title":"6.Network Design with Virtual WAN"},{"location":"lzdesign/nwtopology/#7network-design-with-virtual-network-gateways","text":"Virtual Network gateways have certain features and limitations that need to be considered during network design. While a hup-spoke topology with Virtual Network gateways provides flexibility in terms of NVAs and UDR, the limitations in terms of bandwidth and connections need to be considered as well. The detailed design considerations and recommendations for Virtual Network gateways can be obtained from Traditional Azure networking topology documentation .","title":"7.Network Design with Virtual Network gateways"},{"location":"lzdesign/nwtopology/#8connectivity-to-azure-paas-services","text":"Often, there is a requirement to avoid access to Azure PaaS and customer defined PaaS services through public IP addresses. The detailed design considerations and recommendations for use of techniques such as VNet injection and Private Link can be obtained from Connectivity to Azure PaaS services documentation .","title":"8.Connectivity to Azure PaaS services"},{"location":"lzdesign/nwtopology/#9plan-for-inbound-and-outbound-connectivity","text":"Internet connectivity, Azure Firewall and Application Gateway were discussed briefly in preceding sub-sections. Azure Firewall and Application Gateway are fully managed services so there are no operational or management costs. NVAs can be used instead if special functionality that these Azure services do not provide are needed. The detailed design considerations and recommendations for the use of these components to comprehensively secure Internet traffic can be obtained from Plan for inbound and outbound internet connectivity documentation .","title":"9.Plan for Inbound and Outbound Connectivity"},{"location":"lzdesign/nwtopology/#10plan-for-application-delivery","text":"The Azure Load Balancer and Application Gateway were discussed briefly in previous sub-sections. Azure Application Gateway allows the secure delivery of HTTP/S applications at a regional level while Azure Front Door allows the secure delivery of highly available HTTP/S applications across Azure regions. The detailed design considerations and recommendations for the use of these components to deliver internal and external facing applications in a secure, highly scalable and highly available manner can be obtained from Plan for application delivery documentation .","title":"10.Plan for Application Delivery"},{"location":"lzdesign/nwtopology/#11plan-for-landing-zone-network-segmentation","text":"NSGs were discussed briefly in a previous sub-section. NSGs help protect traffic across subnets, as well as east/west traffic across the platform (traffic between landing zones) while application security groups at the subnet-level NSGs help protect multitier VMs within the landing zone. The detailed design considerations and recommendations for the use of NSGs and other controls to drive a network zero-trust implementation can be obtained from Plan for landing zone network segmentation documentation .","title":"11.Plan for Landing Zone Network Segmentation"},{"location":"lzdesign/nwtopology/#12plan-for-network-encryption","text":"While two network topologies have been briefly discussed in earlier sub-sections, encryption of traffic between on-premises and Azure has not been covered. While a VPN gateway provides encryption of data passing through the IPSEC tunnel, an ExpressRoute gateway does not provide any encryption by default. The design considerations and recommendations for providing encryption with ExpressRoute gateways and the Virtual WAN gateways can be obtained from Plan for Network Encryption documentation .","title":"12.Plan for Network Encryption"},{"location":"lzdesign/nwtopology/#13plan-for-traffic-inspection","text":"In many industries, organizations require that traffic in Azure is mirrored to a network packet collector for deep inspection and analysis. This requirement typically focuses on inbound and outbound internet traffic. Network Watcher can capture packets although for a limited window of 5 hours. Third party tools are recommended for deep packet inspection. The detailed design considerations and recommendations for mirroring or tapping traffic within Azure Virtual Network can be obtained from Plan for Traffic Inspection documentation .","title":"13.Plan for Traffic Inspection"},{"location":"lzdesign/orgmgmt/","text":"Organization Management \u2693\ufe0e The next critical Landing Zone design element is defining the structure to create and manage subscriptions in order to effectively enforce Role based access control (RBAC) and Azure policies across all subscriptions in an enterprise. This is facilitated in Azure through four layers of management: 1. Management groups : containers that help manage access, policy, and compliance for multiple subscriptions. 2. Subscriptions : A subscription groups together user accounts and the resources that those accounts create. Limits and quotas can be applied, and each organization can use subscriptions to manage costs and resources by group. 3. Resource groups : A resource group is a logical container into which Azure resources such as web apps, databases, and storage accounts are deployed and managed. 4. Resources : Resources are instances of services that you create, such as virtual machines, storage, or SQL databases. Below diagram explains the primary driving factors that needs to be considered for defining the Azure Management scope. Getting started \u2693\ufe0e Build a flexible structure of management groups and subscriptions to organize your resources into a hierarchy for unified policy and access management in the below order while discussing with customer. 1. Management Group \u2693\ufe0e 1.1. Design Recommendations for Management Group \u2693\ufe0e Keep the management group hierarchy reasonably flat with no more than three to four levels, ideally. This restriction reduces management overhead and complexity. Management groups should be used for policy assignment versus billing purposes. This approach necessitates using management groups for their intended purpose in enterprise-scale architecture, which is providing Azure policies for workloads that require the same type of security and compliance under the same management group level. Create management groups under root-level management group to represent the types of workloads to be hosted. Example - Create a top-level sandbox management group to allow users to immediately experiment with Azure. Management groups can also be created based on their security, compliance, connectivity, and feature needs. This grouping structure allows to have a set of Azure policies applied at the management group level for all workloads that require the same security, compliance, connectivity, and feature settings. All the management groups must have a standard naming convention that has a prefix or suffix mg- The following diagram shows an example of creating a hierarchy for governance using management groups. *Each Business Unit (BU) runs its own Azure subscriptions, for easy billing. *Each BU basically has two subscriptions: one for production and one for non-production (segregation of environments and facilitate change management) *Common infrastructure (e.g. core network, identity, Management (log analytics, Azure Arc etc) on its own Azure subscription owned by mg-Platform A Dedicated sandbox Management group and subscription are created to allow users for learning and experimenting with Azure Detailed information on Subscriptions can be obtained from Subscription decision guide . 2.Subscriptions \u2693\ufe0e Subscriptions are a unit of management, billing, and scale within Azure. They play a critical role when designing for large-scale Azure adoption. This section helps capture subscription requirements and design target subscriptions based on critical factors. These factors are environment type, ownership and governance model, organizational structure, and application portfolios. A bad subscription design can impact several aspects of enterprise IT governance: \u2022 Can increase the complexity of IP allocation and management. \u2022 Makes routing and firewall configurations difficult to operate and manage. \u2022 May have to deploy multiple instances of monitoring, anti-virus, patching, backup services thereby increase the cost of management. Subscription Models \u2693\ufe0e There are different subscription models that can be chosen based on the scenario, common scenarios defined below: Enterprise Scenario - Multiple Subscription Model \u2693\ufe0e Enterprise customers should generally adopt multiple subscriptions and should align with the enterprise LZ architecture. Above example shows unique subscriptions based on platform function (Identity) as well as environment type (Dev, Test & Prod). This can be expanded to have additional subscriptions for Network functions, Monitoring and management functions, and for application workloads. Subscription Model for Small Scale Enterprise \u2693\ufe0e This reference implementation is well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure by implementing a network architecture based on the traditional hub and spoke network topology. In this model, all platform operations are combined in a single \u201cPlatform subscription\u201d. Subscription Model for CSP \u2693\ufe0e CSP (Cloud Service Provider) allows Service Providers like Kyndryl own the subscriptions and bill the client's based on usage or negotiated fixed pricing. Kyndryl is not yet a CSP for Azure, at this time, and hence this model is not applicable 2.1 Design Recommendation for Subscriptions \u2693\ufe0e Consider creating separate subscriptions based on below factors: \u2022 Business requirements such as availability, recoverability, performance, cost centres and chargebacks. \u2022 Technical requirements like network connectivity, AD requirement, and considerations around management tools. \u2022 Security considerations like policies, subscription administrators, and implementation of a least privilege administrative model, among others. \u2022 Additional considerations include scalability plans, subscription owner, Office 365 AAD tenant set up, trial Power BI evaluation, trust issues between owners of a subscription and the owner of resources to be deployed. \u2022 Rigid financial or geopolitical controls might require separate financial arrangements for specific subscriptions. \u2022 Platform Management: For small enterprises consider single subscription for platform management. If the operating model facilitates segregating platform administration duties among different teams, then separating with different subscriptions is recommended. Subscription Design Patterns : Additional details on design decisions for subscriptions can be obtained from Subscription decision guide . Naming Standards for Subscriptions \u2693\ufe0e Follow below naming standards for subscriptions Naming component Intent Examples Business unit Top-level division of your company that owns the subscription or workload the resource belongs to. In smaller organizations, this may represent a single corporate top-level organizational element. fin , mktg , product , it , corp Subscription type/environment Summary description of the purpose of the subscription containing the resource. Often broken down by deployment environment type or specific workloads. prod, s*hared, client* Application / Service name Name of the application, workload, or service that the resource is a part of. navigator , emissions , sharepoint , hadoop Region Azure region where the resource is deployed. *westus, eastus2, westeurope, w*estgermany Example \u00b7 IT-PROD-WVD-WESTUS \u00b7 IT-PROD-Backup-EastUS 3.Resource Groups \u2693\ufe0e The most mature way to organize is by business unit or service unit \u2013 the model that gives ownership of resources to corporate functions. For example, the finance department would have a subscription, and the marketing department another. Underneath those subscriptions would be the resource groups corresponding to each application. Resource groups help to group and organize your resources in a container. You can assign Role based permissions to a resource group to restrict who can access the resource with a resource group. 3.1 Design Recommendation for Resource Groups (add/modify as appropriate) \u2693\ufe0e Design guidelines for Resources groups \u2693\ufe0e Resources in a group should have the same life cycle. For example, if an application requires different resources that need to be updated together, such as having a SQL database, a web app, or a mobile app, then it makes sense to group these resources in the same resource group Kyndryl best practice is to keep all resources in a resource group in the same region to reduce latency or cross-region data transfer. Regardless, the group needs a location to specify where the metadata will be stored, which is necessary for compliance policies Grant access with resource groups: use resource groups to control access to your resources Detailed information on Resource Groups can be obtained from Manage Azure Resource Manager resource groups . Naming convention for Resource Groups \u2693\ufe0e Identify the key pieces of information that you want to reflect in a resource name. Different information is relevant for different resource types. The following list provides examples of information that are useful when you construct resource names. Follow below mandatory naming standards for Resource Group Names on Kyndryl managed accounts Naming Component Example Mandatory Resource type use prefix or suffix rg for resources group, vm for virtual machine snet for subnet Yes Business unit It, corp,mktg, fin Yes Application services wvd,sharepoint,sap Yes Subscription type Prod,shared dev Yes Deployment environment Prod,dr,test Optional if the subscription is defined per environment Region Westus,eastus Yes Click here to refer to sample naming convention for individual components. 4. Resources \u2693\ufe0e 4.1 Design Recommendations for Resources \u2693\ufe0e A naming and tagging strategy include business and operational details as components of resource names and metadata tags: The business side of this strategy ensures that resource names and tags include the organizational information needed to identify the teams. Use a resource along with the business owners who are responsible for resource costs. The operational side ensures that names and tags include information that IT teams use to identify the workload, application, environment, criticality, and other information useful for managing resources. Tagging conventions for Resources \u2693\ufe0e Apply tags to resources, resources groups and subscriptions to logically organize them into a taxonomy. Tags required for billing (Department and Cost Centre) will be enforced by policy at Landing Zone Management Group scope. Below are the mandatory tags required for Kyndryl managed resources Tag Name Description Value(s) Name=Value Examples Mandatory Values? Application Name of the Application the resourxe is part of Any name, can be more than value, comma seperated Application = WVD, Sharepoint, HCM, etc Yes Service Name of the Service the resource is part of Any name, can be more than value, comma seperated Application = Procurement, Employee Verification No Environment Deployment Environment of this application, workload or service Prod/Non-Prod, Environment = Prod, NonProd Yes Criticality Business criticality of the Application Mission Critical, Essential Disaster recovery = Mission Critical, Critical, Essential No Kyndryl Managed To identify if resource is managed by Kyndryl Yes/No KyndrylManaged=Yes/No Yes Operations team Team accountable for day to day operations, can be used to define Ops Console target also IBM, Kyndryl etc. Operationteam=BoulderOps No Ticketing Group Incident Ticket queue against which tickets will be created for this resource IBM, Contoso etc. TicketingGroup=IBM_SMT001 Yes Notification Group Notification Group where alerts will be sent to for this resource Notification Group Name or email IDs Notification Group= Yes Kyndryl Customer Code 3 char Customer Identifier for Kyndryl Management Kyndryl CustomerCode=iga Yes Business unit Top level vision of customer that owns the subscription and workload EAP, IT01, etc Business Unit = EAP No Owner Name Owner of the application, workload or service Email addresses OwnerName= john@dn.ibm.com No More details on tagging can be obtained from the Naming and Tagging Convention template . Design Actions by Solution Architects. \u2693\ufe0e Define management group hierarchy, in consulation with client. Identify the appropriate Subscription Model for an account, along with the client IT team. Ensure naming standards are defined & enforced for management groups, subscriptions & resource groups Ensure tagging standards are defined & enforced, and mandatory tags defined to every resource as part of a policy Important note to Kyndryl Architect Though Business Continuity & Disaster Recovery(BCDR) may not be under consideration during the early adoption stage, making the assumption that BCDR will be in scope at some point in future ensures the creation of various elements in the correct region, like Resource Groups, Log Analytics workspaces etc. Azure paired regions must be used to align with Azure BCDR. A separate section related to BCDR will be updated to provide additional guidance","title":"3.Organization Mgmt"},{"location":"lzdesign/orgmgmt/#organization-management","text":"The next critical Landing Zone design element is defining the structure to create and manage subscriptions in order to effectively enforce Role based access control (RBAC) and Azure policies across all subscriptions in an enterprise. This is facilitated in Azure through four layers of management: 1. Management groups : containers that help manage access, policy, and compliance for multiple subscriptions. 2. Subscriptions : A subscription groups together user accounts and the resources that those accounts create. Limits and quotas can be applied, and each organization can use subscriptions to manage costs and resources by group. 3. Resource groups : A resource group is a logical container into which Azure resources such as web apps, databases, and storage accounts are deployed and managed. 4. Resources : Resources are instances of services that you create, such as virtual machines, storage, or SQL databases. Below diagram explains the primary driving factors that needs to be considered for defining the Azure Management scope.","title":"Organization Management"},{"location":"lzdesign/orgmgmt/#getting-started","text":"Build a flexible structure of management groups and subscriptions to organize your resources into a hierarchy for unified policy and access management in the below order while discussing with customer.","title":"Getting started"},{"location":"lzdesign/orgmgmt/#1-management-group","text":"","title":"1. Management Group"},{"location":"lzdesign/orgmgmt/#11-design-recommendations-for-management-group","text":"Keep the management group hierarchy reasonably flat with no more than three to four levels, ideally. This restriction reduces management overhead and complexity. Management groups should be used for policy assignment versus billing purposes. This approach necessitates using management groups for their intended purpose in enterprise-scale architecture, which is providing Azure policies for workloads that require the same type of security and compliance under the same management group level. Create management groups under root-level management group to represent the types of workloads to be hosted. Example - Create a top-level sandbox management group to allow users to immediately experiment with Azure. Management groups can also be created based on their security, compliance, connectivity, and feature needs. This grouping structure allows to have a set of Azure policies applied at the management group level for all workloads that require the same security, compliance, connectivity, and feature settings. All the management groups must have a standard naming convention that has a prefix or suffix mg- The following diagram shows an example of creating a hierarchy for governance using management groups. *Each Business Unit (BU) runs its own Azure subscriptions, for easy billing. *Each BU basically has two subscriptions: one for production and one for non-production (segregation of environments and facilitate change management) *Common infrastructure (e.g. core network, identity, Management (log analytics, Azure Arc etc) on its own Azure subscription owned by mg-Platform A Dedicated sandbox Management group and subscription are created to allow users for learning and experimenting with Azure Detailed information on Subscriptions can be obtained from Subscription decision guide .","title":"1.1. Design Recommendations for Management Group"},{"location":"lzdesign/orgmgmt/#2subscriptions","text":"Subscriptions are a unit of management, billing, and scale within Azure. They play a critical role when designing for large-scale Azure adoption. This section helps capture subscription requirements and design target subscriptions based on critical factors. These factors are environment type, ownership and governance model, organizational structure, and application portfolios. A bad subscription design can impact several aspects of enterprise IT governance: \u2022 Can increase the complexity of IP allocation and management. \u2022 Makes routing and firewall configurations difficult to operate and manage. \u2022 May have to deploy multiple instances of monitoring, anti-virus, patching, backup services thereby increase the cost of management.","title":"2.Subscriptions"},{"location":"lzdesign/orgmgmt/#subscription-models","text":"There are different subscription models that can be chosen based on the scenario, common scenarios defined below:","title":"Subscription Models"},{"location":"lzdesign/orgmgmt/#enterprise-scenario-multiple-subscription-model","text":"Enterprise customers should generally adopt multiple subscriptions and should align with the enterprise LZ architecture. Above example shows unique subscriptions based on platform function (Identity) as well as environment type (Dev, Test & Prod). This can be expanded to have additional subscriptions for Network functions, Monitoring and management functions, and for application workloads.","title":"Enterprise Scenario - Multiple Subscription Model"},{"location":"lzdesign/orgmgmt/#subscription-model-for-small-scale-enterprise","text":"This reference implementation is well suited for customers who want to start with Landing Zones for their net new deployment/development in Azure by implementing a network architecture based on the traditional hub and spoke network topology. In this model, all platform operations are combined in a single \u201cPlatform subscription\u201d.","title":"Subscription Model for Small Scale Enterprise"},{"location":"lzdesign/orgmgmt/#subscription-model-for-csp","text":"CSP (Cloud Service Provider) allows Service Providers like Kyndryl own the subscriptions and bill the client's based on usage or negotiated fixed pricing. Kyndryl is not yet a CSP for Azure, at this time, and hence this model is not applicable","title":"Subscription Model for CSP"},{"location":"lzdesign/orgmgmt/#21-design-recommendation-for-subscriptions","text":"Consider creating separate subscriptions based on below factors: \u2022 Business requirements such as availability, recoverability, performance, cost centres and chargebacks. \u2022 Technical requirements like network connectivity, AD requirement, and considerations around management tools. \u2022 Security considerations like policies, subscription administrators, and implementation of a least privilege administrative model, among others. \u2022 Additional considerations include scalability plans, subscription owner, Office 365 AAD tenant set up, trial Power BI evaluation, trust issues between owners of a subscription and the owner of resources to be deployed. \u2022 Rigid financial or geopolitical controls might require separate financial arrangements for specific subscriptions. \u2022 Platform Management: For small enterprises consider single subscription for platform management. If the operating model facilitates segregating platform administration duties among different teams, then separating with different subscriptions is recommended. Subscription Design Patterns : Additional details on design decisions for subscriptions can be obtained from Subscription decision guide .","title":"2.1 Design Recommendation for Subscriptions"},{"location":"lzdesign/orgmgmt/#naming-standards-for-subscriptions","text":"Follow below naming standards for subscriptions Naming component Intent Examples Business unit Top-level division of your company that owns the subscription or workload the resource belongs to. In smaller organizations, this may represent a single corporate top-level organizational element. fin , mktg , product , it , corp Subscription type/environment Summary description of the purpose of the subscription containing the resource. Often broken down by deployment environment type or specific workloads. prod, s*hared, client* Application / Service name Name of the application, workload, or service that the resource is a part of. navigator , emissions , sharepoint , hadoop Region Azure region where the resource is deployed. *westus, eastus2, westeurope, w*estgermany Example \u00b7 IT-PROD-WVD-WESTUS \u00b7 IT-PROD-Backup-EastUS","title":"Naming Standards for Subscriptions"},{"location":"lzdesign/orgmgmt/#3resource-groups","text":"The most mature way to organize is by business unit or service unit \u2013 the model that gives ownership of resources to corporate functions. For example, the finance department would have a subscription, and the marketing department another. Underneath those subscriptions would be the resource groups corresponding to each application. Resource groups help to group and organize your resources in a container. You can assign Role based permissions to a resource group to restrict who can access the resource with a resource group.","title":"3.Resource Groups"},{"location":"lzdesign/orgmgmt/#31-design-recommendation-for-resource-groups-addmodify-as-appropriate","text":"","title":"3.1 Design Recommendation for Resource Groups (add/modify as appropriate)"},{"location":"lzdesign/orgmgmt/#design-guidelines-for-resources-groups","text":"Resources in a group should have the same life cycle. For example, if an application requires different resources that need to be updated together, such as having a SQL database, a web app, or a mobile app, then it makes sense to group these resources in the same resource group Kyndryl best practice is to keep all resources in a resource group in the same region to reduce latency or cross-region data transfer. Regardless, the group needs a location to specify where the metadata will be stored, which is necessary for compliance policies Grant access with resource groups: use resource groups to control access to your resources Detailed information on Resource Groups can be obtained from Manage Azure Resource Manager resource groups .","title":"Design guidelines for Resources groups"},{"location":"lzdesign/orgmgmt/#naming-convention-for-resource-groups","text":"Identify the key pieces of information that you want to reflect in a resource name. Different information is relevant for different resource types. The following list provides examples of information that are useful when you construct resource names. Follow below mandatory naming standards for Resource Group Names on Kyndryl managed accounts Naming Component Example Mandatory Resource type use prefix or suffix rg for resources group, vm for virtual machine snet for subnet Yes Business unit It, corp,mktg, fin Yes Application services wvd,sharepoint,sap Yes Subscription type Prod,shared dev Yes Deployment environment Prod,dr,test Optional if the subscription is defined per environment Region Westus,eastus Yes Click here to refer to sample naming convention for individual components.","title":"Naming convention for Resource Groups"},{"location":"lzdesign/orgmgmt/#4-resources","text":"","title":"4. Resources"},{"location":"lzdesign/orgmgmt/#41-design-recommendations-for-resources","text":"A naming and tagging strategy include business and operational details as components of resource names and metadata tags: The business side of this strategy ensures that resource names and tags include the organizational information needed to identify the teams. Use a resource along with the business owners who are responsible for resource costs. The operational side ensures that names and tags include information that IT teams use to identify the workload, application, environment, criticality, and other information useful for managing resources.","title":"4.1 Design Recommendations for Resources"},{"location":"lzdesign/orgmgmt/#tagging-conventions-for-resources","text":"Apply tags to resources, resources groups and subscriptions to logically organize them into a taxonomy. Tags required for billing (Department and Cost Centre) will be enforced by policy at Landing Zone Management Group scope. Below are the mandatory tags required for Kyndryl managed resources Tag Name Description Value(s) Name=Value Examples Mandatory Values? Application Name of the Application the resourxe is part of Any name, can be more than value, comma seperated Application = WVD, Sharepoint, HCM, etc Yes Service Name of the Service the resource is part of Any name, can be more than value, comma seperated Application = Procurement, Employee Verification No Environment Deployment Environment of this application, workload or service Prod/Non-Prod, Environment = Prod, NonProd Yes Criticality Business criticality of the Application Mission Critical, Essential Disaster recovery = Mission Critical, Critical, Essential No Kyndryl Managed To identify if resource is managed by Kyndryl Yes/No KyndrylManaged=Yes/No Yes Operations team Team accountable for day to day operations, can be used to define Ops Console target also IBM, Kyndryl etc. Operationteam=BoulderOps No Ticketing Group Incident Ticket queue against which tickets will be created for this resource IBM, Contoso etc. TicketingGroup=IBM_SMT001 Yes Notification Group Notification Group where alerts will be sent to for this resource Notification Group Name or email IDs Notification Group= Yes Kyndryl Customer Code 3 char Customer Identifier for Kyndryl Management Kyndryl CustomerCode=iga Yes Business unit Top level vision of customer that owns the subscription and workload EAP, IT01, etc Business Unit = EAP No Owner Name Owner of the application, workload or service Email addresses OwnerName= john@dn.ibm.com No More details on tagging can be obtained from the Naming and Tagging Convention template .","title":"Tagging conventions for Resources"},{"location":"lzdesign/orgmgmt/#design-actions-by-solution-architects","text":"Define management group hierarchy, in consulation with client. Identify the appropriate Subscription Model for an account, along with the client IT team. Ensure naming standards are defined & enforced for management groups, subscriptions & resource groups Ensure tagging standards are defined & enforced, and mandatory tags defined to every resource as part of a policy Important note to Kyndryl Architect Though Business Continuity & Disaster Recovery(BCDR) may not be under consideration during the early adoption stage, making the assumption that BCDR will be in scope at some point in future ensures the creation of various elements in the correct region, like Resource Groups, Log Analytics workspaces etc. Azure paired regions must be used to align with Azure BCDR. A separate section related to BCDR will be updated to provide additional guidance","title":"Design Actions by Solution Architects."},{"location":"lzdesign/platformauto/","text":"Platform Automation & DevOps \u2693\ufe0e Azure Automation Key Requirements \u2693\ufe0e For Azure, most of processes can be automated across Ready, Adopt, Govern, and Manage phases of Azure Cloud Adoption Framework. This section provides guidance for Azure Landing Zone deployment, Workload deployment, Govern and Azure Day2 Process automation and recommendations considering various design aspects. In an enterprise environment, various repetitive tasks and processes should be well defined, developed as code and deployed in an automated fashion for better control, scale, and avoiding manual errors. This provides speed, agility and self-healing capabilities to the environment. Various below processes and management services should be automated and leverage automation as much as possible. 1.1 Ready Stage Automation \u2693\ufe0e \u00b7 Subscription Automation (Create, Modify, Delete) \u00b7 Resource Group Automation (Create, Delete) \u00b7 Resource Tagging Automation \u00b7 Azure AD User, Group and Roles Assignment Automation \u00b7 Network Automation \u00b7 Security Automation \u00b7 Monitor and Log Analytics Deployment Automation \u00b7 Landing Zone Deployment Automation 1.2 Adopt Stage Automation \u2693\ufe0e \u00b7 Workload Deployment Automation \u00b7 Make Manage Onboarding Automation \u00b7 Azure Migration Automation \u00b7 Process Improvements and Automations \u00b7 Few Ready stage activities like Resource group automation, tagging, network automation etc. 1.3 Governance Automation \u2693\ufe0e \u00b7 Policy Enforcement and Automation \u00b7 Resource Consistency \u00b7 Security & Compliance Automation \u00b7 Cost & Billing Automation 1.4 Manage Automation \u2693\ufe0e \u00b7 Day2 Process Automation \u00b7 Event Based Orchestration \u00b7 Update Management \u00b7 Configuration Management \u00b7 Reduce manual operational effort 2 Azure Automation Framework \u2693\ufe0e AA - the heading should be Kyndryl Automation Framework for Azure Cloud. The current heading implies Automation as provided by Azure. In an enterprise world, landing zone, workload and Policies should be defined, deployed, and managed in an automated ITIL compliant manner. Automation and its benefits can be realized and implemented by various Kyndryl, Azure and 3 rd party products/services as above, which is the recommended automation framework. It has various components as below: 2.1 Automation Framework Components \u2693\ufe0e 2.1.1 Cloud Management Platform (CMP) \u2693\ufe0e Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and validate various custom/Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities. 2.1.2 Business Approval & IT Service Management \u2693\ufe0e There should be capability to raise Service Request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should also be capability of raising change Requests, which needs infrastructure changes. There should also exist an Enterprise grade Configuration Management Database (CMDB). This should be capable to apply and validate various custom/Azure policies and should allow/deny requests accordingly. AA - you seem to imply CMDB needs to be capable to apply policies and allow/deny requests. Is that correct? 2.1.3 Orchestration Engine \u2693\ufe0e An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of maintaining Process automation for make manage, agents installation, custom steps execution etc, and Day 2 Change Request based service flow execution which can coordinate various tasks. AA - This is confusing. Service flows would be handled by a tool such as ServiceNow while process automation would be handled by a tool such as Ansible. We need clarity on which is indicated in the above. 2.1.4 Request Fulfilment Engine \u2693\ufe0e Request fulfilment Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution. 2.1.5 GitHub \u2693\ufe0e Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well. 2.1.6 Service Management Tools \u2693\ufe0e Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc. 2.1.7 Workload \u2693\ufe0e Workloads are any Azure/On-Prem/Other Cloud systems/services which are provisioned, managed and decommissioned as needed. AA - Workloads are applications or services that support applications and perform tasks for end-users. 2.2 Automation Use Cases \u2693\ufe0e Below are various automation use cases which should be able to be executed in an automated fashion . 2.2.1 Landing Zone Deployment Automation \u2693\ufe0e For Landing Zone deployment, management and configuration in consistent manner, Azure automation should be leveraged. There are various landing Zone activities like accounts creation, tagging, security governance, networking, identity etc which should be fulfilled in an automated fashion. With MCMP/3 rd Party/GitOPS, Automation should be triggered which can provision required Azure services after IPC and perform required tasks and update CIs as needed. AA - I am confused. If Landing Zone refers to the platform part as against the customer workloads, how does MCMP play a part? What is IPC here? 2.2.2 Day 1 Automation \u2693\ufe0e For new or existing client Day1 requests can include various Azure services like Compute (Provision VM, VMSS, App Service, AKS etc) Storage (Create Blob storage, File storage, Managed Disk), Network (Create Virtual Network, Load Balancer, Azure Gateway, VPN Gateway), Data platform (Create Cosmos, SQL, Managed SQL database, Synapse database) etc. Request can be raised from MCMP Portal or triggered through 3 rd party apps /APIs/GitOps, which then go through Service Request Creation, Request Approval and Change Task creation. In background, a sequence of tasks are performed in an automated/semi-automated fashion for request fulfilment. E.g. For Managed VM Provisioning, request is raised in MCMP portal, service request and change are created, and VM provisioning, CI discovery and Make manage tasks (patch, monitor, event management, backup, domain join, antivirus, health check enablement etc.) are executed by request fulfilment engine and notification is sent to the requester once request is completed. AA - My understanding is that once MCMP provisions the VM, it will create asset and config items in CMDB, then invoke mkmanage. All this is asynchronous and user will have to poll the inventory UI for updates. There is no CI discovery. Also, Day 1 (and Day 2) have to do with workloads primarily and not the Platform part of the landing zone. 2.2.3 Day 2 Automation \u2693\ufe0e For Day2, requests may include remediate configuration deviation, adding/modifying security and backup policies, modifying various Azure services like Compute (start, stop, restart, add disk, resize VM, enable monitoring/backup/patching, on demand backup etc.), Storage (Modify blob storage, change backup frequency, change storage type etc), Network (Modify VNet, add/update NSG rules etc), Data platform (Database optimization, enable backup etc) etc. Request can be raised from MCMP Portal or triggered through 3 rd party apps /APIs/GitOps, which then go through Service Request Creation, Request Approval and Change Task creation. In background, task or a sequence of tasks are performed in an automated/semi-automated fashion for request fulfilment and CI is updated if there is any CI change. E.g. For Resize VM Day2, request is raised in MCMP portal, service request and change are created, and VM resize is triggered and CI update is with required dependent tasks execution by request fulfilment engine and notification is sent to the requester once request is completed. 3 Azure Automation Options \u2693\ufe0e Azure provides ways to create, configure and manage various Azure services via Azure Portal, REST API, CLI and PowerShell. There are various Hybrid tools, Azure services and automation options as below which can be leveraged for automation. 3.1 Infrastructure as Code (IaC) \u2693\ufe0e IaC is about defining and maintaining infrastructure (Virtual Machines, Network, Storage, Load Balancer etc.) in descriptive model, which can be maintained and versioned by DevOps team as source code. IaC usually contains input configuration file (target state), variables and logic (resource configuration) for resource deployment based on the given configuration. Azure provides native support for IaC via PowerShell and Azure Resource Manager (ARM), and this is also supported by popular third party platforms, such as Terraform, Ansible, and Chef to deploy and manage infrastructure in an automated fashion. 3.2 Azure Policies \u2693\ufe0e Azure Policy provides alerting, denial, and remediation capabilities for the Azure resources for a chosen Azure scope. It helps to evaluate overall state of the environment, drill down to each policy/resource, enforce organizational standards, assess compliance at-scale and bulk remediation for existing resources and automatic remediation for new resources. Common use cases for Azure Policy include implementing governance for resource consistency, regulatory compliance, security, cost visibility etc. Azure policies can be grouped together with Azure Initiatives for easy apply and controls. A new Azure policy or initiative assignment takes about 30 minutes to be applied while New or updated resources within scope of an existing assignment become available in about 15 minutes. A standard compliance scan occurs every 24 hours. For Kyndryl recommended Azure built-in & custom policies, and Azure Initiatives please refer link . 3.3 Azure Blueprint \u2693\ufe0e Azure Blueprint are reusable blueprints consisting of packages of key environment artifacts like Azure Resource Manager templates, role-based access controls and policies simplifying largescale Azure deployments. These templates can be maintained at centralized place with version controlled and can be leveraged for deployment to multiple subscriptions with single click. For deploying the chosen Azure Blueprint, below steps should be followed: \u00b7 Create a new blueprint from the sample/scratch \u00b7 Mark the blueprint as Published \u00b7 Assign the blueprint to an existing subscription 3.4 Azure Automation \u2693\ufe0e Azure Automation is a cloud-based automation and configuration service which can be leveraged for Azure and non-Azure environments. With Azure Automation, runbooks can be authored graphically, in PowerShell, or using Python and can be used for process automation, configuration management, update management, shared capabilities, and heterogeneous features and can be used during deployment, operations, and decommissioning of workloads and resources. Azure automation has changed over time from classic, Azure Service Management (ASM) model to recent, Azure Resource Manager (ARM) model. ARM templates additionally provides resource groups, role-based access control, template deployments, tagging, resource policy etc. capabilities. Along with Azure Graphical runbooks option, ARM templates provides capability to create, maintain and deploy JSON based templates in version control manner. Templates helps in deploying resources consistently and repeatedly. Azure Automation needs an automation account which can integrate with Operations Management Suite (OMS), and the solutions connected to it. Alternately, webhooks can be created for runbooks and can be executed based on OMS search criteria. Azure Automation can also execute runbooks in on-premises environment through on-prem Azure automation hybrid workers, connected with the Azure Automation account. 3.5 ARM Templates \u2693\ufe0e Azure Resource Manager ( ARM ) Templates are Azure native configuration language to define Infrastructure as Code (IaC) for various Azure resources. With ARM templates, Azure resources can be centrally created and updated in declarative and repeatable fashion. Either already available sample templates can be leveraged or new can be written using native tooling in Visual Studio or Visual Studio Code. These have native integration with other Azure resources like Azure Policy to remediate non-compliant resources and Azure DevOps for CI/CD. With ARM, resources can be deployed in parallel to speed up the overall deployment process. 3.6 PowerShell \u2693\ufe0e Azure PowerShell is a set of commandlets for managing Azure resources directly from the PowerShell command line and provides powerful features for automation. There are number of readily available PowerShell modules as well which can be imported into the Automation account to run the runbooks from PowerShell Module Gallery. If runbook uses any of these commands, the respective modules should be imported to the Automation account before executing the runbook The runbooks in Azure automation are completely based on PowerShell. There are below four types of runbooks available: \u00b7 PowerShell \u2013 PowerShell based runbooks are available in Azure Automation which can do basic operations. These are similar as executing Azure PowerShell module-based commands from the Azure portal. \u00b7 PowerShell Workflow - PowerShell Workflow can be used for advanced automations as it can execute more-complex tasks that involve executing steps in parallel, calling other child runbooks, and so forth. It in turn uses Windows Workflow Foundation and allows to set checkpoints in script so that script can be restarted from the checkpoint if an exception occurs during execution. \u00b7 Graphical : Graphical runbooks can be created only from Azure Portal and is good for administrators with no/less PowerShell knowledge. This type of runbook uses a visual authoring model and represents the data flow pictorially in an easy-to-understand fashion. \u00b7 Graphical PowerShell Workflow : Graphical PowerShell Workflow runbooks are based on PowerShell workflows in the back end and can be created and edited only from Azure Portal. Though based on PowerShell, each runbook type has its own features and limitations. Nested run books can be executed. 3.7 Terraform \u2693\ufe0e Terraform is an open-source tool, and provides configuration as code for cloud infrastructure provisioning and management. The Terraform CLI provides a simple mechanism to deploy and version the configuration files to Azure. Common Azure compute, storage, database and network activities can be performed using Terraform templates. 3.8 Runbooks \u2693\ufe0e Azure Runbooks are the basic building blocks of Azure Automation and can be created from scratch, or can be imported from Runbook Gallery which are published by Microsoft or community contributors. These runbooks can be customized and scheduled as needed. Few examples of common runbooks are start/stop all Azure VMs or tagged VMs, turnOn Update Management, backup SQL db to Azure Blob, Send Mail, Backup reports etc. Runbooks can be executed manually or in a scheduled fashion, that results in one or multiple jobs which can run in parallel. An Azure automation worker executes the job(s). Each Job can be managed and drilled down deeper and Job\u2019s input, output and task information can be tracked. Jobs can have status as Completed, Failed, Queued, Running, Stopped, and Suspended. If a runbook is interrupted, it restarts at the beginning. 3.9 Desired State Configuration (DSC) \u2693\ufe0e Azure Desired State Configuration ( DSC ) is an configuration management solution, that helps in maintaining infrastructure configuration as code. DSC is a feature in PowerShell 4.0 and above that helps administrators to automate the configuration of Windows and Linux operating systems. It uses PowerShell and implements the desired state in target machines by leveraging the Local Configuration Manager (LCM). Azure Automation DSC integrates Azure Automation with DSC-based configuration management and can be used to maintain desired state across on-premises physical/virtual machines as well as cloud resources. 3.10 Azure Update Management \u2693\ufe0e Update Management is a configuration component of Azure Automation. Windows and Linux computers, both in Azure and on-premises, send assessment information about missing updates to the Log Analytics workspace. Azure Automation then uses that information to create a schedule for automatic deployment of the missing updates. The following steps needs to be performed for enabling Update Manager: \u00b7 Leverage existing /Create a Log Analytics workspace \u00b7 Create an Automation account \u00b7 Link the Automation account with the Log Analytics workspace \u00b7 Enable Update Management for Azure VMs \u00b7 Enable Update Management for non-Azure VMs Azure Maintenance Configuration can manage platform updates that don\u2019t require a reboot using maintenance control. Maintenance control lets client decide when to apply updates to their virtual infrastructure. AA - Update Management is a service that leverages Azure Automation. So discussing Update Management is out of place here. 3.11 Azure Automanage \u2693\ufe0e Azure provides Azure Automanage feature for Linux and Windows Servers Day2 tasks management in dev/test and Production category based on Azure best practices. This feature is currently in preview. Various Day2 common tasks are as below: \u00b7 Machine Insights Monitoring (Production only) \u00b7 Backup (Production only) \u00b7 Azure Security Center \u00b7 Microsoft Antimalware \u00b7 Update Management \u00b7 Change Tracking and Inventory \u00b7 Guest Configuration \u00b7 Boot Diagnostics \u00b7 Windows Admin Center \u00b7 Azure Automation Account \u00b7 Log Analytics Workspace Built-in Azure Automanage enrols, configures, and monitors virtual machines with best practice as defined in the Microsoft Cloud Adoption Framework for Azure. Below are few current limitations for same. \u00b7 Automanage can be used for the selected scope. \u00b7 By default, this assignment takes effect on newly created resources. \u00b7 Existing resources can be updated via a remediation task after the policy is assigned. For deployIfNotExists policies, the remediation task deploys the specified template. \u00b7 For modify policies, the remediation task edits tags on the existing resources. \u00b7 Upto 500 non-complaint resources can be made complaint 3.12 Ansible & CACF \u2693\ufe0e Ansible is an open source provisioning and configuration management tool and can automate complex applications and provision resources in multiple clouds. Ansible provides collection of Az modules for various Azure activities automation. Ansible Tower Based, Cloud Automation Community Framework ( CACF ) is Kyndryl\u2019s strategic solution that allows automation at enterprise level with a target to get to zero touch with reduced development and release cycle addressed by the community model. CACF leverages cloud native RedHat Automation platform \u2013 Ansible Tower on OpenShift and integrates with Services management processes, the Git repository and the automation playbooks. There is a rich set of are pre-built automations available using Ansible playbooks. Major CACF addressed use cases (on-prem) for existing clients are as below: \u00b7 Event/Incident remediation \u00b7 Patch Scanning and execution \u00b7 Security Health Check \u00b7 Service Request \u00b7 Build and Decommission \u00b7 Other Automation use cases AA - How is a discussion of on-prem use cases relevant here? 3.13 Other Services \u2693\ufe0e Azure provides various other services to manage and control Azure environment in automated fashion as below: \u00b7 Azure Backup for protecting data and backing up entire Windows/Linux VMs using backup extensions and backing up files, folders, and system state using the MARS agent \u00b7 Azure Security Center with Azure Defender for monitoring workloads and finding and fixing vulnerabilities to protect hybrid cloud workloads \u00b7 Azure Advisor analyses Azure services configurations and usage telemetry and offers personalized, actionable recommendations for reliability, security, operational aspects, performance, and cost. This also includes suggested actions that can be taken right away, postpone or dismiss. Advisor Quick Fix makes optimisation at scale faster and easier by allowing users to remediate recommendations for multiple resources simultaneously and with only a few clicks. Same can be accessed by Azure Portal, REST API, CLI and PowerShell. AA - So far as I know, the last two are not examples of Automation. Also Backup is a service that leverages Automation. 4 Automation Recommendation \u2693\ufe0e Kyndryl has already embraced automation as the foundation for every aspect of Infrastructure Services delivery from Landing Zone build, day one infrastructure & application provisioning, day two management of infrastructure & applications which include security compliance, identity & access management, monitoring, logging etc. This automation approach will be further strengthened as part of Kyndryl and client\u2019s cloud adoption journey, using Infrastructure as Code (IaC) model for all Azure operations. Kyndryl will follow its established DevSecOps process for automation content creation, and for deployment wherever applicable. Enabling DevSecOps process and culture for clients is not part of this design scope and should be managed through the DevSecOps offerings. Based on Kyndryl\u2019s and products current capabilities, below is the recommendation for various automations. Note: Recommendations are based on best effort basis and mentioned integration between components/offerings and automation templates/playbooks may not exist/production grade validated and should to be consulted with offering teams before committing to the clients. 4.1 Ready Stage Automation Recommendation \u2693\ufe0e For fully governed landing zone deployment, modify out of the box or build new Azure Blueprints to set up ISO-compliant foundation templates that will accelerate the landing zone build and workload migration. Landing Zone automation will be used to create Landing Zone elements as per recommendations and standards defined in the previous seven sections. Kyndryl proposes ARM template and Terraform templates based automation solution for various CAF Ready stage automation activities for Landing Zone Deployment like Azure managements groups & resource groups creation and deletion, Resource Tagging, Azure AD user & group creation/deletion/modifications, roles Assignment, creation of backup, infrastructure and security policies as per baseline defined in the previous sections, creation of log analytics workspaces and Azure Monitor configuration, creation of VNets, Subnets and all other network elements, peering the workload VNet with Hub network for on-premises integration and connectivity etc. These terraform and ARM templates will be secured and version controlled in Kyndryl\u2019s GitHub and/or client\u2019s GitHub if requested by a client. For Landing Zone Deployments, there are various out of box readily available implementation options available using Azure Blueprints which will be used as base and Kyndryl specific customizations and enhancements will be built, for the initial Landing Zone. Note that the landing zone template will include common platform elements and platform elements specific to workloads like VNets, Subnets, policies etc. and hence these ARM templates need to be additionally customized for each specific client & workload. The first landing zone will include common platform elements, whereas subsequent landing zones for new workloads will contain only workload specific elements. Various automation activities and consistency can also be achieved with pre-defined Azure Policies. Various Landing Zone deployment automation policies are defined in Azure Policies section. There are certain elements of landing zone creation that cannot be automated, which include configuring Active Directory elements, creation of AD groups and client network connectivity aspects and should be performed manually. Kyndryl offerings are working on developing/enhancing ARM templates for Landing Zone automated deployment to define workload and application landscape which will be taken as the input for landing zone creation. Refer to additional documentation on Kyndryl\u2019s Landing Zone Automation design approach and capabilities. 4.2 Adopt Stage Automation Recommendation \u2693\ufe0e For Workload deployment and configuration management automation, Azure ARM templates, and Terraform templates should be used to automate the deployment and configuration of workloads such as VMs, Virtual Machines Scale Sets, SQL databases, AKS etc. These terraform and ARM templates should be secured and version controlled in Kyndryl\u2019s GitHub and/or in customer\u2019s GitHub if requested by a client. These Azure Resources/Composite patterns can be called from CMP, like MCMP where these ARM templates/Composite patterns are provided by Kyndryl offerings like MCDS or SO/ Client IT teams and execution happen in automated/semi-automated fashion along with Make Mange processes. These ARM/Terraform templates can also be triggered using GitOps or third party tools which support REST APIs to have consistent resource deployment. CACF based Ansible Tower automation should be leveraged for make manage onboarding automation for installing required service management agents, custom scripts and software, evidence capture, CI discovery, setting up of monitoring for application and the infrastructure, backup configuration and integration with backend systems. 4.3 Governance Automation Recommendation \u2693\ufe0e While deploying resources, the Cloud Governance team will work with the Operations team to identify business risks and establish a baseline set of policy statements that are intended to mitigate that risk. These policies will be implemented at the outset, then optimized over time based on regular reviews of operations. Recommended Azure Policies can be referred from Policies section. Azure Built in/custom policies/initiatives should be leveraged to evaluate Azure resource at specific intervals and perform deny the change, log the change or remediate non-compliant resources for resource consistency. Azure policies/initiative JSON consisting policy parameters, policy rules, definitions, and policy effects can be developed or modified using Azure Portal, CLI or PowerShell modules and can be applied through Azure Portal for given Management Group, Subscription or Resource Group Scope and optional Resource exclusions. Azure Security Center and Azure Sentinel should be used for assessing compliance and identifying security risks and for detection, response and recovery from threats. Azure Security policies (e.g. CIS Policy) will be configured for detecting and enforcing organization-wide settings to ensure consistent policy adherence and fast violation detection. For centralized cost governance, all in-scope workloads and resources should be properly tagged using Azure Policy. Azure Hybrid Benefit and OS, SQL licencing reuse can be leveraged for further cost optimization. On ongoing basis, resource utilization and performance should be evaluated across the environment and resources will be modified to use the smallest instance or SKU that can support the performance requirements of each resource. Azure policies should be used to identify and terminate any resources that are adding to costs but are not adding to business value. Azure Advisor recommendations along with remediation actions will be leveraged using Azure Portal and REST API. 4.4 Manage Automation Recommendation \u2693\ufe0e Automating Day2 tasks like monitoring, backup, patch management, process automation, configuration management etc. helps administrators to improve efficiency and reduce effort and manual error. MCMS currently leverages Azure Monitoring (In most of cases), and Cloud Native Backup while other processes are executed by CACF Ansible and local automation. Below Cloud Native or CACF Ansible can be proposed for automating these processes. 4.4.1 Azure Automation \u2693\ufe0e Various Azure native solutions, Azure Automation should be leveraged for Azure and non-Azure workloads operations, and decommissioning for various below processes: 4.4.1.1 Process Automation \u2693\ufe0e Process Automation in Azure Automation can be used for frequent, time-consuming, and error-prone cloud management tasks and orchestrating service provider or client processes across Hybrid environments. Orchestration should be written using Logic Apps while execution can be achieved using PowerShell, PowerShell Workflow, and graphical runbooks. Logic can be defined/modified through Azure Automation runbooks . 4.4.1.2 Configuration Management \u2693\ufe0e Azure automation can be leveraged for collecting inventory (Virtual Machines and Server infrastructure) and tracking changes across services, daemons, software, registry, and files and raising alerts. This also supports query in-guest resources for visibility into installed applications and other configuration items. With Azure Automation State Configuration PowerShell Desired state configuration (DSC) for DSC resources and apply configurations to virtual or physical machines from a DSC pull server in the Azure cloud. 4.4.1.3 Update Management \u2693\ufe0e Azure Update Management should be leveraged for patching Windows and Linux VMs Cloud Natively. 4.4.1.4 Azure Backup \u2693\ufe0e Azure Backup service should be leveraged for Azure native Backup solution for Linux and Windows VM in Azure Recovery vaults. 4.4.2 Ansible Automation (CACF) \u2693\ufe0e Kyndryl will leverage it available automation for Day2 tasks automation such as security compliance, patching, identity & access management, monitoring etc. Ansible playbooks will be used for operational efficiency and process automation, automated incident remediations, configuration management, update management, task automation, event automation. Day 2 operations will be automated through ITSM tool like ServiceNow and MCMP (Multi-Cloud Management Platform) which are integrated with Azure through Azure native APIs and Ansible Tower. Client can open service requests through pre-defined service Catalogs in ServiceNow/MCMP. Workflows within these tools are used to validate the request after which automations are initiated through Ansible Tower & Terraform templates. The automation assets \u2013 Ansible playbooks or ARM templates etc, will be created and managed through DevSecOps process and all assets will be managed in GitHub. Currently, DevSecOps is limited to application deployments and upgrades only, on the workload subscriptions and Day2 automations are provided as part of manage services from MCMS & ISPW. The intention is to allow application developers to manage their application deployments and upgrades through DevSecOps process. This DevSecOps platform uses a combination of standard DevOps tool like Jenkins, Azure native DevOps tools and Ansible Tower and can be hosted at Kyndryl\u2019s management network or in client premises, or re-use an existing DevSecOps platform from the client with appropriate changes. Application developers will be able to manage their application deployments and upgrades through DevSecOps process. This DevSecOps platform uses a combination of standard DevOps tool like Jenkins, Azure native DevOps tools and Ansible Tower and will be hosted at Kyndryl\u2019s management network. Design Actions by Solution Architects \u2693\ufe0e o Landing Zone creation will be automated based on all the design defintions from previous sections. o There will be templates provided in a future release to capture landing zone definitions which can be directly used by automation AA - We need to emphasize the following - Solution Architects will be expected to create automation templates (ARM templates, blueprints, Terraform configs or similar artifacts) based on sample templates that will be provided as these design recommendations evolve. The Kyndryl Automation Framework consisting of all the elements described in Section 2.1 are work in progress. The intent for this framework is to implement the use cases in Section 2.2. As and when there is definite progress in this space, the documentation will be updated. Section 3 is mainly information that Solution Architects can use to create custom automization. Section 4 are aids to Solution Architects in creation of their own automation templates. The sub-sections on Manage and Govern Automation Recommendations should not apply to Solution Architects. They could be there to provide insight into how automation in these phases is provided using Azure tooling. Overall, there are numerous language corrections to be made and also more concise and abbreviated wording to be used.","title":"8.Platform Automation"},{"location":"lzdesign/platformauto/#platform-automation-devops","text":"","title":"Platform Automation &amp; DevOps"},{"location":"lzdesign/platformauto/#azure-automation-key-requirements","text":"For Azure, most of processes can be automated across Ready, Adopt, Govern, and Manage phases of Azure Cloud Adoption Framework. This section provides guidance for Azure Landing Zone deployment, Workload deployment, Govern and Azure Day2 Process automation and recommendations considering various design aspects. In an enterprise environment, various repetitive tasks and processes should be well defined, developed as code and deployed in an automated fashion for better control, scale, and avoiding manual errors. This provides speed, agility and self-healing capabilities to the environment. Various below processes and management services should be automated and leverage automation as much as possible.","title":"Azure Automation Key Requirements"},{"location":"lzdesign/platformauto/#11-ready-stage-automation","text":"\u00b7 Subscription Automation (Create, Modify, Delete) \u00b7 Resource Group Automation (Create, Delete) \u00b7 Resource Tagging Automation \u00b7 Azure AD User, Group and Roles Assignment Automation \u00b7 Network Automation \u00b7 Security Automation \u00b7 Monitor and Log Analytics Deployment Automation \u00b7 Landing Zone Deployment Automation","title":"1.1  Ready Stage Automation"},{"location":"lzdesign/platformauto/#12-adopt-stage-automation","text":"\u00b7 Workload Deployment Automation \u00b7 Make Manage Onboarding Automation \u00b7 Azure Migration Automation \u00b7 Process Improvements and Automations \u00b7 Few Ready stage activities like Resource group automation, tagging, network automation etc.","title":"1.2  Adopt Stage Automation"},{"location":"lzdesign/platformauto/#13-governance-automation","text":"\u00b7 Policy Enforcement and Automation \u00b7 Resource Consistency \u00b7 Security & Compliance Automation \u00b7 Cost & Billing Automation","title":"1.3  Governance Automation"},{"location":"lzdesign/platformauto/#14-manage-automation","text":"\u00b7 Day2 Process Automation \u00b7 Event Based Orchestration \u00b7 Update Management \u00b7 Configuration Management \u00b7 Reduce manual operational effort","title":"1.4  Manage Automation"},{"location":"lzdesign/platformauto/#2-azure-automation-framework","text":"AA - the heading should be Kyndryl Automation Framework for Azure Cloud. The current heading implies Automation as provided by Azure. In an enterprise world, landing zone, workload and Policies should be defined, deployed, and managed in an automated ITIL compliant manner. Automation and its benefits can be realized and implemented by various Kyndryl, Azure and 3 rd party products/services as above, which is the recommended automation framework. It has various components as below:","title":"2  Azure Automation Framework"},{"location":"lzdesign/platformauto/#21-automation-framework-components","text":"","title":"2.1  Automation Framework Components"},{"location":"lzdesign/platformauto/#211-cloud-management-platform-cmp","text":"Cloud Management Platform/ Hybrid Cloud Management platform should provide centralized Dashboard and Service Catalog for raising and tracking various Day1 and Day2 requests. This should be capable to apply and validate various custom/Azure policies and should allow/deny requests accordingly. CMP should be customizable, providing authentication, reporting, analytics, alerting, notification, Configuration Item (CI) Discovery etc capabilities.","title":"2.1.1   Cloud Management Platform (CMP)"},{"location":"lzdesign/platformauto/#212-business-approval-it-service-management","text":"There should be capability to raise Service Request and obtain technical and business approvals for the requests raised from Cloud Management Platform, CLI or API Calls. There should also be capability of raising change Requests, which needs infrastructure changes. There should also exist an Enterprise grade Configuration Management Database (CMDB). This should be capable to apply and validate various custom/Azure policies and should allow/deny requests accordingly. AA - you seem to imply CMDB needs to be capable to apply policies and allow/deny requests. Is that correct?","title":"2.1.2   Business Approval &amp; IT Service Management"},{"location":"lzdesign/platformauto/#213-orchestration-engine","text":"An enterprise automation system should have a Workflow or an Orchestration Engine, which is capable of maintaining Process automation for make manage, agents installation, custom steps execution etc, and Day 2 Change Request based service flow execution which can coordinate various tasks. AA - This is confusing. Service flows would be handled by a tool such as ServiceNow while process automation would be handled by a tool such as Ansible. We need clarity on which is indicated in the above.","title":"2.1.3   Orchestration Engine"},{"location":"lzdesign/platformauto/#214-request-fulfilment-engine","text":"Request fulfilment Engine is the execution engine, which is responsible to execute sequence of steps on target endpoints and can interact with various service management tools for various configurations and operations execution.","title":"2.1.4   Request Fulfilment Engine"},{"location":"lzdesign/platformauto/#215-github","text":"Kyndryl or Client GitHub will be used to maintain various artifacts as Infrastructure as code including policies, ARM templates, resource templates, action definitions, playbooks, Job Templates etc. in version controlled manner and can be used with Azure DevOps as well.","title":"2.1.5   GitHub"},{"location":"lzdesign/platformauto/#216-service-management-tools","text":"Various Cloud Native or Custom tools will be leveraged for various Service Management capabilities like monitoring, log analytics, patch management, Backup and Disaster Recovery, Security and Compliance Management etc.","title":"2.1.6   Service Management Tools"},{"location":"lzdesign/platformauto/#217-workload","text":"Workloads are any Azure/On-Prem/Other Cloud systems/services which are provisioned, managed and decommissioned as needed. AA - Workloads are applications or services that support applications and perform tasks for end-users.","title":"2.1.7   Workload"},{"location":"lzdesign/platformauto/#22-automation-use-cases","text":"Below are various automation use cases which should be able to be executed in an automated fashion .","title":"2.2  Automation Use Cases"},{"location":"lzdesign/platformauto/#221-landing-zone-deployment-automation","text":"For Landing Zone deployment, management and configuration in consistent manner, Azure automation should be leveraged. There are various landing Zone activities like accounts creation, tagging, security governance, networking, identity etc which should be fulfilled in an automated fashion. With MCMP/3 rd Party/GitOPS, Automation should be triggered which can provision required Azure services after IPC and perform required tasks and update CIs as needed. AA - I am confused. If Landing Zone refers to the platform part as against the customer workloads, how does MCMP play a part? What is IPC here?","title":"2.2.1   Landing Zone Deployment Automation"},{"location":"lzdesign/platformauto/#222-day-1-automation","text":"For new or existing client Day1 requests can include various Azure services like Compute (Provision VM, VMSS, App Service, AKS etc) Storage (Create Blob storage, File storage, Managed Disk), Network (Create Virtual Network, Load Balancer, Azure Gateway, VPN Gateway), Data platform (Create Cosmos, SQL, Managed SQL database, Synapse database) etc. Request can be raised from MCMP Portal or triggered through 3 rd party apps /APIs/GitOps, which then go through Service Request Creation, Request Approval and Change Task creation. In background, a sequence of tasks are performed in an automated/semi-automated fashion for request fulfilment. E.g. For Managed VM Provisioning, request is raised in MCMP portal, service request and change are created, and VM provisioning, CI discovery and Make manage tasks (patch, monitor, event management, backup, domain join, antivirus, health check enablement etc.) are executed by request fulfilment engine and notification is sent to the requester once request is completed. AA - My understanding is that once MCMP provisions the VM, it will create asset and config items in CMDB, then invoke mkmanage. All this is asynchronous and user will have to poll the inventory UI for updates. There is no CI discovery. Also, Day 1 (and Day 2) have to do with workloads primarily and not the Platform part of the landing zone.","title":"2.2.2   Day 1 Automation"},{"location":"lzdesign/platformauto/#223-day-2-automation","text":"For Day2, requests may include remediate configuration deviation, adding/modifying security and backup policies, modifying various Azure services like Compute (start, stop, restart, add disk, resize VM, enable monitoring/backup/patching, on demand backup etc.), Storage (Modify blob storage, change backup frequency, change storage type etc), Network (Modify VNet, add/update NSG rules etc), Data platform (Database optimization, enable backup etc) etc. Request can be raised from MCMP Portal or triggered through 3 rd party apps /APIs/GitOps, which then go through Service Request Creation, Request Approval and Change Task creation. In background, task or a sequence of tasks are performed in an automated/semi-automated fashion for request fulfilment and CI is updated if there is any CI change. E.g. For Resize VM Day2, request is raised in MCMP portal, service request and change are created, and VM resize is triggered and CI update is with required dependent tasks execution by request fulfilment engine and notification is sent to the requester once request is completed.","title":"2.2.3   Day 2 Automation"},{"location":"lzdesign/platformauto/#3-azure-automation-options","text":"Azure provides ways to create, configure and manage various Azure services via Azure Portal, REST API, CLI and PowerShell. There are various Hybrid tools, Azure services and automation options as below which can be leveraged for automation.","title":"3  Azure Automation Options"},{"location":"lzdesign/platformauto/#31-infrastructure-as-code-iac","text":"IaC is about defining and maintaining infrastructure (Virtual Machines, Network, Storage, Load Balancer etc.) in descriptive model, which can be maintained and versioned by DevOps team as source code. IaC usually contains input configuration file (target state), variables and logic (resource configuration) for resource deployment based on the given configuration. Azure provides native support for IaC via PowerShell and Azure Resource Manager (ARM), and this is also supported by popular third party platforms, such as Terraform, Ansible, and Chef to deploy and manage infrastructure in an automated fashion.","title":"3.1  Infrastructure as Code (IaC)"},{"location":"lzdesign/platformauto/#32-azure-policies","text":"Azure Policy provides alerting, denial, and remediation capabilities for the Azure resources for a chosen Azure scope. It helps to evaluate overall state of the environment, drill down to each policy/resource, enforce organizational standards, assess compliance at-scale and bulk remediation for existing resources and automatic remediation for new resources. Common use cases for Azure Policy include implementing governance for resource consistency, regulatory compliance, security, cost visibility etc. Azure policies can be grouped together with Azure Initiatives for easy apply and controls. A new Azure policy or initiative assignment takes about 30 minutes to be applied while New or updated resources within scope of an existing assignment become available in about 15 minutes. A standard compliance scan occurs every 24 hours. For Kyndryl recommended Azure built-in & custom policies, and Azure Initiatives please refer link .","title":"3.2  Azure Policies"},{"location":"lzdesign/platformauto/#33-azure-blueprint","text":"Azure Blueprint are reusable blueprints consisting of packages of key environment artifacts like Azure Resource Manager templates, role-based access controls and policies simplifying largescale Azure deployments. These templates can be maintained at centralized place with version controlled and can be leveraged for deployment to multiple subscriptions with single click. For deploying the chosen Azure Blueprint, below steps should be followed: \u00b7 Create a new blueprint from the sample/scratch \u00b7 Mark the blueprint as Published \u00b7 Assign the blueprint to an existing subscription","title":"3.3  Azure Blueprint"},{"location":"lzdesign/platformauto/#34-azure-automation","text":"Azure Automation is a cloud-based automation and configuration service which can be leveraged for Azure and non-Azure environments. With Azure Automation, runbooks can be authored graphically, in PowerShell, or using Python and can be used for process automation, configuration management, update management, shared capabilities, and heterogeneous features and can be used during deployment, operations, and decommissioning of workloads and resources. Azure automation has changed over time from classic, Azure Service Management (ASM) model to recent, Azure Resource Manager (ARM) model. ARM templates additionally provides resource groups, role-based access control, template deployments, tagging, resource policy etc. capabilities. Along with Azure Graphical runbooks option, ARM templates provides capability to create, maintain and deploy JSON based templates in version control manner. Templates helps in deploying resources consistently and repeatedly. Azure Automation needs an automation account which can integrate with Operations Management Suite (OMS), and the solutions connected to it. Alternately, webhooks can be created for runbooks and can be executed based on OMS search criteria. Azure Automation can also execute runbooks in on-premises environment through on-prem Azure automation hybrid workers, connected with the Azure Automation account.","title":"3.4  Azure Automation"},{"location":"lzdesign/platformauto/#35-arm-templates","text":"Azure Resource Manager ( ARM ) Templates are Azure native configuration language to define Infrastructure as Code (IaC) for various Azure resources. With ARM templates, Azure resources can be centrally created and updated in declarative and repeatable fashion. Either already available sample templates can be leveraged or new can be written using native tooling in Visual Studio or Visual Studio Code. These have native integration with other Azure resources like Azure Policy to remediate non-compliant resources and Azure DevOps for CI/CD. With ARM, resources can be deployed in parallel to speed up the overall deployment process.","title":"3.5  ARM Templates"},{"location":"lzdesign/platformauto/#36-powershell","text":"Azure PowerShell is a set of commandlets for managing Azure resources directly from the PowerShell command line and provides powerful features for automation. There are number of readily available PowerShell modules as well which can be imported into the Automation account to run the runbooks from PowerShell Module Gallery. If runbook uses any of these commands, the respective modules should be imported to the Automation account before executing the runbook The runbooks in Azure automation are completely based on PowerShell. There are below four types of runbooks available: \u00b7 PowerShell \u2013 PowerShell based runbooks are available in Azure Automation which can do basic operations. These are similar as executing Azure PowerShell module-based commands from the Azure portal. \u00b7 PowerShell Workflow - PowerShell Workflow can be used for advanced automations as it can execute more-complex tasks that involve executing steps in parallel, calling other child runbooks, and so forth. It in turn uses Windows Workflow Foundation and allows to set checkpoints in script so that script can be restarted from the checkpoint if an exception occurs during execution. \u00b7 Graphical : Graphical runbooks can be created only from Azure Portal and is good for administrators with no/less PowerShell knowledge. This type of runbook uses a visual authoring model and represents the data flow pictorially in an easy-to-understand fashion. \u00b7 Graphical PowerShell Workflow : Graphical PowerShell Workflow runbooks are based on PowerShell workflows in the back end and can be created and edited only from Azure Portal. Though based on PowerShell, each runbook type has its own features and limitations. Nested run books can be executed.","title":"3.6  PowerShell"},{"location":"lzdesign/platformauto/#37-terraform","text":"Terraform is an open-source tool, and provides configuration as code for cloud infrastructure provisioning and management. The Terraform CLI provides a simple mechanism to deploy and version the configuration files to Azure. Common Azure compute, storage, database and network activities can be performed using Terraform templates.","title":"3.7  Terraform"},{"location":"lzdesign/platformauto/#38-runbooks","text":"Azure Runbooks are the basic building blocks of Azure Automation and can be created from scratch, or can be imported from Runbook Gallery which are published by Microsoft or community contributors. These runbooks can be customized and scheduled as needed. Few examples of common runbooks are start/stop all Azure VMs or tagged VMs, turnOn Update Management, backup SQL db to Azure Blob, Send Mail, Backup reports etc. Runbooks can be executed manually or in a scheduled fashion, that results in one or multiple jobs which can run in parallel. An Azure automation worker executes the job(s). Each Job can be managed and drilled down deeper and Job\u2019s input, output and task information can be tracked. Jobs can have status as Completed, Failed, Queued, Running, Stopped, and Suspended. If a runbook is interrupted, it restarts at the beginning.","title":"3.8  Runbooks"},{"location":"lzdesign/platformauto/#39-desired-state-configuration-dsc","text":"Azure Desired State Configuration ( DSC ) is an configuration management solution, that helps in maintaining infrastructure configuration as code. DSC is a feature in PowerShell 4.0 and above that helps administrators to automate the configuration of Windows and Linux operating systems. It uses PowerShell and implements the desired state in target machines by leveraging the Local Configuration Manager (LCM). Azure Automation DSC integrates Azure Automation with DSC-based configuration management and can be used to maintain desired state across on-premises physical/virtual machines as well as cloud resources.","title":"3.9  Desired State Configuration (DSC)"},{"location":"lzdesign/platformauto/#310-azure-update-management","text":"Update Management is a configuration component of Azure Automation. Windows and Linux computers, both in Azure and on-premises, send assessment information about missing updates to the Log Analytics workspace. Azure Automation then uses that information to create a schedule for automatic deployment of the missing updates. The following steps needs to be performed for enabling Update Manager: \u00b7 Leverage existing /Create a Log Analytics workspace \u00b7 Create an Automation account \u00b7 Link the Automation account with the Log Analytics workspace \u00b7 Enable Update Management for Azure VMs \u00b7 Enable Update Management for non-Azure VMs Azure Maintenance Configuration can manage platform updates that don\u2019t require a reboot using maintenance control. Maintenance control lets client decide when to apply updates to their virtual infrastructure. AA - Update Management is a service that leverages Azure Automation. So discussing Update Management is out of place here.","title":"3.10   Azure Update Management"},{"location":"lzdesign/platformauto/#311-azure-automanage","text":"Azure provides Azure Automanage feature for Linux and Windows Servers Day2 tasks management in dev/test and Production category based on Azure best practices. This feature is currently in preview. Various Day2 common tasks are as below: \u00b7 Machine Insights Monitoring (Production only) \u00b7 Backup (Production only) \u00b7 Azure Security Center \u00b7 Microsoft Antimalware \u00b7 Update Management \u00b7 Change Tracking and Inventory \u00b7 Guest Configuration \u00b7 Boot Diagnostics \u00b7 Windows Admin Center \u00b7 Azure Automation Account \u00b7 Log Analytics Workspace Built-in Azure Automanage enrols, configures, and monitors virtual machines with best practice as defined in the Microsoft Cloud Adoption Framework for Azure. Below are few current limitations for same. \u00b7 Automanage can be used for the selected scope. \u00b7 By default, this assignment takes effect on newly created resources. \u00b7 Existing resources can be updated via a remediation task after the policy is assigned. For deployIfNotExists policies, the remediation task deploys the specified template. \u00b7 For modify policies, the remediation task edits tags on the existing resources. \u00b7 Upto 500 non-complaint resources can be made complaint","title":"3.11   Azure Automanage"},{"location":"lzdesign/platformauto/#312-ansible-cacf","text":"Ansible is an open source provisioning and configuration management tool and can automate complex applications and provision resources in multiple clouds. Ansible provides collection of Az modules for various Azure activities automation. Ansible Tower Based, Cloud Automation Community Framework ( CACF ) is Kyndryl\u2019s strategic solution that allows automation at enterprise level with a target to get to zero touch with reduced development and release cycle addressed by the community model. CACF leverages cloud native RedHat Automation platform \u2013 Ansible Tower on OpenShift and integrates with Services management processes, the Git repository and the automation playbooks. There is a rich set of are pre-built automations available using Ansible playbooks. Major CACF addressed use cases (on-prem) for existing clients are as below: \u00b7 Event/Incident remediation \u00b7 Patch Scanning and execution \u00b7 Security Health Check \u00b7 Service Request \u00b7 Build and Decommission \u00b7 Other Automation use cases AA - How is a discussion of on-prem use cases relevant here?","title":"3.12   Ansible &amp; CACF"},{"location":"lzdesign/platformauto/#313-other-services","text":"Azure provides various other services to manage and control Azure environment in automated fashion as below: \u00b7 Azure Backup for protecting data and backing up entire Windows/Linux VMs using backup extensions and backing up files, folders, and system state using the MARS agent \u00b7 Azure Security Center with Azure Defender for monitoring workloads and finding and fixing vulnerabilities to protect hybrid cloud workloads \u00b7 Azure Advisor analyses Azure services configurations and usage telemetry and offers personalized, actionable recommendations for reliability, security, operational aspects, performance, and cost. This also includes suggested actions that can be taken right away, postpone or dismiss. Advisor Quick Fix makes optimisation at scale faster and easier by allowing users to remediate recommendations for multiple resources simultaneously and with only a few clicks. Same can be accessed by Azure Portal, REST API, CLI and PowerShell. AA - So far as I know, the last two are not examples of Automation. Also Backup is a service that leverages Automation.","title":"3.13   Other Services"},{"location":"lzdesign/platformauto/#4-automation-recommendation","text":"Kyndryl has already embraced automation as the foundation for every aspect of Infrastructure Services delivery from Landing Zone build, day one infrastructure & application provisioning, day two management of infrastructure & applications which include security compliance, identity & access management, monitoring, logging etc. This automation approach will be further strengthened as part of Kyndryl and client\u2019s cloud adoption journey, using Infrastructure as Code (IaC) model for all Azure operations. Kyndryl will follow its established DevSecOps process for automation content creation, and for deployment wherever applicable. Enabling DevSecOps process and culture for clients is not part of this design scope and should be managed through the DevSecOps offerings. Based on Kyndryl\u2019s and products current capabilities, below is the recommendation for various automations. Note: Recommendations are based on best effort basis and mentioned integration between components/offerings and automation templates/playbooks may not exist/production grade validated and should to be consulted with offering teams before committing to the clients.","title":"4  Automation Recommendation"},{"location":"lzdesign/platformauto/#41-ready-stage-automation-recommendation","text":"For fully governed landing zone deployment, modify out of the box or build new Azure Blueprints to set up ISO-compliant foundation templates that will accelerate the landing zone build and workload migration. Landing Zone automation will be used to create Landing Zone elements as per recommendations and standards defined in the previous seven sections. Kyndryl proposes ARM template and Terraform templates based automation solution for various CAF Ready stage automation activities for Landing Zone Deployment like Azure managements groups & resource groups creation and deletion, Resource Tagging, Azure AD user & group creation/deletion/modifications, roles Assignment, creation of backup, infrastructure and security policies as per baseline defined in the previous sections, creation of log analytics workspaces and Azure Monitor configuration, creation of VNets, Subnets and all other network elements, peering the workload VNet with Hub network for on-premises integration and connectivity etc. These terraform and ARM templates will be secured and version controlled in Kyndryl\u2019s GitHub and/or client\u2019s GitHub if requested by a client. For Landing Zone Deployments, there are various out of box readily available implementation options available using Azure Blueprints which will be used as base and Kyndryl specific customizations and enhancements will be built, for the initial Landing Zone. Note that the landing zone template will include common platform elements and platform elements specific to workloads like VNets, Subnets, policies etc. and hence these ARM templates need to be additionally customized for each specific client & workload. The first landing zone will include common platform elements, whereas subsequent landing zones for new workloads will contain only workload specific elements. Various automation activities and consistency can also be achieved with pre-defined Azure Policies. Various Landing Zone deployment automation policies are defined in Azure Policies section. There are certain elements of landing zone creation that cannot be automated, which include configuring Active Directory elements, creation of AD groups and client network connectivity aspects and should be performed manually. Kyndryl offerings are working on developing/enhancing ARM templates for Landing Zone automated deployment to define workload and application landscape which will be taken as the input for landing zone creation. Refer to additional documentation on Kyndryl\u2019s Landing Zone Automation design approach and capabilities.","title":"4.1  Ready Stage Automation Recommendation"},{"location":"lzdesign/platformauto/#42-adopt-stage-automation-recommendation","text":"For Workload deployment and configuration management automation, Azure ARM templates, and Terraform templates should be used to automate the deployment and configuration of workloads such as VMs, Virtual Machines Scale Sets, SQL databases, AKS etc. These terraform and ARM templates should be secured and version controlled in Kyndryl\u2019s GitHub and/or in customer\u2019s GitHub if requested by a client. These Azure Resources/Composite patterns can be called from CMP, like MCMP where these ARM templates/Composite patterns are provided by Kyndryl offerings like MCDS or SO/ Client IT teams and execution happen in automated/semi-automated fashion along with Make Mange processes. These ARM/Terraform templates can also be triggered using GitOps or third party tools which support REST APIs to have consistent resource deployment. CACF based Ansible Tower automation should be leveraged for make manage onboarding automation for installing required service management agents, custom scripts and software, evidence capture, CI discovery, setting up of monitoring for application and the infrastructure, backup configuration and integration with backend systems.","title":"4.2  Adopt Stage Automation Recommendation"},{"location":"lzdesign/platformauto/#43-governance-automation-recommendation","text":"While deploying resources, the Cloud Governance team will work with the Operations team to identify business risks and establish a baseline set of policy statements that are intended to mitigate that risk. These policies will be implemented at the outset, then optimized over time based on regular reviews of operations. Recommended Azure Policies can be referred from Policies section. Azure Built in/custom policies/initiatives should be leveraged to evaluate Azure resource at specific intervals and perform deny the change, log the change or remediate non-compliant resources for resource consistency. Azure policies/initiative JSON consisting policy parameters, policy rules, definitions, and policy effects can be developed or modified using Azure Portal, CLI or PowerShell modules and can be applied through Azure Portal for given Management Group, Subscription or Resource Group Scope and optional Resource exclusions. Azure Security Center and Azure Sentinel should be used for assessing compliance and identifying security risks and for detection, response and recovery from threats. Azure Security policies (e.g. CIS Policy) will be configured for detecting and enforcing organization-wide settings to ensure consistent policy adherence and fast violation detection. For centralized cost governance, all in-scope workloads and resources should be properly tagged using Azure Policy. Azure Hybrid Benefit and OS, SQL licencing reuse can be leveraged for further cost optimization. On ongoing basis, resource utilization and performance should be evaluated across the environment and resources will be modified to use the smallest instance or SKU that can support the performance requirements of each resource. Azure policies should be used to identify and terminate any resources that are adding to costs but are not adding to business value. Azure Advisor recommendations along with remediation actions will be leveraged using Azure Portal and REST API.","title":"4.3  Governance Automation Recommendation"},{"location":"lzdesign/platformauto/#44-manage-automation-recommendation","text":"Automating Day2 tasks like monitoring, backup, patch management, process automation, configuration management etc. helps administrators to improve efficiency and reduce effort and manual error. MCMS currently leverages Azure Monitoring (In most of cases), and Cloud Native Backup while other processes are executed by CACF Ansible and local automation. Below Cloud Native or CACF Ansible can be proposed for automating these processes.","title":"4.4  Manage Automation Recommendation"},{"location":"lzdesign/platformauto/#441-azure-automation","text":"Various Azure native solutions, Azure Automation should be leveraged for Azure and non-Azure workloads operations, and decommissioning for various below processes:","title":"4.4.1   Azure Automation"},{"location":"lzdesign/platformauto/#4411-process-automation","text":"Process Automation in Azure Automation can be used for frequent, time-consuming, and error-prone cloud management tasks and orchestrating service provider or client processes across Hybrid environments. Orchestration should be written using Logic Apps while execution can be achieved using PowerShell, PowerShell Workflow, and graphical runbooks. Logic can be defined/modified through Azure Automation runbooks .","title":"4.4.1.1   Process Automation"},{"location":"lzdesign/platformauto/#4412-configuration-management","text":"Azure automation can be leveraged for collecting inventory (Virtual Machines and Server infrastructure) and tracking changes across services, daemons, software, registry, and files and raising alerts. This also supports query in-guest resources for visibility into installed applications and other configuration items. With Azure Automation State Configuration PowerShell Desired state configuration (DSC) for DSC resources and apply configurations to virtual or physical machines from a DSC pull server in the Azure cloud.","title":"4.4.1.2   Configuration Management"},{"location":"lzdesign/platformauto/#4413-update-management","text":"Azure Update Management should be leveraged for patching Windows and Linux VMs Cloud Natively.","title":"4.4.1.3  Update Management"},{"location":"lzdesign/platformauto/#4414-azure-backup","text":"Azure Backup service should be leveraged for Azure native Backup solution for Linux and Windows VM in Azure Recovery vaults.","title":"4.4.1.4  Azure Backup"},{"location":"lzdesign/platformauto/#442-ansible-automation-cacf","text":"Kyndryl will leverage it available automation for Day2 tasks automation such as security compliance, patching, identity & access management, monitoring etc. Ansible playbooks will be used for operational efficiency and process automation, automated incident remediations, configuration management, update management, task automation, event automation. Day 2 operations will be automated through ITSM tool like ServiceNow and MCMP (Multi-Cloud Management Platform) which are integrated with Azure through Azure native APIs and Ansible Tower. Client can open service requests through pre-defined service Catalogs in ServiceNow/MCMP. Workflows within these tools are used to validate the request after which automations are initiated through Ansible Tower & Terraform templates. The automation assets \u2013 Ansible playbooks or ARM templates etc, will be created and managed through DevSecOps process and all assets will be managed in GitHub. Currently, DevSecOps is limited to application deployments and upgrades only, on the workload subscriptions and Day2 automations are provided as part of manage services from MCMS & ISPW. The intention is to allow application developers to manage their application deployments and upgrades through DevSecOps process. This DevSecOps platform uses a combination of standard DevOps tool like Jenkins, Azure native DevOps tools and Ansible Tower and can be hosted at Kyndryl\u2019s management network or in client premises, or re-use an existing DevSecOps platform from the client with appropriate changes. Application developers will be able to manage their application deployments and upgrades through DevSecOps process. This DevSecOps platform uses a combination of standard DevOps tool like Jenkins, Azure native DevOps tools and Ansible Tower and will be hosted at Kyndryl\u2019s management network.","title":"4.4.2   Ansible Automation (CACF)"},{"location":"lzdesign/platformauto/#design-actions-by-solution-architects","text":"o Landing Zone creation will be automated based on all the design defintions from previous sections. o There will be templates provided in a future release to capture landing zone definitions which can be directly used by automation AA - We need to emphasize the following - Solution Architects will be expected to create automation templates (ARM templates, blueprints, Terraform configs or similar artifacts) based on sample templates that will be provided as these design recommendations evolve. The Kyndryl Automation Framework consisting of all the elements described in Section 2.1 are work in progress. The intent for this framework is to implement the use cases in Section 2.2. As and when there is definite progress in this space, the documentation will be updated. Section 3 is mainly information that Solution Architects can use to create custom automization. Section 4 are aids to Solution Architects in creation of their own automation templates. The sub-sections on Manage and Govern Automation Recommendations should not apply to Solution Architects. They could be there to provide insight into how automation in these phases is provided using Azure tooling. Overall, there are numerous language corrections to be made and also more concise and abbreviated wording to be used.","title":"Design Actions by Solution Architects"},{"location":"lzdesign/vmagents/","text":"Azure Virtual Machine (VM) Agents \u2693\ufe0e The core functionality from a workload perspective is generally built into the operating system (OS) image that is run in a VM. However, there are additional requirements such as configuration, additional software installation, monitoring, security and resiliency that are in general addressed by agents/extensions that are installed in the Guest OS. Azure VM extensions are small applications that provide post-deployment configuration and automation tasks on Azure VMs. For example, if a virtual machine requires software installation, anti-virus protection, or to run a script inside of it, a VM extension can be used. Azure VM extensions can be run with the Azure CLI, PowerShell, Azure Resource Manager templates, and the Azure portal. Extensions can be bundled with a new VM deployment, or run against any existing system. This document discusses the functions and installation of the agents/extensions that are pertinent to managed services such as monitoring, security and backup that have been covered in previous sections. The intent is to detail the additional installation and configuration required to support managed services for VMs. The agents/extensions to be discussed are classified into the following groups - Core Agents for Windows VMs Core Agents for Linux VMs VM Extensions for Security VM Extensions for Resiliency VM Extensions for Azure Monitor Third-party VM Extensions Core Agents for Windows VMs \u2693\ufe0e Functions Supported \u2693\ufe0e The Azure VM Agent is a secure lightweight process running in a Windows VM that performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Azure VM Agent supports automatic collection and transfer of Event Logs, OS Logs, Azure Logs and some registry keys to the VM's host for use in the investigation of issues. Install Extensions NOTE: The Azure Fabric Controller is part of the Azure management plane. It is responsible to assign infrastructure resources that Azure resources being created by customers map to. Agents on VMs access this Controller using the special IP address of 168.63.129.16. Azure VM Agent Installation \u2693\ufe0e The Azure VM Agent is pre-installed on any Windows VM deployed from an Azure Marketplace image and can be configured using any of the Azure Portal, PowerShell, Command Line Interface, or an Azure Resource Manager template. In this case updates of the Azure VM Agent are automatically performed. When using a custom VM image, the VM Agent installer can be downloaded and manually installed. In this case updates of the Azure VM Agent need to be performed manually. The Windows package has two parts - Provisioning Agent (PA) and Windows Guest Agent (WinGA). It is assumed that the PA assists with the VM configuration while the WinGA gathers and loads various logs into the VM's host and also provides an environment for other VM extensions. Verification of this assumption will be taken up shortly and the document updated based on the findings. Core Agents for Linux VMs \u2693\ufe0e Functions Supported \u2693\ufe0e The Linux VM (Guest) Agent runs in a Linux VM and performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Image Provisioning, Networking and Kernel Configuration (Details can be viewed at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-linux ) Install Extensions Linux VM Agent Installation \u2693\ufe0e The Linux VM Agent is pre-installed in images obtained from the Azure Marketplace. Azure endorses certain Linux distributions that integrate the Linux VM Agent package into their images and repositories. When creating custom images from such endorsed Linux distributions, Installation can use the relevant RPM or DEB package. The Linux VM Agent uses a configuration file (/etc/waagent.conf) that specifies if configuration is to be performed at provisioning time and if so, what the configuration should be. It is our understanding a default configuration is provided with all images obtained from the Azure Marketplace. For other images it is expected the configuration is provided by the image builder. With regard to updates, it is our understanding VM instances based on images obtained from the Azure Marketplace will have their Linux VM Agents updated automatically. Other instances will need manual updation. Virtual Machine (VM) Extensions for Security \u2693\ufe0e Some commonly used extensions for Security are covered briefly below. Azure Disk Encryption for Linux \u2693\ufe0e This extension leverages the dm-crypt subsystem in Linux to provide full disk encryption on select Azure Linux distributions and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-linux . Azure Disk Encryption for Windows \u2693\ufe0e This extension leverages BitLocker to provide full disk encryption on Azure virtual machines running Windows and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-windows . Key Vault for Linux \u2693\ufe0e This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-linux . Key Vault for Windows \u2693\ufe0e This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-windows . Azure Policy guest configuration for Linux and Windows \u2693\ufe0e This extension performs audit and configuration operations inside virtual machines for policies such as security baseline definitions. It is needed by policies such as security baseline definitions for VMs that need to check settings inside the VMs. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/guest-configuration . Desired State Configuration (DSC) for Linux \u2693\ufe0e This extension is used to install the Open Management Infrastructure (OMI) and DSC agents that in turn leverage the Azure Automation service to ensure the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. Note that currently this extension cannot co-exist with Log Analytics Agent. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-linux . Desired State Configuration (DSC) for Windows \u2693\ufe0e This extension is used to download a Powershell DSC configuration to a VM and to call into the Powershell DSC to enact the received DSC configuration, thus ensuring the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-windows . Azure Defender for Servers \u2693\ufe0e Azure Defender adds a number of security protections for Windows and Linux VMs two of which are Defender for Endpoint and Vulnerability scanning. Defender for Endpoint provides comprehensive endpoint threat detection and response. When enabled for Windows VMs, Defender for Endpoint uses the sensor built into Windows that provides data to Defender for Endpoint, presumably through the Log Analytics agent. When enabled for Linux VMs that run the supported Linux distributions, the integration of the auditd service with the Log Analytics agent is leveraged. There is no need to explicitly install an agent for either Windows or Linux VMs. Vulnerability scanning uses a vulnerability scanner provided by Qualys. An extension is provided that needs to be run on those Windows and Linux VMs for which Azure Defender is enabled and that are identified by Security Center as suitable to run the extension. So installation and configuration is typically driven through the Security Center interface on the Azure Portal. Microsoft Antimalware for Windows \u2693\ufe0e This is a real-time protection that helps identify and remove viruses, spyware, and other malicious software. It leverages the Antimalware extension that needs to be installed on a Windows VM when it is being either created or existing. Along with the extension binary a configuration file is pushed to the Windows VM for use in the configuration of the extension. Note that for certain older versions of Windows, this feature needs to be installed for malware protection, along with Defender for Endpoint. For the more recent versions of Windows, it is our understanding that Defender for Endpoint will cover malware protection as well. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/security/fundamentals/antimalware . VM Extensions for Resiliency \u2693\ufe0e Some of the extensions that support Backup are discussed briefly below. Azure Backup for SQL Server \u2693\ufe0e Azure Backup supports backup of SQL Server running in Azure VMs. Since it needs permission to access the application and fetch the necessary details, it installs the AzureBackupWindowsWorkload extension on the VM when it is registered for backup. More details on this are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/backup-azure-sql-server-running-azure-vm . Microsoft Azure Recovery Services (MARS) Agent \u2693\ufe0e This agent is used to selectively protect Windows files and folders, protect entire Windows volumes and entire Windows system state. It is to be first downloaded from the Azure Portal and installed manually on the Windows VMs that need any of the protections listed above. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-about-mars . Microsoft Azure Backup Server (MABS) for VMware \u2693\ufe0e MABS can be used to protect VMware VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. No agents are required as MABS uses the vCenter Server and the hypervisor to perform the operations. More details on this approach are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-backup-server-vmware . Microsoft Azure Backup Server (MABS) for Hyper-V \u2693\ufe0e MABS can do a host or guest-level backup of Hyper-V VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. At the host level, the MABS protection agent is installed on the Hyper-V host server or cluster and protects the entire VMs and data files running on that host. At the guest level, the agent is installed on each virtual machine and protects the workload present on that machine. There are pros and cons for both approaches that are detailed at https://docs.microsoft.com/en-us/azure/backup/back-up-hyper-v-virtual-machines-mabs . Azure Disk Backup \u2693\ufe0e Azure Disk Backup is a native, cloud-based backup solution that protects data in managed disks. It is a crash-consistent solution that uses incremental snapshots. Azure Backup Vault is used to configure backup for the managed disks. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/disk-backup-overview . Azure Blob Backup \u2693\ufe0e This is a managed, local data protection solution that protects blobs from corruption and accidental deletion. It uses the same storage account as the blob itself. It is managed from either the Backup Vault or the Backup Center. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/blob-backup-overview . Azure File Share Backup \u2693\ufe0e This is a native, cloud based backup solution that protects file shares and eliminates any overheads due to on-premises backup solutions. It leverages the Azure Backup services to take snapshots that are then held in the same storage account as the file share being backed up. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/azure-file-share-backup-overview . SAP HANA Database on Azure VM Backup \u2693\ufe0e Azure Backup service supports backup of SAP HANA databases running on Azure VMs. Each Azure VM running SAP HANA databases is registered with a Recovery Services vault and the databases to be backed up are discovered. Azure Backup service then installs an Azure Backup Plugin for HANA on the Azure VM. Backup of databases can now be configured. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/sap-hana-db-about . VM Snapshot for Azure Backup for Linux \u2693\ufe0e This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-linux . VM Snapshot for Azure Backup for Windows \u2693\ufe0e This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-windows . VM Extensions for Azure Monitor \u2693\ufe0e Azure Monitor leverages a number of options in terms of agents that warrant a more detailed discussion. Linux VM Extensions \u2693\ufe0e The recently launched Azure Monitor agent consolidates the main functions of Metrics and Log gathering and provides additional capabilities such as sending data to multiple workspaces and improved management of extensions. However it has certain drawbacks in comparison to the existing agents such as lack of support for gathering file based and IIS logs. It is to be installed manually via the Portal or CLI. Simultaneous installation on multiple VMs can be performed through the creation of Data Collection Rules. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/azure-monitor-agent-overview?tabs=PowerShellWindows . In contrast there are multiple existing agents each of which is described in terms of its functionality below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Syslog and Performance data to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-linux . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-linux . Linux Diagnostic Extension (LAD) and Telegraf agent - Can be installed on a VM either as it is deployed or existing. The LAD gathers syslog, other file based logs and performance counters and sends them to Azure Blob Storage and Event Hubs. The Telegraf agent can be used to send performance counters to Azure Monitor Metrics. More details on these are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-linux . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on an existing VM. For unknown reasons this extension does not appear to be available for installation on VMs as they are being created. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics Windows VM Extensions \u2693\ufe0e The recently launched Azure Monitor agent functions in the same manner for Windows as described above for Linux. In contrast the multiple existing agents are described below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Event Logs, Performance data, File based logs, IIS logs and other data in support of various Insights services to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-windows . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-windows Windows Diagnostics Extension (WAD) - Can be installed on a VM either as it is deployed or existing. The WAD gathers Windows Event Logs, a variety of other logs and performance counters and sends them to Azure Storage, Application Insights and Event Hub. It also sends the performance counters to Azure Monitor Metrics. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-windows . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics Recommendations \u2693\ufe0e The recently launched Azure Monitor agent has the following limitations - Lack of production support for Azure Security Center and Azure Sentinel services Lack of production support for Azure Monitor services such as VM Insights, VM Insights guest health, SQL Insights Lack of Production support for Azure solutions such as Change Tracking and Update Management The recommendation, given the expectation that most engagements will involve managed services for Production workloads, is to continue with the use of the multiple existing agents while waiting for the newly launched Azure Monitor agent to mature. A detailed comparison of the newly launched Azure Monitor agent with the existing agents can be viewed at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/agents-overview . Third-party VM Extensions \u2693\ufe0e A number of third party VM extensions are also available covering monitoring, security, resiliency and installation of drivers for special-purpose hardware such as GPUs. This section covers two such third-party extensions that are worth mentioning. Datadog Extension for Monitoring of Linux and Windows VMs \u2693\ufe0e Datadog agents can be be installed as extensions to gather metrics, traces and logs from Azure VMs (both Linux and Windows) and send them to the Datadog service which can then allow users to view the data through dashboards, graphs and monitors. The service can also be configured to send alerts. The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Datadog account needs to be set up and an API Key obtained that can then be used by agents to connect to the Datadog service. The agents can also collect application metrics so correlation of an application\u2019s performance with the VM level metrics can be performed. The agent monitors services running in an Azure VM, such as IIS and SQL Server, as well as non-Windows integrations such as MySQL, NGINX, and Cassandra. More details on this extension are provided at https://docs.datadoghq.com/integrations/guide/azure-portal/ . Symantec Cloud Workload Protection (CWP) Extension for Linux and Windows VMs \u2693\ufe0e Symantec CWP agents can be installed as extensions to provide security for Azure VMs (both Linux and Windows) with application protection, intrusion detection/prevention, real-time Anti-Malware, and real-time file integrity monitoring (RT-FIM). The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Cloud Workload Protection account needs to be set up with Symantec from which credentials such as Customer ID, Domain ID and Customer Secret Key need to be obtained. These credentials then need to be used in the configuration of the agent on a VM. More details on this extension are provided at https://techdocs.broadcom.com/us/en/symantec-security-software/endpoint-security-and-management/cloud-workload-protection/1-0/Getting_Started_1/architecture-azure-v133783726-d187e9316.html . Summary \u2693\ufe0e The table below summarizes the manner of installation for all the agents/extensions discussed above. It is assumed that the core VM agents are present by default, either because Azure standard images were used or the core VM agents were installed as one of the first configuration steps performed on VMs that were booted from custom images. VM Agent/Extension Standard Image Custom Image Disk Encryption for Linux Manual Manual Disk Encryption for Windows Manual Manual Key Vault for Linux Manual Manual Key Vault for Windows Manual Manual Policy Guest Configuration (DSC) for Linux Manual Manual Policy Guest Configuration (DSC) for Windows Manual Manual Defender for Endpoint for Linux Pre-installed Pre-installed Defender for Endpoint for Windows Pre-installed Pre-installed Vulnerability Scanner for Linux Manual Manual Vulnerability Scanner for Windows Manual Manual Microsoft Antimalware for Windows Manual Manual Backup for SQL Server for Windows Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server MARS Agent for Windows Manual Manual MABS for VMware No agents required No agents required MABS for Hyper-V Manual Manual Disk Backup No agents required No agents required Blob Backup No agents required No agents required File Share Backup No agents required No agents required SAP HANA Database Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server VM Snapshot for Azure Backup for Linux Auto-installed on triggering of first backup Auto-installed on triggering of first backup VM Snapshot for Azure Backup for Windows Auto-installed on triggering of first backup Auto-installed on triggering of first backup Monitor Agent for Linux Manual Manual Monitor Agent for Windows Manual Manual Log Analytics Agent for Linux Auto-installed on Security Center enablement Auto-installed on Security Center enablement Log Analytics Agent for Windows Auto-installed on Security Center enablement Auto-installed on Security Center enablement Dependency Agent for Linux Manual Manual Dependency Agent for Windows Manual Manual Linux Diagnostic Agent and Telegraf Agent Manual Manual Windows Diagnostic Agent Manual Manual Network Watcher for Linux Manual Manual Network Watcher for Windows Manual Manual Performance Diagnostics agent for Linux Manual Manual Performance Diagnostics agent for Manual Manual Datadog extenstion for Monitoring of Linux and Windows Manual Manual Symantec Cloud Workload Protection extension for Linux and Windows Manual Manual","title":"Azure Virtual Machine (VM) Agents"},{"location":"lzdesign/vmagents/#azure-virtual-machine-vm-agents","text":"The core functionality from a workload perspective is generally built into the operating system (OS) image that is run in a VM. However, there are additional requirements such as configuration, additional software installation, monitoring, security and resiliency that are in general addressed by agents/extensions that are installed in the Guest OS. Azure VM extensions are small applications that provide post-deployment configuration and automation tasks on Azure VMs. For example, if a virtual machine requires software installation, anti-virus protection, or to run a script inside of it, a VM extension can be used. Azure VM extensions can be run with the Azure CLI, PowerShell, Azure Resource Manager templates, and the Azure portal. Extensions can be bundled with a new VM deployment, or run against any existing system. This document discusses the functions and installation of the agents/extensions that are pertinent to managed services such as monitoring, security and backup that have been covered in previous sections. The intent is to detail the additional installation and configuration required to support managed services for VMs. The agents/extensions to be discussed are classified into the following groups - Core Agents for Windows VMs Core Agents for Linux VMs VM Extensions for Security VM Extensions for Resiliency VM Extensions for Azure Monitor Third-party VM Extensions","title":"Azure Virtual Machine (VM) Agents"},{"location":"lzdesign/vmagents/#core-agents-for-windows-vms","text":"","title":"Core Agents for Windows VMs"},{"location":"lzdesign/vmagents/#functions-supported","text":"The Azure VM Agent is a secure lightweight process running in a Windows VM that performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Azure VM Agent supports automatic collection and transfer of Event Logs, OS Logs, Azure Logs and some registry keys to the VM's host for use in the investigation of issues. Install Extensions NOTE: The Azure Fabric Controller is part of the Azure management plane. It is responsible to assign infrastructure resources that Azure resources being created by customers map to. Agents on VMs access this Controller using the special IP address of 168.63.129.16.","title":"Functions Supported"},{"location":"lzdesign/vmagents/#azure-vm-agent-installation","text":"The Azure VM Agent is pre-installed on any Windows VM deployed from an Azure Marketplace image and can be configured using any of the Azure Portal, PowerShell, Command Line Interface, or an Azure Resource Manager template. In this case updates of the Azure VM Agent are automatically performed. When using a custom VM image, the VM Agent installer can be downloaded and manually installed. In this case updates of the Azure VM Agent need to be performed manually. The Windows package has two parts - Provisioning Agent (PA) and Windows Guest Agent (WinGA). It is assumed that the PA assists with the VM configuration while the WinGA gathers and loads various logs into the VM's host and also provides an environment for other VM extensions. Verification of this assumption will be taken up shortly and the document updated based on the findings.","title":"Azure VM Agent Installation"},{"location":"lzdesign/vmagents/#core-agents-for-linux-vms","text":"","title":"Core Agents for Linux VMs"},{"location":"lzdesign/vmagents/#functions-supported_1","text":"The Linux VM (Guest) Agent runs in a Linux VM and performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Image Provisioning, Networking and Kernel Configuration (Details can be viewed at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-linux ) Install Extensions","title":"Functions Supported"},{"location":"lzdesign/vmagents/#linux-vm-agent-installation","text":"The Linux VM Agent is pre-installed in images obtained from the Azure Marketplace. Azure endorses certain Linux distributions that integrate the Linux VM Agent package into their images and repositories. When creating custom images from such endorsed Linux distributions, Installation can use the relevant RPM or DEB package. The Linux VM Agent uses a configuration file (/etc/waagent.conf) that specifies if configuration is to be performed at provisioning time and if so, what the configuration should be. It is our understanding a default configuration is provided with all images obtained from the Azure Marketplace. For other images it is expected the configuration is provided by the image builder. With regard to updates, it is our understanding VM instances based on images obtained from the Azure Marketplace will have their Linux VM Agents updated automatically. Other instances will need manual updation.","title":"Linux VM Agent Installation"},{"location":"lzdesign/vmagents/#virtual-machine-vm-extensions-for-security","text":"Some commonly used extensions for Security are covered briefly below.","title":"Virtual Machine (VM) Extensions for Security"},{"location":"lzdesign/vmagents/#azure-disk-encryption-for-linux","text":"This extension leverages the dm-crypt subsystem in Linux to provide full disk encryption on select Azure Linux distributions and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-linux .","title":"Azure Disk Encryption for Linux"},{"location":"lzdesign/vmagents/#azure-disk-encryption-for-windows","text":"This extension leverages BitLocker to provide full disk encryption on Azure virtual machines running Windows and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-windows .","title":"Azure Disk Encryption for Windows"},{"location":"lzdesign/vmagents/#key-vault-for-linux","text":"This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-linux .","title":"Key Vault for Linux"},{"location":"lzdesign/vmagents/#key-vault-for-windows","text":"This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-windows .","title":"Key Vault for Windows"},{"location":"lzdesign/vmagents/#azure-policy-guest-configuration-for-linux-and-windows","text":"This extension performs audit and configuration operations inside virtual machines for policies such as security baseline definitions. It is needed by policies such as security baseline definitions for VMs that need to check settings inside the VMs. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/guest-configuration .","title":"Azure Policy guest configuration for Linux and Windows"},{"location":"lzdesign/vmagents/#desired-state-configuration-dsc-for-linux","text":"This extension is used to install the Open Management Infrastructure (OMI) and DSC agents that in turn leverage the Azure Automation service to ensure the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. Note that currently this extension cannot co-exist with Log Analytics Agent. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-linux .","title":"Desired State Configuration (DSC) for Linux"},{"location":"lzdesign/vmagents/#desired-state-configuration-dsc-for-windows","text":"This extension is used to download a Powershell DSC configuration to a VM and to call into the Powershell DSC to enact the received DSC configuration, thus ensuring the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-windows .","title":"Desired State Configuration (DSC) for Windows"},{"location":"lzdesign/vmagents/#azure-defender-for-servers","text":"Azure Defender adds a number of security protections for Windows and Linux VMs two of which are Defender for Endpoint and Vulnerability scanning. Defender for Endpoint provides comprehensive endpoint threat detection and response. When enabled for Windows VMs, Defender for Endpoint uses the sensor built into Windows that provides data to Defender for Endpoint, presumably through the Log Analytics agent. When enabled for Linux VMs that run the supported Linux distributions, the integration of the auditd service with the Log Analytics agent is leveraged. There is no need to explicitly install an agent for either Windows or Linux VMs. Vulnerability scanning uses a vulnerability scanner provided by Qualys. An extension is provided that needs to be run on those Windows and Linux VMs for which Azure Defender is enabled and that are identified by Security Center as suitable to run the extension. So installation and configuration is typically driven through the Security Center interface on the Azure Portal.","title":"Azure Defender for Servers"},{"location":"lzdesign/vmagents/#microsoft-antimalware-for-windows","text":"This is a real-time protection that helps identify and remove viruses, spyware, and other malicious software. It leverages the Antimalware extension that needs to be installed on a Windows VM when it is being either created or existing. Along with the extension binary a configuration file is pushed to the Windows VM for use in the configuration of the extension. Note that for certain older versions of Windows, this feature needs to be installed for malware protection, along with Defender for Endpoint. For the more recent versions of Windows, it is our understanding that Defender for Endpoint will cover malware protection as well. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/security/fundamentals/antimalware .","title":"Microsoft Antimalware for Windows"},{"location":"lzdesign/vmagents/#vm-extensions-for-resiliency","text":"Some of the extensions that support Backup are discussed briefly below.","title":"VM Extensions for Resiliency"},{"location":"lzdesign/vmagents/#azure-backup-for-sql-server","text":"Azure Backup supports backup of SQL Server running in Azure VMs. Since it needs permission to access the application and fetch the necessary details, it installs the AzureBackupWindowsWorkload extension on the VM when it is registered for backup. More details on this are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/backup-azure-sql-server-running-azure-vm .","title":"Azure Backup for SQL Server"},{"location":"lzdesign/vmagents/#microsoft-azure-recovery-services-mars-agent","text":"This agent is used to selectively protect Windows files and folders, protect entire Windows volumes and entire Windows system state. It is to be first downloaded from the Azure Portal and installed manually on the Windows VMs that need any of the protections listed above. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-about-mars .","title":"Microsoft Azure Recovery Services (MARS) Agent"},{"location":"lzdesign/vmagents/#microsoft-azure-backup-server-mabs-for-vmware","text":"MABS can be used to protect VMware VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. No agents are required as MABS uses the vCenter Server and the hypervisor to perform the operations. More details on this approach are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-backup-server-vmware .","title":"Microsoft Azure Backup Server (MABS) for VMware"},{"location":"lzdesign/vmagents/#microsoft-azure-backup-server-mabs-for-hyper-v","text":"MABS can do a host or guest-level backup of Hyper-V VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. At the host level, the MABS protection agent is installed on the Hyper-V host server or cluster and protects the entire VMs and data files running on that host. At the guest level, the agent is installed on each virtual machine and protects the workload present on that machine. There are pros and cons for both approaches that are detailed at https://docs.microsoft.com/en-us/azure/backup/back-up-hyper-v-virtual-machines-mabs .","title":"Microsoft Azure Backup Server (MABS) for Hyper-V"},{"location":"lzdesign/vmagents/#azure-disk-backup","text":"Azure Disk Backup is a native, cloud-based backup solution that protects data in managed disks. It is a crash-consistent solution that uses incremental snapshots. Azure Backup Vault is used to configure backup for the managed disks. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/disk-backup-overview .","title":"Azure Disk Backup"},{"location":"lzdesign/vmagents/#azure-blob-backup","text":"This is a managed, local data protection solution that protects blobs from corruption and accidental deletion. It uses the same storage account as the blob itself. It is managed from either the Backup Vault or the Backup Center. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/blob-backup-overview .","title":"Azure Blob Backup"},{"location":"lzdesign/vmagents/#azure-file-share-backup","text":"This is a native, cloud based backup solution that protects file shares and eliminates any overheads due to on-premises backup solutions. It leverages the Azure Backup services to take snapshots that are then held in the same storage account as the file share being backed up. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/azure-file-share-backup-overview .","title":"Azure File Share Backup"},{"location":"lzdesign/vmagents/#sap-hana-database-on-azure-vm-backup","text":"Azure Backup service supports backup of SAP HANA databases running on Azure VMs. Each Azure VM running SAP HANA databases is registered with a Recovery Services vault and the databases to be backed up are discovered. Azure Backup service then installs an Azure Backup Plugin for HANA on the Azure VM. Backup of databases can now be configured. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/sap-hana-db-about .","title":"SAP HANA Database on Azure VM Backup"},{"location":"lzdesign/vmagents/#vm-snapshot-for-azure-backup-for-linux","text":"This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-linux .","title":"VM Snapshot for Azure Backup for Linux"},{"location":"lzdesign/vmagents/#vm-snapshot-for-azure-backup-for-windows","text":"This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-windows .","title":"VM Snapshot for Azure Backup for Windows"},{"location":"lzdesign/vmagents/#vm-extensions-for-azure-monitor","text":"Azure Monitor leverages a number of options in terms of agents that warrant a more detailed discussion.","title":"VM Extensions for Azure Monitor"},{"location":"lzdesign/vmagents/#linux-vm-extensions","text":"The recently launched Azure Monitor agent consolidates the main functions of Metrics and Log gathering and provides additional capabilities such as sending data to multiple workspaces and improved management of extensions. However it has certain drawbacks in comparison to the existing agents such as lack of support for gathering file based and IIS logs. It is to be installed manually via the Portal or CLI. Simultaneous installation on multiple VMs can be performed through the creation of Data Collection Rules. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/azure-monitor-agent-overview?tabs=PowerShellWindows . In contrast there are multiple existing agents each of which is described in terms of its functionality below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Syslog and Performance data to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-linux . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-linux . Linux Diagnostic Extension (LAD) and Telegraf agent - Can be installed on a VM either as it is deployed or existing. The LAD gathers syslog, other file based logs and performance counters and sends them to Azure Blob Storage and Event Hubs. The Telegraf agent can be used to send performance counters to Azure Monitor Metrics. More details on these are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-linux . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on an existing VM. For unknown reasons this extension does not appear to be available for installation on VMs as they are being created. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics","title":"Linux VM Extensions"},{"location":"lzdesign/vmagents/#windows-vm-extensions","text":"The recently launched Azure Monitor agent functions in the same manner for Windows as described above for Linux. In contrast the multiple existing agents are described below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Event Logs, Performance data, File based logs, IIS logs and other data in support of various Insights services to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-windows . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-windows Windows Diagnostics Extension (WAD) - Can be installed on a VM either as it is deployed or existing. The WAD gathers Windows Event Logs, a variety of other logs and performance counters and sends them to Azure Storage, Application Insights and Event Hub. It also sends the performance counters to Azure Monitor Metrics. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-windows . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics","title":"Windows VM Extensions"},{"location":"lzdesign/vmagents/#recommendations","text":"The recently launched Azure Monitor agent has the following limitations - Lack of production support for Azure Security Center and Azure Sentinel services Lack of production support for Azure Monitor services such as VM Insights, VM Insights guest health, SQL Insights Lack of Production support for Azure solutions such as Change Tracking and Update Management The recommendation, given the expectation that most engagements will involve managed services for Production workloads, is to continue with the use of the multiple existing agents while waiting for the newly launched Azure Monitor agent to mature. A detailed comparison of the newly launched Azure Monitor agent with the existing agents can be viewed at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/agents-overview .","title":"Recommendations"},{"location":"lzdesign/vmagents/#third-party-vm-extensions","text":"A number of third party VM extensions are also available covering monitoring, security, resiliency and installation of drivers for special-purpose hardware such as GPUs. This section covers two such third-party extensions that are worth mentioning.","title":"Third-party VM Extensions"},{"location":"lzdesign/vmagents/#datadog-extension-for-monitoring-of-linux-and-windows-vms","text":"Datadog agents can be be installed as extensions to gather metrics, traces and logs from Azure VMs (both Linux and Windows) and send them to the Datadog service which can then allow users to view the data through dashboards, graphs and monitors. The service can also be configured to send alerts. The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Datadog account needs to be set up and an API Key obtained that can then be used by agents to connect to the Datadog service. The agents can also collect application metrics so correlation of an application\u2019s performance with the VM level metrics can be performed. The agent monitors services running in an Azure VM, such as IIS and SQL Server, as well as non-Windows integrations such as MySQL, NGINX, and Cassandra. More details on this extension are provided at https://docs.datadoghq.com/integrations/guide/azure-portal/ .","title":"Datadog Extension for Monitoring of Linux and Windows VMs"},{"location":"lzdesign/vmagents/#symantec-cloud-workload-protection-cwp-extension-for-linux-and-windows-vms","text":"Symantec CWP agents can be installed as extensions to provide security for Azure VMs (both Linux and Windows) with application protection, intrusion detection/prevention, real-time Anti-Malware, and real-time file integrity monitoring (RT-FIM). The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Cloud Workload Protection account needs to be set up with Symantec from which credentials such as Customer ID, Domain ID and Customer Secret Key need to be obtained. These credentials then need to be used in the configuration of the agent on a VM. More details on this extension are provided at https://techdocs.broadcom.com/us/en/symantec-security-software/endpoint-security-and-management/cloud-workload-protection/1-0/Getting_Started_1/architecture-azure-v133783726-d187e9316.html .","title":"Symantec Cloud Workload Protection (CWP) Extension for Linux and Windows VMs"},{"location":"lzdesign/vmagents/#summary","text":"The table below summarizes the manner of installation for all the agents/extensions discussed above. It is assumed that the core VM agents are present by default, either because Azure standard images were used or the core VM agents were installed as one of the first configuration steps performed on VMs that were booted from custom images. VM Agent/Extension Standard Image Custom Image Disk Encryption for Linux Manual Manual Disk Encryption for Windows Manual Manual Key Vault for Linux Manual Manual Key Vault for Windows Manual Manual Policy Guest Configuration (DSC) for Linux Manual Manual Policy Guest Configuration (DSC) for Windows Manual Manual Defender for Endpoint for Linux Pre-installed Pre-installed Defender for Endpoint for Windows Pre-installed Pre-installed Vulnerability Scanner for Linux Manual Manual Vulnerability Scanner for Windows Manual Manual Microsoft Antimalware for Windows Manual Manual Backup for SQL Server for Windows Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server MARS Agent for Windows Manual Manual MABS for VMware No agents required No agents required MABS for Hyper-V Manual Manual Disk Backup No agents required No agents required Blob Backup No agents required No agents required File Share Backup No agents required No agents required SAP HANA Database Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server VM Snapshot for Azure Backup for Linux Auto-installed on triggering of first backup Auto-installed on triggering of first backup VM Snapshot for Azure Backup for Windows Auto-installed on triggering of first backup Auto-installed on triggering of first backup Monitor Agent for Linux Manual Manual Monitor Agent for Windows Manual Manual Log Analytics Agent for Linux Auto-installed on Security Center enablement Auto-installed on Security Center enablement Log Analytics Agent for Windows Auto-installed on Security Center enablement Auto-installed on Security Center enablement Dependency Agent for Linux Manual Manual Dependency Agent for Windows Manual Manual Linux Diagnostic Agent and Telegraf Agent Manual Manual Windows Diagnostic Agent Manual Manual Network Watcher for Linux Manual Manual Network Watcher for Windows Manual Manual Performance Diagnostics agent for Linux Manual Manual Performance Diagnostics agent for Manual Manual Datadog extenstion for Monitoring of Linux and Windows Manual Manual Symantec Cloud Workload Protection extension for Linux and Windows Manual Manual","title":"Summary"},{"location":"lzdesignsample1/backupdr/","text":"Backup and restore. \u2693\ufe0e This section provides Backup guidance for the sample 3 tier workload deployment architecture. Azure provides native backup and restore service for backing up cloud resources like Azure VMs and PaaS databases. This guidance is based on native backup service. There are third party backup and resiliency services are available in Azure marketplace. Those are not evaluated/considered in this design guidance. Key Requirements \u2693\ufe0e There are 3 key requirements for the sample 3 tier workload. Securely backup the VM, databases and the data in the storage accounts as per the application/enterprise backup policy Monitor backup failures and backup policy compliance and notify the Ops team on non-compliance. Backup reporting dashboards Design recommendations \u2693\ufe0e The high-level backup solution design for a 3-tier application is as below The solution involves below key components. Azure Recovery vault to protect VM data, SQL DB data and the application data stored in the storage accounts. Backup Policy : It specifies the frequency and the retention period. A backup policy is important to provide an optimised backup solution that meets costs and application requirements. Log Analytics Workspace : Store the backup log data centrally in a Log Analytics workspace for monitoring, troubleshooting, and reporting purpose. Azure monitoring and Alerting : Use Azure monitor to monitor backup scenarios like backup failures, non-compliance, backup deletion etc. Backup Centre : Setup and configure centralized Backup Centre to provide insight from recovery vaults data and from Log analytics data. Recovery Vault: \u2693\ufe0e Azure Backup uses vaults (Recovery Services and Backup vaults) to orchestrate and manage backups. It also uses vaults to store backed-up data. Effective vault design helps manage backup assets in Azure to support Application and business priorities. Recovery vault is scoped to a subscription and to a region. Hence it is recommended to have a Recovery vault per workload subscription and in the same region as that of application workload. Use single vault to backup VMs, databases and storage accounts. Set the storage replication type as \u201c Geo-redundant \u201d. Set the restore option as \u201c Cross Region Restore (CRR) \u201d Set encryption to \u201c Customer-managed keys \u201d. Backup Policy: \u2693\ufe0e Backup policy specifies what needs to be backed up, when and how long the backup needs to be retained. Backup policy is applied when you configure the backup. It is possible to have multiple backup policies as per application requirements, regulatory requirements, and DR requirements (RPO/RTO). Consider the following guidelines when creating Backup Policy: Schedule considerations Group VMs that require the same schedule start time, frequency, and retention settings within a single policy. Ensure the backup scheduled start time is during non-peak production application time. To distribute backup traffic, consider backing up different VMs at different times of the day and make sure the times do not overlap. Retention considerations Plan for \u201cdaily\", \"weekly\", \"monthly\" and \"yearly\" retentions. This will have an impact on costs and storage. Retention period can be adjusted after the backup policy is applied. If you're retiring or decommissioning your data source (VM, application), but need to retain data for audit or compliance purposes then use the option \u201c Stop protection and retain backup data \u201d. \u201cStop protection and delete backup data \u201d - This option will stop the backup and will delete all the recovery points. Use this option carefully. Backup policy or Virtual Machines \u2693\ufe0e A sample backup policy for VM backup as below. Backup policy for SQL Managed Instance \u2693\ufe0e It is a fully managed service. Azure decides the backup frequency and the retention period. Pls refer Automated backups for Azure SQL Managed Instance for further details Security considerations: \u2693\ufe0e Azure Backup provides confidentiality, integrity, and availability assurances against deliberate attacks and abuse of your valuable data and systems. Consider the following security guidelines for your Azure Backup solution: Azure role-based access control (Azure RBAC) enables fine-grained access management. Grant \u201c Backup contributor \u201d or \u201c Backup operator \u201d role to users necessary to perform their jobs. Enable \u201c soft delete option \u201d to protect the backup data from unintentional deletes. For SQL DB workloads use Private Endpoints to securely backup the data to the vault. Monitoring and Alerting \u2693\ufe0e It is important to monitor all backup solutions and get notified on important scenarios such as backup failures, security breach etc. This section details the monitoring and notification considerations. Recovery vault provides in-built job monitoring for operations such as backup jobs, backup restore, delete backup, and so on. It is possible to have multiple recovery vaults for a given application. Hence it is recommended to have centralized backup monitoring and alerting solution deployed in the \u201cmanagement\u201d subscription. Enable \u201c Diagnostic logging \u201d and configure the setting to send the log to a central backup log analytics workspace in management subscription as shown above. Configure the alerting to send notification through Azure Monitor for critical backup scenarios such as backup non-compliance, backup job failures, backup deletes, backup restores etc. Use log queries to perform complex analysis and gain deep insights on the backup solution. Reporting and Dashboard: \u2693\ufe0e Enable Backup Reports using Azure Backup centre to understand and optimize backup storage growth, stopping backups for non-critical workloads or deleted VMs. This will help you to get an aggregated view of your entire estate from a backup perspective. This includes not only information on your backup items, but also resources that aren't configured for backup. This ensures that you never miss protecting critical data in your growing estate and your backups are optimized for non-critical workloads or deleted workloads. The recommendations are. Setup and configure the backup centre in the management subscription for the centralized backup reporting and dashboards. Configure the Backup Centre with backup Log analytics workspace in the management subscription to generate in-depth reports from all recovery vaults. Enable/create below reports. Forecasting of cloud storage consumed. Auditing of backups and restores. Identifying key trends at different levels of granularity. Backup compliance","title":"6.Business Continuity & Disaster Recovery"},{"location":"lzdesignsample1/backupdr/#backup-and-restore","text":"This section provides Backup guidance for the sample 3 tier workload deployment architecture. Azure provides native backup and restore service for backing up cloud resources like Azure VMs and PaaS databases. This guidance is based on native backup service. There are third party backup and resiliency services are available in Azure marketplace. Those are not evaluated/considered in this design guidance.","title":"Backup and restore."},{"location":"lzdesignsample1/backupdr/#key-requirements","text":"There are 3 key requirements for the sample 3 tier workload. Securely backup the VM, databases and the data in the storage accounts as per the application/enterprise backup policy Monitor backup failures and backup policy compliance and notify the Ops team on non-compliance. Backup reporting dashboards","title":"Key Requirements"},{"location":"lzdesignsample1/backupdr/#design-recommendations","text":"The high-level backup solution design for a 3-tier application is as below The solution involves below key components. Azure Recovery vault to protect VM data, SQL DB data and the application data stored in the storage accounts. Backup Policy : It specifies the frequency and the retention period. A backup policy is important to provide an optimised backup solution that meets costs and application requirements. Log Analytics Workspace : Store the backup log data centrally in a Log Analytics workspace for monitoring, troubleshooting, and reporting purpose. Azure monitoring and Alerting : Use Azure monitor to monitor backup scenarios like backup failures, non-compliance, backup deletion etc. Backup Centre : Setup and configure centralized Backup Centre to provide insight from recovery vaults data and from Log analytics data.","title":"Design recommendations"},{"location":"lzdesignsample1/backupdr/#recovery-vault","text":"Azure Backup uses vaults (Recovery Services and Backup vaults) to orchestrate and manage backups. It also uses vaults to store backed-up data. Effective vault design helps manage backup assets in Azure to support Application and business priorities. Recovery vault is scoped to a subscription and to a region. Hence it is recommended to have a Recovery vault per workload subscription and in the same region as that of application workload. Use single vault to backup VMs, databases and storage accounts. Set the storage replication type as \u201c Geo-redundant \u201d. Set the restore option as \u201c Cross Region Restore (CRR) \u201d Set encryption to \u201c Customer-managed keys \u201d.","title":"Recovery Vault:"},{"location":"lzdesignsample1/backupdr/#backup-policy","text":"Backup policy specifies what needs to be backed up, when and how long the backup needs to be retained. Backup policy is applied when you configure the backup. It is possible to have multiple backup policies as per application requirements, regulatory requirements, and DR requirements (RPO/RTO). Consider the following guidelines when creating Backup Policy: Schedule considerations Group VMs that require the same schedule start time, frequency, and retention settings within a single policy. Ensure the backup scheduled start time is during non-peak production application time. To distribute backup traffic, consider backing up different VMs at different times of the day and make sure the times do not overlap. Retention considerations Plan for \u201cdaily\", \"weekly\", \"monthly\" and \"yearly\" retentions. This will have an impact on costs and storage. Retention period can be adjusted after the backup policy is applied. If you're retiring or decommissioning your data source (VM, application), but need to retain data for audit or compliance purposes then use the option \u201c Stop protection and retain backup data \u201d. \u201cStop protection and delete backup data \u201d - This option will stop the backup and will delete all the recovery points. Use this option carefully.","title":"Backup Policy:"},{"location":"lzdesignsample1/backupdr/#backup-policy-or-virtual-machines","text":"A sample backup policy for VM backup as below.","title":"Backup policy or Virtual Machines"},{"location":"lzdesignsample1/backupdr/#backup-policy-for-sql-managed-instance","text":"It is a fully managed service. Azure decides the backup frequency and the retention period. Pls refer Automated backups for Azure SQL Managed Instance for further details","title":"Backup policy for SQL Managed Instance"},{"location":"lzdesignsample1/backupdr/#security-considerations","text":"Azure Backup provides confidentiality, integrity, and availability assurances against deliberate attacks and abuse of your valuable data and systems. Consider the following security guidelines for your Azure Backup solution: Azure role-based access control (Azure RBAC) enables fine-grained access management. Grant \u201c Backup contributor \u201d or \u201c Backup operator \u201d role to users necessary to perform their jobs. Enable \u201c soft delete option \u201d to protect the backup data from unintentional deletes. For SQL DB workloads use Private Endpoints to securely backup the data to the vault.","title":"Security considerations:"},{"location":"lzdesignsample1/backupdr/#monitoring-and-alerting","text":"It is important to monitor all backup solutions and get notified on important scenarios such as backup failures, security breach etc. This section details the monitoring and notification considerations. Recovery vault provides in-built job monitoring for operations such as backup jobs, backup restore, delete backup, and so on. It is possible to have multiple recovery vaults for a given application. Hence it is recommended to have centralized backup monitoring and alerting solution deployed in the \u201cmanagement\u201d subscription. Enable \u201c Diagnostic logging \u201d and configure the setting to send the log to a central backup log analytics workspace in management subscription as shown above. Configure the alerting to send notification through Azure Monitor for critical backup scenarios such as backup non-compliance, backup job failures, backup deletes, backup restores etc. Use log queries to perform complex analysis and gain deep insights on the backup solution.","title":"Monitoring and Alerting"},{"location":"lzdesignsample1/backupdr/#reporting-and-dashboard","text":"Enable Backup Reports using Azure Backup centre to understand and optimize backup storage growth, stopping backups for non-critical workloads or deleted VMs. This will help you to get an aggregated view of your entire estate from a backup perspective. This includes not only information on your backup items, but also resources that aren't configured for backup. This ensures that you never miss protecting critical data in your growing estate and your backups are optimized for non-critical workloads or deleted workloads. The recommendations are. Setup and configure the backup centre in the management subscription for the centralized backup reporting and dashboards. Configure the Backup Centre with backup Log analytics workspace in the management subscription to generate in-depth reports from all recovery vaults. Enable/create below reports. Forecasting of cloud storage consumed. Auditing of backups and restores. Identifying key trends at different levels of granularity. Backup compliance","title":"Reporting and Dashboard:"},{"location":"lzdesignsample1/designapproach/","text":"Design Approach \u2693\ufe0e The approach consists of addressing the critical design areas outlined in the Design document , one after the other. Design considerations and recommendations are considered against the requirements that are either stated in the Workload description or assumed where needed. The outcome are design decisions that then drive the solution design. Practically, the approach may be more iterative to address the possible inter-dependencies between these areas.","title":"Design Approach"},{"location":"lzdesignsample1/designapproach/#design-approach","text":"The approach consists of addressing the critical design areas outlined in the Design document , one after the other. Design considerations and recommendations are considered against the requirements that are either stated in the Workload description or assumed where needed. The outcome are design decisions that then drive the solution design. Practically, the approach may be more iterative to address the possible inter-dependencies between these areas.","title":"Design Approach"},{"location":"lzdesignsample1/eaenrol/","text":"Enterprise Enrolment & Azure AD Tenant \u2693\ufe0e Tasks to be completed: # Tasks 1 Enterprise Agreement (EA) enrollment 2 Azure Active Directory Tenant creation 3 Subscription creation Enterprise Agreement (EA) enrolment \u2693\ufe0e There are two licensing models as explained in the Solution Guidance document: EA (Enterprise Agreement) and CSP (Cloud Solution Provider). At this time Kyndryl is yet to adopt the CSP model and hence the pre-req is for the client to have an Enterprise Agreement enrollment. Validate that the account already has an EA agreement with Microsoft. If there is a need for Kyndryl to be involved in this EA enrollment process, reach out to Kyndryl-Microsoft Alliance representative Azure AD Tenant Creation \u2693\ufe0e It is possible that an AAD was already defined as part of the EA enrolment and initial subscription creation process. Validate with EA administrator if an AAD tenant already exist & confirm if the existing AAD tenant can be used for this new application, which is the most common scenario. If a new AAD tenant should be created because the current EA is for a totally different business, then EA administrator should create a new AAD tenant. For this use case, existing AAD tenant will be used. Subscription Creation \u2693\ufe0e Subscriptions should be created based on the Organization & Management Group design. EA Administrator should create subscriptions as per design recommendations from Organization & Management Group section. As per above design recommendations, below subscriptions should be created: Subscription Type Subscription Names (as defined in LZ design standards \u2013 Link to be added) Management subscription it-prod-management-eastus01 Connectivity subscription it-prod-connectivity-eastus01 Landing Zone subscription for the specific Web Application workload it-prod-app1-eastus01, it-dev-app1-eastus01","title":"1.Enterprise Enrollment & Azure AD Tenants"},{"location":"lzdesignsample1/eaenrol/#enterprise-enrolment-azure-ad-tenant","text":"Tasks to be completed: # Tasks 1 Enterprise Agreement (EA) enrollment 2 Azure Active Directory Tenant creation 3 Subscription creation","title":"Enterprise Enrolment &amp; Azure AD Tenant"},{"location":"lzdesignsample1/eaenrol/#enterprise-agreement-ea-enrolment","text":"There are two licensing models as explained in the Solution Guidance document: EA (Enterprise Agreement) and CSP (Cloud Solution Provider). At this time Kyndryl is yet to adopt the CSP model and hence the pre-req is for the client to have an Enterprise Agreement enrollment. Validate that the account already has an EA agreement with Microsoft. If there is a need for Kyndryl to be involved in this EA enrollment process, reach out to Kyndryl-Microsoft Alliance representative","title":"Enterprise Agreement (EA) enrolment"},{"location":"lzdesignsample1/eaenrol/#azure-ad-tenant-creation","text":"It is possible that an AAD was already defined as part of the EA enrolment and initial subscription creation process. Validate with EA administrator if an AAD tenant already exist & confirm if the existing AAD tenant can be used for this new application, which is the most common scenario. If a new AAD tenant should be created because the current EA is for a totally different business, then EA administrator should create a new AAD tenant. For this use case, existing AAD tenant will be used.","title":"Azure AD Tenant Creation"},{"location":"lzdesignsample1/eaenrol/#subscription-creation","text":"Subscriptions should be created based on the Organization & Management Group design. EA Administrator should create subscriptions as per design recommendations from Organization & Management Group section. As per above design recommendations, below subscriptions should be created: Subscription Type Subscription Names (as defined in LZ design standards \u2013 Link to be added) Management subscription it-prod-management-eastus01 Connectivity subscription it-prod-connectivity-eastus01 Landing Zone subscription for the specific Web Application workload it-prod-app1-eastus01, it-dev-app1-eastus01","title":"Subscription Creation"},{"location":"lzdesignsample1/govcomp/","text":"Azure governance \u2693\ufe0e Security baseline for beta customer \u2693\ufe0e For windows/Linux Operating systems Apply the GTS Standards tech specs and create a image and use that image to deploy the VM Publish this image as private gallery APPLY the following security initiative at root level o Azure Security Benchmark o ISO 27001 - o Azure CIS 1.3.0 o Beta customer initiative --. Custom initiative Define a custom initiative Allowed Storage Account SKUs (Deny): Allow only Premium SKU Allowed Resource Type (Deny): Defines the resource types that you can deploy. Its effect is to deny all resources that aren't part of this defined list. Allowed Locations (Deny): Restricts the available locations for new resources. Its effect is used to enforce your geo-compliance requirements. Allowed Virtual Machine SKUs (Deny): Specifies a set of virtual machine SKUs that you can deploy. Add a tag to resources (Modify): Applies a required tag and its default value if it's not specified by the deploy request. Not allowed resource types (Deny): Prevents a list of resource types from being deployed. Data Protection \u2693\ufe0e Data protection is one of the mandatory requirements for Beta customer this section covers \u00b7 Data at rest Encryption \u00b7 Data at in-transit encryption \u00b7 Data Isolation \u00b7 Data Redundancy Data redundancy \u2693\ufe0e When you create your storage account for platform services, select one of the following replication options: \u00b7 For production and Central services Geo-redundant storage (GRS) : Geo-redundant storage is enabled for your storage account by default when you create it. GRS maintains six copies of your data. With GRS, your data is replicated three times within the primary region. Data is also replicated three times in a secondary region hundreds of miles away from the primary region, providing the highest level of durability. In the event of a failure at the primary region, Azure Storage fails over to the secondary region. GRS helps ensure that your data is durable in two separate regions. Data at rest Encryption \u2693\ufe0e Use Azure Key Vault where they keys are generated One Key vault per subscription No subscription contributor should have access on the key vault, all the key vaults keys, certificates and secrets must be managed centrally. Use these keys to encrypt the VM disks, Storage account and recovery vault The following key vault requirements Use Premium SKUs where hardware-security-module-protected keys are required. Underlying hardware security modules (HSMs) are FIPS 140-2 Level 2 compliant Use a vault per application per subscription (Development, Production, management). Enable firewall and virtual network service endpoint on the vault to control access to the key vault. Use the platform-central Azure Monitor Log Analytics workspace to audit key, certificate, and secret usage within each instance of Key Vault. Lock down access to your subscription, resource group and Key Vaults (Azure RBAC) Turn on Firewall and VNET Azure Defender (Azure Security Center) is enabled for Azure Key Vault Enable Soft delete Enable Purge protection Data Encryption in Transit \u2693\ufe0e S2S VPN established in the HUB subscriptions provides the encryption, if the traffic is with in the Azure vNet peering is used which passes though Microsoft backup and Microsoft manages the security and this traffic never flows via internet Data Isolation \u2693\ufe0e Data segregation : Use Azure subscription to provide data segregation Tags \u2693\ufe0e Tag Name/Key Name Description value Example Name Required? Workload name Name of the workload the resource it supports Application Name Workload name = JEE Workload name= .NET Yes Environment Deployment Environment of this application, workload or service Env Environment = Prod, Environment = Dev, Yes Disaster recovery Business criticality of the Application DR DR=Yes DR= No Yes Business unit Top level vision of betacustomer that owns the subscription and workload , Cost center {number/Character} Business unit =EAP, business unit =IT01 Yes Owner Name Owner of the application, workload or service Owner Email Owner name = ambatia@us.ibm.com Yes Operations team Team accountable for day to day operations IBM, IBM_MCMS_Managed Yes Customer code Locks \u2693\ufe0e Apply \u201cCannot delete lock\u201d on production subscription resources Security logs \u2693\ufe0e The below table outlines the list of security logs that needs to send to Central log analytics workspace Log Type Is Security Log Log Specification EventHub retention ** Security Ops** Log Analytics retentions Subscription requirement AAD Log Yes -AuditLogs -SignInLogs -NonInteractiveUserSignInLogs -ServicePrincipalSignInLogs -ManagedIdentitySignInLogs -ProvisioningLogs Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) N/A Azure Activity Log Yes Administrative Security ServiceHealth Alert Recommendation Policy Autoscale ResourceHealth Standard tier with 1 day retention Log Analytics (logs collect to Security Ops workspace with 90 days retention) Check on Azure Portal with (90 days retention) Resource Metrics Check on Azure Portal with (90 days retention) Check on Azure Portal with (90 days retention) NSG Flow Log Yes v2 Log Analytics logs collect to Security Ops workspace with 30 days retention) Not required Storage account log Yes storageread storagewrite storagedelete Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required App Gateway Access Log and Firewall Log Yes ApplicationGatewayAccessLog ApplicationGatewayPerformanceLog ApplicationGatewayFirewallLog Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required Azure DDoS Protection Standard Yes DDoSProtectionNotifications DDosMitigationFlowLogs DDosMitigationReports Standard tier with 1-day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) No required Azure SQL Database (SQL and SQL Managed Instance) Audits logs Yes Audit, SQLSecurityAuditEvents Standard tier with 1-day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required Application Insights No Options for Security Ops to enable application insights in Log Analytics logs collect to Security Ops workspace with 30 days retention) App Owner add application insights in Subscription Log Analytics workspace per subscription if required","title":"7.Governance & Compliance"},{"location":"lzdesignsample1/govcomp/#azure-governance","text":"","title":"Azure  governance"},{"location":"lzdesignsample1/govcomp/#security-baseline-for-beta-customer","text":"For windows/Linux Operating systems Apply the GTS Standards tech specs and create a image and use that image to deploy the VM Publish this image as private gallery APPLY the following security initiative at root level o Azure Security Benchmark o ISO 27001 - o Azure CIS 1.3.0 o Beta customer initiative --. Custom initiative Define a custom initiative Allowed Storage Account SKUs (Deny): Allow only Premium SKU Allowed Resource Type (Deny): Defines the resource types that you can deploy. Its effect is to deny all resources that aren't part of this defined list. Allowed Locations (Deny): Restricts the available locations for new resources. Its effect is used to enforce your geo-compliance requirements. Allowed Virtual Machine SKUs (Deny): Specifies a set of virtual machine SKUs that you can deploy. Add a tag to resources (Modify): Applies a required tag and its default value if it's not specified by the deploy request. Not allowed resource types (Deny): Prevents a list of resource types from being deployed.","title":"Security baseline for beta customer"},{"location":"lzdesignsample1/govcomp/#data-protection","text":"Data protection is one of the mandatory requirements for Beta customer this section covers \u00b7 Data at rest Encryption \u00b7 Data at in-transit encryption \u00b7 Data Isolation \u00b7 Data Redundancy","title":"Data Protection"},{"location":"lzdesignsample1/govcomp/#data-redundancy","text":"When you create your storage account for platform services, select one of the following replication options: \u00b7 For production and Central services Geo-redundant storage (GRS) : Geo-redundant storage is enabled for your storage account by default when you create it. GRS maintains six copies of your data. With GRS, your data is replicated three times within the primary region. Data is also replicated three times in a secondary region hundreds of miles away from the primary region, providing the highest level of durability. In the event of a failure at the primary region, Azure Storage fails over to the secondary region. GRS helps ensure that your data is durable in two separate regions.","title":"Data redundancy"},{"location":"lzdesignsample1/govcomp/#data-at-rest-encryption","text":"Use Azure Key Vault where they keys are generated One Key vault per subscription No subscription contributor should have access on the key vault, all the key vaults keys, certificates and secrets must be managed centrally. Use these keys to encrypt the VM disks, Storage account and recovery vault The following key vault requirements Use Premium SKUs where hardware-security-module-protected keys are required. Underlying hardware security modules (HSMs) are FIPS 140-2 Level 2 compliant Use a vault per application per subscription (Development, Production, management). Enable firewall and virtual network service endpoint on the vault to control access to the key vault. Use the platform-central Azure Monitor Log Analytics workspace to audit key, certificate, and secret usage within each instance of Key Vault. Lock down access to your subscription, resource group and Key Vaults (Azure RBAC) Turn on Firewall and VNET Azure Defender (Azure Security Center) is enabled for Azure Key Vault Enable Soft delete Enable Purge protection","title":"Data at rest Encryption"},{"location":"lzdesignsample1/govcomp/#data-encryption-in-transit","text":"S2S VPN established in the HUB subscriptions provides the encryption, if the traffic is with in the Azure vNet peering is used which passes though Microsoft backup and Microsoft manages the security and this traffic never flows via internet","title":"Data Encryption in Transit"},{"location":"lzdesignsample1/govcomp/#data-isolation","text":"Data segregation : Use Azure subscription to provide data segregation","title":"Data Isolation"},{"location":"lzdesignsample1/govcomp/#tags","text":"Tag Name/Key Name Description value Example Name Required? Workload name Name of the workload the resource it supports Application Name Workload name = JEE Workload name= .NET Yes Environment Deployment Environment of this application, workload or service Env Environment = Prod, Environment = Dev, Yes Disaster recovery Business criticality of the Application DR DR=Yes DR= No Yes Business unit Top level vision of betacustomer that owns the subscription and workload , Cost center {number/Character} Business unit =EAP, business unit =IT01 Yes Owner Name Owner of the application, workload or service Owner Email Owner name = ambatia@us.ibm.com Yes Operations team Team accountable for day to day operations IBM, IBM_MCMS_Managed Yes Customer code","title":"Tags"},{"location":"lzdesignsample1/govcomp/#locks","text":"Apply \u201cCannot delete lock\u201d on production subscription resources","title":"Locks"},{"location":"lzdesignsample1/govcomp/#security-logs","text":"The below table outlines the list of security logs that needs to send to Central log analytics workspace Log Type Is Security Log Log Specification EventHub retention ** Security Ops** Log Analytics retentions Subscription requirement AAD Log Yes -AuditLogs -SignInLogs -NonInteractiveUserSignInLogs -ServicePrincipalSignInLogs -ManagedIdentitySignInLogs -ProvisioningLogs Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) N/A Azure Activity Log Yes Administrative Security ServiceHealth Alert Recommendation Policy Autoscale ResourceHealth Standard tier with 1 day retention Log Analytics (logs collect to Security Ops workspace with 90 days retention) Check on Azure Portal with (90 days retention) Resource Metrics Check on Azure Portal with (90 days retention) Check on Azure Portal with (90 days retention) NSG Flow Log Yes v2 Log Analytics logs collect to Security Ops workspace with 30 days retention) Not required Storage account log Yes storageread storagewrite storagedelete Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required App Gateway Access Log and Firewall Log Yes ApplicationGatewayAccessLog ApplicationGatewayPerformanceLog ApplicationGatewayFirewallLog Standard tier with 1 day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required Azure DDoS Protection Standard Yes DDoSProtectionNotifications DDosMitigationFlowLogs DDosMitigationReports Standard tier with 1-day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) No required Azure SQL Database (SQL and SQL Managed Instance) Audits logs Yes Audit, SQLSecurityAuditEvents Standard tier with 1-day retention Log Analytics logs collect to Security Ops workspace with 30 days retention) Assign permission via Resource-context Mode to the Security Ops 's workspace if required Application Insights No Options for Security Ops to enable application insights in Log Analytics logs collect to Security Ops workspace with 30 days retention) App Owner add application insights in Subscription Log Analytics workspace per subscription if required","title":"Security logs"},{"location":"lzdesignsample1/idaccess/","text":"Identity & Access Management \u2693\ufe0e Tasks to be completed: # Tasks 1 Choose appropriate identity model 2 Choose appropriate authentication method & establish SSO 3 Implement Roles needed for Platform & Workload management 4 Implement best practice standards for IAM Choose Identity Model \u2693\ufe0e Assumption is that an on-premise Active Directory exists and hence an Hybrid Identity model will be used Choose Authentication Method \u2693\ufe0e Since on-premise AD exists, [ Pass-Through authentication with Azure AD ] ( https://docs.microsoft.com/en-us/azure/active-directory/hybrid/how-to-connect-pta ) will be used for this enterprise scenario. This authentication method Azure AD will use the on-premise system for authentication, such as AD or any third-party system, to validate the user's sign-in Implement Roles for Platform & Workload Management \u2693\ufe0e \u2022 Create below Azure AD groups, and assign below roles and scope to each group Persona (=Groups in Azure AD) Responsibilities Azure Roles (Built-In or Custom) Scope (Set of resources this access applies to) Azure Platform Admin (Group) Mgmt Group & Subscription Lifecycle Mgmt \u2022 Owner NOTE: There should be more than one person associated with this role, and not more than three All Subscriptions Identity & Access Admin Assign permissions to Users/Groups/Service Principals \u2022 User Access Administrator, AAD Role: \u201cUser Administrator\u201d All Subscriptions Patching & Compliance Compute \u2022 Virtual Machine Contributor All Subscriptions Security Admin Horizontal security responsibility across the entire Azure estate and the Azure Key Vault purge policy Security Admin,Sentinel Contributor,Sentinel Automation Contributor All Subscriptions Storage Admin Management of Storage Accounts Storage Account Contributor All Subscriptions Network Admin Connectivity management: Virtual networks, UDRs, NSGs, NVAs, VPN, Azure ExpressRoute, and others \u2022 Network Contributor All Subscriptions Backup/Restore Admin Account wide management of Backup and Restore \u2022 Backup Contributor All Subscriptions VM Admin Account wide management of Backup and Restore \u2022 Virtual Machine Contributor All Subscriptions SysOps/ Distributed Ops/AppOps Operational view of platform and application elements \u2022 Reader \u2022 it-prod-app1-eastus Automation & DevOps Admin (Group), Contains User and Service Principals Use Automation for Day 1 provisioning and use automation for Day 2 operations \u2022 Contributor All Subscriptions App Owners Application Owners, to create and manage workloads \u2022 Contributor \u2022 it-prod-app1-eastus, \u2022 it-dev-app1-eastus Implement Azure AD for application authentication \u2693\ufe0e It is important to use centralized access control for applications too as Azure AD provides native options for the same, explained below Source Destination Authentication Application (J2EE & .NET) SQL Managed Instance Service System to Service authentication using Managed Identity \u2013 System Assigned Users Application (J2EE & .NET) \u2022 Register the app(s) in Azure AD and modify the app to point to Azure AD for authentication, Refer to this quickstart for configuring .NET app to use Azure AD authentication Implement IAM best practices \u2693\ufe0e a. Enable Multi-factor authentication for all users, through risk based conditional access policies. b. Enable Monitoring of Azure Active Directory. \u2022 This is done by configuring \u201cDiagnostic Setting\u201d within Azure AD, select all logs & enabling at least Storage & Log Analytics workspace destinations. \u2022 Log Workspace & Storage destination should be the ones defined for security (Rajesh\u2019s Link to be added). Enabling Primary & Secondary Controls \u2693\ufe0e Primary Controls : There is no mechanism to enable primary controls through Azure native tools or services. Secondary Controls : Secondary controls shall be enabled through Azure AD Access Reviews feature. There is no specific guidance on what and how on this topic in this release, and will be provided in a future release. AD License Requirements \u2693\ufe0e In order to enable all above controls for identitiy & access management, specifically for risk based conditional access policy & for Azure AD reporting and monitoring, Azure AD Premium P2 license is required","title":"2.Identity & Access Management"},{"location":"lzdesignsample1/idaccess/#identity-access-management","text":"Tasks to be completed: # Tasks 1 Choose appropriate identity model 2 Choose appropriate authentication method & establish SSO 3 Implement Roles needed for Platform & Workload management 4 Implement best practice standards for IAM","title":"Identity &amp; Access Management"},{"location":"lzdesignsample1/idaccess/#choose-identity-model","text":"Assumption is that an on-premise Active Directory exists and hence an Hybrid Identity model will be used","title":"Choose Identity Model"},{"location":"lzdesignsample1/idaccess/#choose-authentication-method","text":"Since on-premise AD exists, [ Pass-Through authentication with Azure AD ] ( https://docs.microsoft.com/en-us/azure/active-directory/hybrid/how-to-connect-pta ) will be used for this enterprise scenario. This authentication method Azure AD will use the on-premise system for authentication, such as AD or any third-party system, to validate the user's sign-in","title":"Choose Authentication Method"},{"location":"lzdesignsample1/idaccess/#implement-roles-for-platform-workload-management","text":"\u2022 Create below Azure AD groups, and assign below roles and scope to each group Persona (=Groups in Azure AD) Responsibilities Azure Roles (Built-In or Custom) Scope (Set of resources this access applies to) Azure Platform Admin (Group) Mgmt Group & Subscription Lifecycle Mgmt \u2022 Owner NOTE: There should be more than one person associated with this role, and not more than three All Subscriptions Identity & Access Admin Assign permissions to Users/Groups/Service Principals \u2022 User Access Administrator, AAD Role: \u201cUser Administrator\u201d All Subscriptions Patching & Compliance Compute \u2022 Virtual Machine Contributor All Subscriptions Security Admin Horizontal security responsibility across the entire Azure estate and the Azure Key Vault purge policy Security Admin,Sentinel Contributor,Sentinel Automation Contributor All Subscriptions Storage Admin Management of Storage Accounts Storage Account Contributor All Subscriptions Network Admin Connectivity management: Virtual networks, UDRs, NSGs, NVAs, VPN, Azure ExpressRoute, and others \u2022 Network Contributor All Subscriptions Backup/Restore Admin Account wide management of Backup and Restore \u2022 Backup Contributor All Subscriptions VM Admin Account wide management of Backup and Restore \u2022 Virtual Machine Contributor All Subscriptions SysOps/ Distributed Ops/AppOps Operational view of platform and application elements \u2022 Reader \u2022 it-prod-app1-eastus Automation & DevOps Admin (Group), Contains User and Service Principals Use Automation for Day 1 provisioning and use automation for Day 2 operations \u2022 Contributor All Subscriptions App Owners Application Owners, to create and manage workloads \u2022 Contributor \u2022 it-prod-app1-eastus, \u2022 it-dev-app1-eastus","title":"Implement Roles for Platform &amp; Workload Management"},{"location":"lzdesignsample1/idaccess/#implement-azure-ad-for-application-authentication","text":"It is important to use centralized access control for applications too as Azure AD provides native options for the same, explained below Source Destination Authentication Application (J2EE & .NET) SQL Managed Instance Service System to Service authentication using Managed Identity \u2013 System Assigned Users Application (J2EE & .NET) \u2022 Register the app(s) in Azure AD and modify the app to point to Azure AD for authentication, Refer to this quickstart for configuring .NET app to use Azure AD authentication","title":"Implement Azure AD for application authentication"},{"location":"lzdesignsample1/idaccess/#implement-iam-best-practices","text":"a. Enable Multi-factor authentication for all users, through risk based conditional access policies. b. Enable Monitoring of Azure Active Directory. \u2022 This is done by configuring \u201cDiagnostic Setting\u201d within Azure AD, select all logs & enabling at least Storage & Log Analytics workspace destinations. \u2022 Log Workspace & Storage destination should be the ones defined for security (Rajesh\u2019s Link to be added).","title":"Implement IAM best practices"},{"location":"lzdesignsample1/idaccess/#enabling-primary-secondary-controls","text":"Primary Controls : There is no mechanism to enable primary controls through Azure native tools or services. Secondary Controls : Secondary controls shall be enabled through Azure AD Access Reviews feature. There is no specific guidance on what and how on this topic in this release, and will be provided in a future release.","title":"Enabling Primary &amp; Secondary Controls"},{"location":"lzdesignsample1/idaccess/#ad-license-requirements","text":"In order to enable all above controls for identitiy & access management, specifically for risk based conditional access policy & for Azure AD reporting and monitoring, Azure AD Premium P2 license is required","title":"AD License Requirements"},{"location":"lzdesignsample1/lzsample/","text":"Section Title Here \u2693\ufe0e Add content 1.1 - Adding Links and Highlighting \u2693\ufe0e Type content here, highlight where needed. Add links where needed CAF 1.1.1 - How to add images \u2693\ufe0e Add Pictures to /images folder Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size: 1.1.1.1 - How to Add TABLE \u2693\ufe0e Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"Section Title Here"},{"location":"lzdesignsample1/lzsample/#section-title-here","text":"Add content","title":"Section Title Here"},{"location":"lzdesignsample1/lzsample/#11-adding-links-and-highlighting","text":"Type content here, highlight where needed. Add links where needed CAF","title":"1.1 - Adding Links and Highlighting"},{"location":"lzdesignsample1/lzsample/#111-how-to-add-images","text":"Add Pictures to /images folder Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size:","title":"1.1.1 - How to add images"},{"location":"lzdesignsample1/lzsample/#1111-how-to-add-table","text":"Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"1.1.1.1 - How to Add TABLE"},{"location":"lzdesignsample1/monitoringmgmt/","text":"Monitoring \u2693\ufe0e Enable monitoring across Infrastructure, Applications, Security, other Azure services. Enable Azure Monitor metrics collection for Azure VMs, VMSS, storage, Network and PaaS components Enable Azure Security Center monitoring Enable VM Insights for monitoring the performance and health of virtual machines and virtual machine scale sets for their running processes and dependencies on other resources. Define dynamic thresholds for Azure alerts Define Monitoring Dashboards for centralized view across the platform. Enable Application performance and health monitoring for both IaaS and PaaS resources Enable monitoring of application components in relation to infra and service health, monitor transactions across components, and enable response to failures Log Analytics \u2693\ufe0e Create one log analytics workspace named log-landingzone in the Management subscription for storing metrics and logs for Prod app1, Prod app2, platform logs and security & audit logs and for querying, analyzing, and reporting data for troubleshooting and deep diagnostics. Proper RBAC and workspace context for central admin team and resource-context for resource teams need to be enabled for granular level logs access. Enable log monitoring for VMs, VMSS, storage, Networking, and PaaS components Enable diagnostics settings for activity Logs for VMs, VMSS, storage, network and PaaS resources sent to Log Analytics Store all monitoring, diagnostic and AD logs for 90 days and then move to Azure storage for longer audit purposes. Metrics will be captured from Azure VM, Application, SQL database and other resources, sent to metrics time series db, and respective application and underlying infrastructure logs will be sent to log-app1-prod1-useast1-landingzone and log-app2-prod1-useast1-landingzone for app1 and app2 respectively. Azure common platform logs will be sent to log-plat-central-global-common log analytics workspace while security, audit and access logs will be sent to log-sec-aud-acc-global-common log analytics workspace. Metrics and Logs will be used to derive insights and will create actions based on alert rules. Events will be sent to event hub and long term logs will be sent to the archival storage. Management \u2693\ufe0e Leverage Azure policies for setting conventions for resources to enforce organization-wide settings to ensure consistent policy adherence and fast violation detection. Enable Azure status reports for identifying Azure maintenance or other Azure common issues. Enable Windows and Linux VMs patching using Update Management configurations via Azure Policy. Create 4 automation accounts for run book and enable automated notifications for respective app, platform and audit teams. Enable resource locks for accidental VMs deletion Enable resource-centric access control mode with granular Azure RBAC. Alerts generated through Azure Monitor Metrics & Log Analytics should be forwarded to Kyndryl\u2019s Netcool Event Management system","title":"5.Monitoring & Management"},{"location":"lzdesignsample1/monitoringmgmt/#monitoring","text":"Enable monitoring across Infrastructure, Applications, Security, other Azure services. Enable Azure Monitor metrics collection for Azure VMs, VMSS, storage, Network and PaaS components Enable Azure Security Center monitoring Enable VM Insights for monitoring the performance and health of virtual machines and virtual machine scale sets for their running processes and dependencies on other resources. Define dynamic thresholds for Azure alerts Define Monitoring Dashboards for centralized view across the platform. Enable Application performance and health monitoring for both IaaS and PaaS resources Enable monitoring of application components in relation to infra and service health, monitor transactions across components, and enable response to failures","title":"Monitoring"},{"location":"lzdesignsample1/monitoringmgmt/#log-analytics","text":"Create one log analytics workspace named log-landingzone in the Management subscription for storing metrics and logs for Prod app1, Prod app2, platform logs and security & audit logs and for querying, analyzing, and reporting data for troubleshooting and deep diagnostics. Proper RBAC and workspace context for central admin team and resource-context for resource teams need to be enabled for granular level logs access. Enable log monitoring for VMs, VMSS, storage, Networking, and PaaS components Enable diagnostics settings for activity Logs for VMs, VMSS, storage, network and PaaS resources sent to Log Analytics Store all monitoring, diagnostic and AD logs for 90 days and then move to Azure storage for longer audit purposes. Metrics will be captured from Azure VM, Application, SQL database and other resources, sent to metrics time series db, and respective application and underlying infrastructure logs will be sent to log-app1-prod1-useast1-landingzone and log-app2-prod1-useast1-landingzone for app1 and app2 respectively. Azure common platform logs will be sent to log-plat-central-global-common log analytics workspace while security, audit and access logs will be sent to log-sec-aud-acc-global-common log analytics workspace. Metrics and Logs will be used to derive insights and will create actions based on alert rules. Events will be sent to event hub and long term logs will be sent to the archival storage.","title":"Log Analytics"},{"location":"lzdesignsample1/monitoringmgmt/#management","text":"Leverage Azure policies for setting conventions for resources to enforce organization-wide settings to ensure consistent policy adherence and fast violation detection. Enable Azure status reports for identifying Azure maintenance or other Azure common issues. Enable Windows and Linux VMs patching using Update Management configurations via Azure Policy. Create 4 automation accounts for run book and enable automated notifications for respective app, platform and audit teams. Enable resource locks for accidental VMs deletion Enable resource-centric access control mode with granular Azure RBAC. Alerts generated through Azure Monitor Metrics & Log Analytics should be forwarded to Kyndryl\u2019s Netcool Event Management system","title":"Management"},{"location":"lzdesignsample1/nwtopo/","text":"\u2693\ufe0e Network Topology and Connectivity \u2693\ufe0e This document focuses on the design of the network for the proposed workloads. The first section focuses on applying the design considerations and recommendations presented in the design documentation. The remaining sections focus on the design for each of the VNets that are identified as needed from the above effort. For each VNet, the subnets required, the resources needed to be hosted in each subnet and their configuration, the User Defined Routes (UDRs) to be configured for each subnet and Network Security Groups (NSGs) to be configured for each subnet are described. It is to be noted that the intent is to demonstrate the use of the proposed design approach and not necessarily to produce a rigourous and working solution design. So while UDRs and NSGs are detailed, their correctness needs to be validated through a Proof of Concept. Design Considerations and Recommendations \u2693\ufe0e Plan for IP addressing \u2693\ufe0e \u2022 It is envisaged three VNets will be needed to host the workloads -- one for both the applications in Dev/Test and one for each application in Production. \u2022 IP address ranges for the VNets in the proposed solution will be chosen such that there is no overlap across Azure regions and customer premises. \u2022 The private IP address range of 192.168.0.0/16 will be used to assign IP address ranges for the VNets in cloud. \u2022 It is assumed, given the size of the proposed environments and the associated workloads, that IPv4 private IP address ranges will suffice and IPv6 will not be needed. \u2022 The size of the IP address ranges assigned to the VNets in the proposed solution will be chosen as to not waste IP addresses while at the same time preventing the possibility of IP address range reallocation in the future that would be disruptive. DNS for on-premise and Azure resources \u2693\ufe0e \u2022 Name resolution for Internet-facing domain will be provided by the customer premises network. Azure DNS support for such domains will not be used. \u2022 A custom private DNS zone (azure.customer.com, for example) along with auto-registration of all the Azure resources will be used. All VNets will be linked to this zone so lookups can be performed from anywhere within the environment. \u2022 For lookups performed from customer premises, conditional forwarding will be configured on the DNS servers in the customer premises. Azure Firewall will be the forwarder that will receive such lookup requests from the customer premises and perform the lookup against Azure private DNS. \u2022 Kubernetes clusters in Azure are not in scope for this design effort. \u2022 For this exercise, lookups from the cloud for names of resources in the customer premises is not envisaged. If needed this can be achieved through custom DNS resolvers that conditionally forward lookup requests from the cloud to the customer premise DNS servers. The custom DNS resolver then needs to be configured as the DNS resolver for all Azure resources. The same DNS resolver can forward lookup requests from both Azure resources and customer premises to Azure DNS as well in which case the suggested DNS configuration on the Azure Firewall can be dispensed with. Define an Azure network topology \u2693\ufe0e Virtual WAN is a Microsoft-managed service used to meet large-scale interconnectivity requirements. It is suitable for \u2022 Connecting VNets in multiple Azure regions and multiple customer premises via different transports. \u2022 Routing between customer sites that connect to the cloud network using different transports. A traditional hub-and-spoke network topology helps build customized secure large-scale networks in Azure with routing and security managed by the customer. It is suitable for \u2022 When a full mesh network between the Azure VNets and multiple customer premises is not needed \u2022 Only a few customer sites (typically less than 30) need to be connected to Azure cloud \u2022 Custom routing is needed between the VNets and customer premises For the solution under consideration, there is only one customer premise. Custom routing will provide flexibility in configuring the network. Therefore, the hub-and-spoke network topology is preferred for this solution. Network Design with Virtual Network gateways \u2693\ufe0e From the requirements, on-premise users needs to reach workloads in different VNets spread across two subscriptions. Also, the management services in the VNets spread across three subscriptions will need to talk to one another as well as to the Azure resources hosting the workloads. So a hub-and-spoke with VNets peered to a Hub VNet hosting a Virtual Network gateway that provides transitive connectivity as needed is the choice. For isolation between the three tiers of the proposed web applications that are hosted in a VNet, user defined routing will be implemented. An Azure DDoS Protection standard protection plan will be shared across all VNets in a single Azure AD Tenant to protect resources with public IP addresses. Border Gateway Protocol (BGP) will be enabled on the Virtual Network gateways. Connectivity to Azure \u2693\ufe0e The purpose of connecting to the Cloud environment from on-premise is to manage the VMs and application configuration. High volume data transfers and transactions are not expected, and bandwidth should not exceed 10Gbps. So, the VPN Gateway providing both site-to-site and user-to-site connectivity is chosen as the Virtual Network Gateway between on-premise and the Cloud environment considering the lower cost reduction and faster implementation. As and when details are provided on the bandwidth and performance requirements for the connectivity between customer premise and the cloud, the right SKU for the VPN gateways can be selected. For higher resilience, deploy zone-redundant VPN gateways (where available). Connectivity to Azure PaaS Services \u2693\ufe0e Virtual network injection for the Azure SQL service is chosen to make it available from within the VNet hosting the rest of the application stack. Azure PaaS services that have been injected into a virtual network still perform management plane operations by using public IP addresses. Ensure that this communication is locked down within the virtual network by using UDRs and NSGs. Private Link will not be used in this solution. There is no need to make the Azure Paas Services accessible from the customer premises. There is no need to make Azure PaaS services available through service endpoints. Private Link and DNS Integration \u2693\ufe0e The only Azure PaaS service used in this solution is the SQL Server Managed Instance. This service will be deployed through VNet integration injection and Private Link will not be used. Therefore there is no need to design for Private Link integration with DNS. Plan for Inbound and Outbound Connectivity \u2693\ufe0e Use Azure Firewall to govern Azure outbound traffic to the internet support Non-HTTP/S inbound connections provide isolation between the Dev/Test and Production environment provide traffic filtering between on-premise and the cloud environments. Forced tunneling of Internet traffic from the cloud via the customer premises will not be considered. Plan for Application Delivery \u2693\ufe0e Azure Application Gateway v2 with WAF enabled will be deployed in the VNet of the workload subscription along with applications to secure inbound HTTP/S connections. A DDoS standard protection plan for all public IP addresses will be enabled for the VNets in the Production subscription. Plan for Landing Zone Network Segmentation \u2693\ufe0e NSGs will be used to allow only the necessary traffic and deny all others between the subnets within a VNet as well as across VNets. UDRs will be used to route all outbound flows/connections through a centralized Azure Firewall. Since the proposed design proposes to deploy each tier of an application in its own subnet, Application Service Groups (ASGs) are not necessary. Plan for Network Encryption \u2693\ufe0e The proposed solution will use VPN gateways which support IPSEC tunnels end-to-end between customer premises and the cloud environment. Plan for Traffic Inspection \u2693\ufe0e The proposed solution will rely on Network Watcher primarily to monitor NSG flows. Given most of the Internet traffic of interest is HTTPS, support for logging of this data in Azure Firewall is limited and will not be used. Hub VNet Design \u2693\ufe0e The Hub VNet will host the network services of Bastion, VPN gateway and Azure Firewall. Note that each of the network elements occupies its own subnet. The diagram below illustrates. The Hub VNet will be peered with the VNets hosting the workloads so routes are set up between all the subnets in all the VNets. The VPN gateway is configured to establish IPSEC tunnels with peers in the customer premise. It is route based so it can be configured to use BGP for dynamic routing. To use the Firewall for routing and reduce the load on the VPN gateway, UDR is set up for the subnet hosting the VPN gateway making the Firewall the next hop for all the Workload VNets. The Firewall also functions as a DNS Proxy for lookups on the Azure private DNS from the customer premise. The paths for some of the common traffic flows are given below -- \u2022 VM access via Portal/Bastion -- Portal <-> Bastion <-> VM (via VNET peering). The Bastion acts as a reverse proxy so the return traffic from the VM is routed back to the Bastion. \u2022 On-premise access to VM -- On-premise <-> VPN gateway <-> Firewall <-> VM. The return traffic is forced to go through the Firewall via UDR for the VM subnet. \u2022 VM access to Internet -- VM <-> Firewall via UDR for the VM subnet. \u2022 On-premise access to application on VM -- On-premise <-> VPN gateway <-> Firewall <-> Application Gateway <-> application on VM. The return traffic uses the same path in reverse. \u2022 Internet access to application on VM -- Internet <-> Application Gateway <-> application on VM. The return traffic uses the same path in reverse. VNet Design for .NET Application -- Production \u2693\ufe0e This section provides a high-level design for the VNet hosting the production level .NET application. The diagram below illustrates. The VNet is composed of 5 subnets. Subnet-Ext LB and Subnet-Int LB host application gateways that load balance traffic at the application/L7 layer across servers downstream for them. Subnet-Web hosts Web servers. Subnet-App hosts Application servers. Subnet-Database hosts Azure SQL Managed Instance. Subnet-Ext LB \u2693\ufe0e This subnet hosts an instance of the Application Gateway with the Web Application Firewall. This gateway is L7 aware and performs load distribution and balancing across downstream Web servers. Some of the main configuration details are \u2022 This is both Internet facing and customer premise facing and has both a public and private virtual IP address \u2022 Listeners for both Internet and on-premise traffic are configured to run health probes and load balance across a set of Web servers hosted in the Subnet-Web that is discussed next. \u2022 SSL termination will be configured on the Application Server. SSL to Web servers can be provided if customer desires it. \u2022 WAF is to be configured with separate policies for Internet and customer premises traffic. Subnet-Ext LB UDR \u2693\ufe0e Azure resources in this subnet need to access only the Web servers in the Subnet-Web and no other subnet. UDR is configured to restrict the paths to and from this subnet accordingly. The main routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to default (0.0.0.0/0) via Internet. This covers the Internet traffic. \u2022 Enable routes to customer premise networks via the Azure Firewall in the Connectivity VNet subscription. This covers the customer premise traffic. \u2022 Enable route to Subnet-Web via the VNet gateway. Subnet-Ext LB NSGs \u2693\ufe0e It is desirable to restrict traffic to certain protocols and ports for the sources and destinations permitted by the routes in UDR. The main NSG rules are \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Internet and customer premises (0.0.0.0/0). \u2022 Allow outgoing TCP traffic only to ports 80 and 443 in the Subnet-Web. \u2022 All other ports and protocols are to be denied. \u2022 NOTE -- It is assumed Azure management will use ports 80 and 443 on the Application gateway. If that is not the case it needs to be determined which ports are used and a rule is to be created to open those ports. Subnet-Web \u2693\ufe0e This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-Web UDR \u2693\ufe0e Azure resources in this subnet need to access both the Application Gateway and downstream Application Gateway. A path to the Bastion host as also a path to the Internet is needed. A route is also needed to the customer premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Ext LB. \u2022 Enable route to the Subnet-Int LB. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-Web NSGs \u2693\ufe0e Application data flows from the Application gateway and to the downstream Internal Load Balancer need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Ext LB. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-Int LB. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet. Subnet-Int LB \u2693\ufe0e This subnet hosts an Application Gateway that only has a private virtual IP address. It load balances requests from the Web servers across Application servers. Subnet-Int LB UDR \u2693\ufe0e Routes are needed to the Web servers and Application servers. Routes are also needed to the Internet for possible management traffic. \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-Int LB NSGs \u2693\ufe0e Application data flows from the Web servers and to the downstream Application servers need to be permitted on ports 80 and 443. If Azure management uses the same ports, these ports need to be accessible from the Internet as well. \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Subnet-Web. \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Internet. \u2022 Allow outgoing TCP traffic only to ports 80 and 443 in the Subnet-App. \u2022 All other ports and protocols are to be denied. \u2022 NOTE -- It is assumed Azure management will use ports 80 and 443 on the Application gateway. If that is not the case it needs to be determined which ports are used and a rule is to be created to open those ports. Subnet-App \u2693\ufe0e This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-App UDR \u2693\ufe0e Routes are needed to the Internal Application gateway and to the Database service. A path to the Bastion host is also needed. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Int LB . \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-App NSGs \u2693\ufe0e Application data flows from the internal Application gateway need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Int LB. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet. Subnet-Database \u2693\ufe0e This subnet is dedicated to the hosting of Managed Instance SQL service. This is a PaaS service that is injected into the workload VNet to be available to the applications in that VNet. The configuration mainly consists of ensuring the subnet is dedicated to the PaaS service and delegation of the subnet to Microsoft.Sql/managedInstances resource provider. When the Managed Instance SQL service is instantiated a network intent policy is implemented by Azure on the VNet infrastructure elements which prevents any misconfiguration that could block operation of the service. Azure will also configure the routes and NSGs needed for management traffic to and from the service. More details on connectivity for Azure SQL Server Managed Instance can be obtained from Connectivity architecture for Azure SQL Managed Instance . Subnet-Database UDR \u2693\ufe0e Routes are needed to the Application servers. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. Enable route to the Subnet-App. Enable route to the customer premises via the Azure Firewall. Enable route to the default (0.0.0.0/0) via the Internet. Azure will configure the routes needed for the PaaS service to access other Azure services such as monitoring, storage, and backup. Subnet-Database NSGs \u2693\ufe0e Application data flows to the Database need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. Allow incoming traffic to the on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. NOTE - It needs to be determined if traffic to and from other ports needs to be explicitly denied or whether the Azure configuration of the SQL Managed Instance service will address this. VNet Design for J2EE Application -- Production \u2693\ufe0e This section provides a high-level design for the VNet hosting the production level J2EE application. The diagram below illustrates. The design is pretty much the same as the .NET design except that there is no Internal Load Balancer between the Web servers and the App Servers. The affected subnets are the Subnet-Web and the Subnet-App the changes for which are described below. Subnet-Web \u2693\ufe0e This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-Web UDR \u2693\ufe0e Azure resources in this subnet need to access both the Application Gateway and downstream Application servers. A path to the Bastion host as well as a path to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Ext LB. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-Web NSGs \u2693\ufe0e Application data flows from the Application gateway and to the downstream Application servers need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Ext LB. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-App. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet. Subnet-App \u2693\ufe0e This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-App UDR \u2693\ufe0e Routes are needed to the Web servers and to the Database service. A path to the Bastion host is also needed. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-App NSGs \u2693\ufe0e Application data flows from the Web servers need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Web. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet. VNet Design for .NET and J2EE Applications -- Dev/Test \u2693\ufe0e This section provides a high-level design for the VNet hosting the Dev/Test level J2EE application and .NET application. The diagram below illustrates. The design is much simplified with single servers at each of the Web and Application tiers for both the J2EE and .NET incarnations. As a result, the load balancers have been dispensed with. Both the applications share the single SQL Managed Instance. The affected subnets are the Subnet-Web and the Subnet-App the changes for which are described below. Subnet-Web \u2693\ufe0e This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-Web UDR \u2693\ufe0e Azure resources in this subnet need to access the customer premises, the Internet and downstream Application servers. A path to the Bastion host as well as to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-Web NSGs \u2693\ufe0e Application data flows from the customer premises and the Internet to the downstream Application servers need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Internet. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the customer premises. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premises. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-App. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet. Subnet-App \u2693\ufe0e This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific. Subnet-App UDR \u2693\ufe0e Routes are needed to the Web servers and to the Database service. A path to the Bastion host as well as to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet. Subnet-App NSGs \u2693\ufe0e Application data flows from the Web servers need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Web. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"4.Network Topology & Connectivity"},{"location":"lzdesignsample1/nwtopo/#_1","text":"","title":""},{"location":"lzdesignsample1/nwtopo/#network-topology-and-connectivity","text":"This document focuses on the design of the network for the proposed workloads. The first section focuses on applying the design considerations and recommendations presented in the design documentation. The remaining sections focus on the design for each of the VNets that are identified as needed from the above effort. For each VNet, the subnets required, the resources needed to be hosted in each subnet and their configuration, the User Defined Routes (UDRs) to be configured for each subnet and Network Security Groups (NSGs) to be configured for each subnet are described. It is to be noted that the intent is to demonstrate the use of the proposed design approach and not necessarily to produce a rigourous and working solution design. So while UDRs and NSGs are detailed, their correctness needs to be validated through a Proof of Concept.","title":"Network Topology and Connectivity"},{"location":"lzdesignsample1/nwtopo/#design-considerations-and-recommendations","text":"","title":"Design Considerations and Recommendations"},{"location":"lzdesignsample1/nwtopo/#plan-for-ip-addressing","text":"\u2022 It is envisaged three VNets will be needed to host the workloads -- one for both the applications in Dev/Test and one for each application in Production. \u2022 IP address ranges for the VNets in the proposed solution will be chosen such that there is no overlap across Azure regions and customer premises. \u2022 The private IP address range of 192.168.0.0/16 will be used to assign IP address ranges for the VNets in cloud. \u2022 It is assumed, given the size of the proposed environments and the associated workloads, that IPv4 private IP address ranges will suffice and IPv6 will not be needed. \u2022 The size of the IP address ranges assigned to the VNets in the proposed solution will be chosen as to not waste IP addresses while at the same time preventing the possibility of IP address range reallocation in the future that would be disruptive.","title":"Plan for IP addressing"},{"location":"lzdesignsample1/nwtopo/#dns-for-on-premise-and-azure-resources","text":"\u2022 Name resolution for Internet-facing domain will be provided by the customer premises network. Azure DNS support for such domains will not be used. \u2022 A custom private DNS zone (azure.customer.com, for example) along with auto-registration of all the Azure resources will be used. All VNets will be linked to this zone so lookups can be performed from anywhere within the environment. \u2022 For lookups performed from customer premises, conditional forwarding will be configured on the DNS servers in the customer premises. Azure Firewall will be the forwarder that will receive such lookup requests from the customer premises and perform the lookup against Azure private DNS. \u2022 Kubernetes clusters in Azure are not in scope for this design effort. \u2022 For this exercise, lookups from the cloud for names of resources in the customer premises is not envisaged. If needed this can be achieved through custom DNS resolvers that conditionally forward lookup requests from the cloud to the customer premise DNS servers. The custom DNS resolver then needs to be configured as the DNS resolver for all Azure resources. The same DNS resolver can forward lookup requests from both Azure resources and customer premises to Azure DNS as well in which case the suggested DNS configuration on the Azure Firewall can be dispensed with.","title":"DNS for on-premise and Azure resources"},{"location":"lzdesignsample1/nwtopo/#define-an-azure-network-topology","text":"Virtual WAN is a Microsoft-managed service used to meet large-scale interconnectivity requirements. It is suitable for \u2022 Connecting VNets in multiple Azure regions and multiple customer premises via different transports. \u2022 Routing between customer sites that connect to the cloud network using different transports. A traditional hub-and-spoke network topology helps build customized secure large-scale networks in Azure with routing and security managed by the customer. It is suitable for \u2022 When a full mesh network between the Azure VNets and multiple customer premises is not needed \u2022 Only a few customer sites (typically less than 30) need to be connected to Azure cloud \u2022 Custom routing is needed between the VNets and customer premises For the solution under consideration, there is only one customer premise. Custom routing will provide flexibility in configuring the network. Therefore, the hub-and-spoke network topology is preferred for this solution.","title":"Define an Azure network topology"},{"location":"lzdesignsample1/nwtopo/#network-design-with-virtual-network-gateways","text":"From the requirements, on-premise users needs to reach workloads in different VNets spread across two subscriptions. Also, the management services in the VNets spread across three subscriptions will need to talk to one another as well as to the Azure resources hosting the workloads. So a hub-and-spoke with VNets peered to a Hub VNet hosting a Virtual Network gateway that provides transitive connectivity as needed is the choice. For isolation between the three tiers of the proposed web applications that are hosted in a VNet, user defined routing will be implemented. An Azure DDoS Protection standard protection plan will be shared across all VNets in a single Azure AD Tenant to protect resources with public IP addresses. Border Gateway Protocol (BGP) will be enabled on the Virtual Network gateways.","title":"Network Design with Virtual Network gateways"},{"location":"lzdesignsample1/nwtopo/#connectivity-to-azure","text":"The purpose of connecting to the Cloud environment from on-premise is to manage the VMs and application configuration. High volume data transfers and transactions are not expected, and bandwidth should not exceed 10Gbps. So, the VPN Gateway providing both site-to-site and user-to-site connectivity is chosen as the Virtual Network Gateway between on-premise and the Cloud environment considering the lower cost reduction and faster implementation. As and when details are provided on the bandwidth and performance requirements for the connectivity between customer premise and the cloud, the right SKU for the VPN gateways can be selected. For higher resilience, deploy zone-redundant VPN gateways (where available).","title":"Connectivity to Azure"},{"location":"lzdesignsample1/nwtopo/#connectivity-to-azure-paas-services","text":"Virtual network injection for the Azure SQL service is chosen to make it available from within the VNet hosting the rest of the application stack. Azure PaaS services that have been injected into a virtual network still perform management plane operations by using public IP addresses. Ensure that this communication is locked down within the virtual network by using UDRs and NSGs. Private Link will not be used in this solution. There is no need to make the Azure Paas Services accessible from the customer premises. There is no need to make Azure PaaS services available through service endpoints.","title":"Connectivity to Azure PaaS Services"},{"location":"lzdesignsample1/nwtopo/#private-link-and-dns-integration","text":"The only Azure PaaS service used in this solution is the SQL Server Managed Instance. This service will be deployed through VNet integration injection and Private Link will not be used. Therefore there is no need to design for Private Link integration with DNS.","title":"Private Link and DNS Integration"},{"location":"lzdesignsample1/nwtopo/#plan-for-inbound-and-outbound-connectivity","text":"Use Azure Firewall to govern Azure outbound traffic to the internet support Non-HTTP/S inbound connections provide isolation between the Dev/Test and Production environment provide traffic filtering between on-premise and the cloud environments. Forced tunneling of Internet traffic from the cloud via the customer premises will not be considered.","title":"Plan for Inbound and Outbound Connectivity"},{"location":"lzdesignsample1/nwtopo/#plan-for-application-delivery","text":"Azure Application Gateway v2 with WAF enabled will be deployed in the VNet of the workload subscription along with applications to secure inbound HTTP/S connections. A DDoS standard protection plan for all public IP addresses will be enabled for the VNets in the Production subscription.","title":"Plan for Application Delivery"},{"location":"lzdesignsample1/nwtopo/#plan-for-landing-zone-network-segmentation","text":"NSGs will be used to allow only the necessary traffic and deny all others between the subnets within a VNet as well as across VNets. UDRs will be used to route all outbound flows/connections through a centralized Azure Firewall. Since the proposed design proposes to deploy each tier of an application in its own subnet, Application Service Groups (ASGs) are not necessary.","title":"Plan for Landing Zone Network Segmentation"},{"location":"lzdesignsample1/nwtopo/#plan-for-network-encryption","text":"The proposed solution will use VPN gateways which support IPSEC tunnels end-to-end between customer premises and the cloud environment.","title":"Plan for Network Encryption"},{"location":"lzdesignsample1/nwtopo/#plan-for-traffic-inspection","text":"The proposed solution will rely on Network Watcher primarily to monitor NSG flows. Given most of the Internet traffic of interest is HTTPS, support for logging of this data in Azure Firewall is limited and will not be used.","title":"Plan for Traffic Inspection"},{"location":"lzdesignsample1/nwtopo/#hub-vnet-design","text":"The Hub VNet will host the network services of Bastion, VPN gateway and Azure Firewall. Note that each of the network elements occupies its own subnet. The diagram below illustrates. The Hub VNet will be peered with the VNets hosting the workloads so routes are set up between all the subnets in all the VNets. The VPN gateway is configured to establish IPSEC tunnels with peers in the customer premise. It is route based so it can be configured to use BGP for dynamic routing. To use the Firewall for routing and reduce the load on the VPN gateway, UDR is set up for the subnet hosting the VPN gateway making the Firewall the next hop for all the Workload VNets. The Firewall also functions as a DNS Proxy for lookups on the Azure private DNS from the customer premise. The paths for some of the common traffic flows are given below -- \u2022 VM access via Portal/Bastion -- Portal <-> Bastion <-> VM (via VNET peering). The Bastion acts as a reverse proxy so the return traffic from the VM is routed back to the Bastion. \u2022 On-premise access to VM -- On-premise <-> VPN gateway <-> Firewall <-> VM. The return traffic is forced to go through the Firewall via UDR for the VM subnet. \u2022 VM access to Internet -- VM <-> Firewall via UDR for the VM subnet. \u2022 On-premise access to application on VM -- On-premise <-> VPN gateway <-> Firewall <-> Application Gateway <-> application on VM. The return traffic uses the same path in reverse. \u2022 Internet access to application on VM -- Internet <-> Application Gateway <-> application on VM. The return traffic uses the same path in reverse.","title":"Hub VNet Design"},{"location":"lzdesignsample1/nwtopo/#vnet-design-for-net-application-production","text":"This section provides a high-level design for the VNet hosting the production level .NET application. The diagram below illustrates. The VNet is composed of 5 subnets. Subnet-Ext LB and Subnet-Int LB host application gateways that load balance traffic at the application/L7 layer across servers downstream for them. Subnet-Web hosts Web servers. Subnet-App hosts Application servers. Subnet-Database hosts Azure SQL Managed Instance.","title":"VNet Design for .NET Application -- Production"},{"location":"lzdesignsample1/nwtopo/#subnet-ext-lb","text":"This subnet hosts an instance of the Application Gateway with the Web Application Firewall. This gateway is L7 aware and performs load distribution and balancing across downstream Web servers. Some of the main configuration details are \u2022 This is both Internet facing and customer premise facing and has both a public and private virtual IP address \u2022 Listeners for both Internet and on-premise traffic are configured to run health probes and load balance across a set of Web servers hosted in the Subnet-Web that is discussed next. \u2022 SSL termination will be configured on the Application Server. SSL to Web servers can be provided if customer desires it. \u2022 WAF is to be configured with separate policies for Internet and customer premises traffic.","title":"Subnet-Ext LB"},{"location":"lzdesignsample1/nwtopo/#subnet-ext-lb-udr","text":"Azure resources in this subnet need to access only the Web servers in the Subnet-Web and no other subnet. UDR is configured to restrict the paths to and from this subnet accordingly. The main routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to default (0.0.0.0/0) via Internet. This covers the Internet traffic. \u2022 Enable routes to customer premise networks via the Azure Firewall in the Connectivity VNet subscription. This covers the customer premise traffic. \u2022 Enable route to Subnet-Web via the VNet gateway.","title":"Subnet-Ext LB UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-ext-lb-nsgs","text":"It is desirable to restrict traffic to certain protocols and ports for the sources and destinations permitted by the routes in UDR. The main NSG rules are \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Internet and customer premises (0.0.0.0/0). \u2022 Allow outgoing TCP traffic only to ports 80 and 443 in the Subnet-Web. \u2022 All other ports and protocols are to be denied. \u2022 NOTE -- It is assumed Azure management will use ports 80 and 443 on the Application gateway. If that is not the case it needs to be determined which ports are used and a rule is to be created to open those ports.","title":"Subnet-Ext LB NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-web","text":"This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-Web"},{"location":"lzdesignsample1/nwtopo/#subnet-web-udr","text":"Azure resources in this subnet need to access both the Application Gateway and downstream Application Gateway. A path to the Bastion host as also a path to the Internet is needed. A route is also needed to the customer premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Ext LB. \u2022 Enable route to the Subnet-Int LB. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-Web UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-web-nsgs","text":"Application data flows from the Application gateway and to the downstream Internal Load Balancer need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Ext LB. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-Int LB. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-Web NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-int-lb","text":"This subnet hosts an Application Gateway that only has a private virtual IP address. It load balances requests from the Web servers across Application servers.","title":"Subnet-Int LB"},{"location":"lzdesignsample1/nwtopo/#subnet-int-lb-udr","text":"Routes are needed to the Web servers and Application servers. Routes are also needed to the Internet for possible management traffic. \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-Int LB UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-int-lb-nsgs","text":"Application data flows from the Web servers and to the downstream Application servers need to be permitted on ports 80 and 443. If Azure management uses the same ports, these ports need to be accessible from the Internet as well. \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Subnet-Web. \u2022 Allow incoming TCP traffic only to ports 80 and 443 from the Internet. \u2022 Allow outgoing TCP traffic only to ports 80 and 443 in the Subnet-App. \u2022 All other ports and protocols are to be denied. \u2022 NOTE -- It is assumed Azure management will use ports 80 and 443 on the Application gateway. If that is not the case it needs to be determined which ports are used and a rule is to be created to open those ports.","title":"Subnet-Int LB NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-app","text":"This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-App"},{"location":"lzdesignsample1/nwtopo/#subnet-app-udr","text":"Routes are needed to the Internal Application gateway and to the Database service. A path to the Bastion host is also needed. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Int LB . \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-App UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-app-nsgs","text":"Application data flows from the internal Application gateway need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Int LB. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-App NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-database","text":"This subnet is dedicated to the hosting of Managed Instance SQL service. This is a PaaS service that is injected into the workload VNet to be available to the applications in that VNet. The configuration mainly consists of ensuring the subnet is dedicated to the PaaS service and delegation of the subnet to Microsoft.Sql/managedInstances resource provider. When the Managed Instance SQL service is instantiated a network intent policy is implemented by Azure on the VNet infrastructure elements which prevents any misconfiguration that could block operation of the service. Azure will also configure the routes and NSGs needed for management traffic to and from the service. More details on connectivity for Azure SQL Server Managed Instance can be obtained from Connectivity architecture for Azure SQL Managed Instance .","title":"Subnet-Database"},{"location":"lzdesignsample1/nwtopo/#subnet-database-udr","text":"Routes are needed to the Application servers. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. Enable route to the Subnet-App. Enable route to the customer premises via the Azure Firewall. Enable route to the default (0.0.0.0/0) via the Internet. Azure will configure the routes needed for the PaaS service to access other Azure services such as monitoring, storage, and backup.","title":"Subnet-Database UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-database-nsgs","text":"Application data flows to the Database need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. Allow incoming traffic to the on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. NOTE - It needs to be determined if traffic to and from other ports needs to be explicitly denied or whether the Azure configuration of the SQL Managed Instance service will address this.","title":"Subnet-Database NSGs"},{"location":"lzdesignsample1/nwtopo/#vnet-design-for-j2ee-application-production","text":"This section provides a high-level design for the VNet hosting the production level J2EE application. The diagram below illustrates. The design is pretty much the same as the .NET design except that there is no Internal Load Balancer between the Web servers and the App Servers. The affected subnets are the Subnet-Web and the Subnet-App the changes for which are described below.","title":"VNet Design for J2EE Application -- Production"},{"location":"lzdesignsample1/nwtopo/#subnet-web_1","text":"This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-Web"},{"location":"lzdesignsample1/nwtopo/#subnet-web-udr_1","text":"Azure resources in this subnet need to access both the Application Gateway and downstream Application servers. A path to the Bastion host as well as a path to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Ext LB. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-Web UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-web-nsgs_1","text":"Application data flows from the Application gateway and to the downstream Application servers need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Ext LB. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-App. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-Web NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-app_1","text":"This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-App"},{"location":"lzdesignsample1/nwtopo/#subnet-app-udr_1","text":"Routes are needed to the Web servers and to the Database service. A path to the Bastion host is also needed. A route is also needed to the Internet and customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-App UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-app-nsgs_1","text":"Application data flows from the Web servers need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Web. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-App NSGs"},{"location":"lzdesignsample1/nwtopo/#vnet-design-for-net-and-j2ee-applications-devtest","text":"This section provides a high-level design for the VNet hosting the Dev/Test level J2EE application and .NET application. The diagram below illustrates. The design is much simplified with single servers at each of the Web and Application tiers for both the J2EE and .NET incarnations. As a result, the load balancers have been dispensed with. Both the applications share the single SQL Managed Instance. The affected subnets are the Subnet-Web and the Subnet-App the changes for which are described below.","title":"VNet Design for .NET and J2EE Applications -- Dev/Test"},{"location":"lzdesignsample1/nwtopo/#subnet-web_2","text":"This subnet hosts Web servers that provide the presentation layer in a three-tier Web application stack. Each Web server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-Web"},{"location":"lzdesignsample1/nwtopo/#subnet-web-udr_2","text":"Azure resources in this subnet need to access the customer premises, the Internet and downstream Application servers. A path to the Bastion host as well as to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and also allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-App. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-Web UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-web-nsgs_2","text":"Application data flows from the customer premises and the Internet to the downstream Application servers need to be permitted on ports 80 and 443. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Internet. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the customer premises. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premises. \u2022 Allow outgoing traffic to the ports 80 and 443 to the Subnet-App. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-Web NSGs"},{"location":"lzdesignsample1/nwtopo/#subnet-app_2","text":"This subnet hosts Application servers that provide the business logic in a three tier Web application stack. Each Application server supports application traffic as well as health probes on ports 80 and 443. Rest of the configuration is application specific.","title":"Subnet-App"},{"location":"lzdesignsample1/nwtopo/#subnet-app-udr_2","text":"Routes are needed to the Web servers and to the Database service. A path to the Bastion host as well as to the Internet is needed. A route is also needed to the customer-premises via the Azure Firewall. This allows on-premise access to the VMs and allows Azure management traffic. The proposed routes are \u2022 Disable route to the VNet ranges of IP addresses. This prevents traffic to and from all subnets in the same VNet. \u2022 Enable route to the Subnet-Web. \u2022 Enable route to the Subnet-Database. \u2022 Enable route to the Bastion host. \u2022 Enable route to the customer premises via the Azure Firewall. \u2022 Enable route to the default (0.0.0.0/0) via the Internet.","title":"Subnet-App UDR"},{"location":"lzdesignsample1/nwtopo/#subnet-app-nsgs_2","text":"Application data flows from the Web servers need to be permitted on ports 80 and 443. Assuming the defaults for SQL Server, application data flows to the downstream Database service need to be permitted on TCP ports 1433, 4022, 135, 1434 and UDP ports 1434. The VMs themselves need to be accessible via the Bastion and customer premises for configuration. Finally, it is assumed Internet access via ports 22 and 3389 is needed for Azure management. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from the Bastion. \u2022 Allow incoming TCP traffic to the ports 80 and 443 from the Subnet-Web. \u2022 Allow outgoing TCP traffic to the ports 1433, 4022, 135, 1434 on the Subnet-Database. \u2022 Allow incoming TCP traffic to the ports 22 and 3389 from customer premise. \u2022 Allow incoming traffic to the ports 22 and 3389 from the Internet. \u2022 Allow outgoing traffic for all ports to the Internet.","title":"Subnet-App NSGs"},{"location":"lzdesignsample1/organdmgmt/","text":"Management groups, Subscription and Resources groups for Beta Customer* \u2693\ufe0e Management groups Design principle \u2693\ufe0e \u00b7*Management group hierarchy reasonably kept at two levels Only, ideally. This restriction reduces management overhead and complexity. IT Platform IT Management Platform. IT Connectivity. IT Landing Zone IT Prod. IT Dev. Tags on all resources are enforced by Azure Policy. These tags make it possible to query and horizontally navigate across the management group hierarchy. \u200b Subscription Design principle \u2693\ufe0e Management boundary: Subscriptions provide a management boundary for governance and isolation, which allows for a clear separation of concerns hence the following subscriptions are create Development environment Production environment Management environment Connectivity subscription Resources groups and Resources Design principle \u2693\ufe0e Resources in a group should have the same life cycle. Grant access with resource groups: use resource groups to control access to your resources. Common services, usage permission to specific user groups Subscription List of components (qty) Resources groups Design Principle it-prod-management-eastus01 Key vault (1) Rg-prod-management-keyvault-eastus01 Common services, usage permission to specific user groups it-prod-management-eastus01 Sentinel (1) Long analytics for Sentinel (1) Rg-prod-management-security-eastus01 Common services, usage permission to specific user groups it-prod-management-eastus01 Storage Account (Log archival) (1) Rg-prod-management-platform-eastus01 Common services, usage permission to specific user groups it-prod-connectivity-eastus01 Virtual network (1) Subnets (3) VPN Gateway (1) Rg-prod-connectivity-networkhub-eastus01 Common services, usage permission to specific user groups it-prod-connectivity-eastus01 Baston (1) Firewall (1) Rg-prod-connectivity-networksecurity-eastus01 Common services, usage permission to specific user groups it-prod-app-eastus01 Common services Key vault (1) Recovery vault (1) Rg-prod-app-commonservices-eastus-01 Common services, usage permission to specific user groups it-prod-app-eastus01 .Net Application VNET (1) Subnets (5) Application gateway (2) VMs (6) Azure SQL (1) Rg-prod-app-app1-eastus01 Same life cycle it-prod-app-eastus01 Java Application VNET (1) Subnets (4) Application gateway (1) \u00b7 VM (6) Azure SQL (1) \u00b7 Rg-prod-app-app2-eastus01 Same life cycle it-dev-app-eastus01 Common services Key vault (1) Recovery vault (1) Rg-dev-app-commonservices-eastus-01 Common services, usage permission to specific user groups it-dev-app-eastus01 .Net Application VNET (1) Subnets (5) Application gateway (2) VMs (6) Azure SQL (1) Rg-dev-app-app1-eastus01 Same life cycle it-dev-app-eastus01 Java Application VNET (1) Subnets (4) Application gateway (1) \u00b7 VM (6) Azure SQL (1) \u00b7 Rg-dev-app-app2-eastus01 Same life cycle","title":"3.Organization & Management Group"},{"location":"lzdesignsample1/organdmgmt/#management-groups-subscription-and-resources-groups-for-beta-customer","text":"","title":"Management groups, Subscription and Resources groups for Beta Customer*"},{"location":"lzdesignsample1/organdmgmt/#management-groups-design-principle","text":"\u00b7*Management group hierarchy reasonably kept at two levels Only, ideally. This restriction reduces management overhead and complexity. IT Platform IT Management Platform. IT Connectivity. IT Landing Zone IT Prod. IT Dev. Tags on all resources are enforced by Azure Policy. These tags make it possible to query and horizontally navigate across the management group hierarchy. \u200b","title":"Management groups Design principle"},{"location":"lzdesignsample1/organdmgmt/#subscription-design-principle","text":"Management boundary: Subscriptions provide a management boundary for governance and isolation, which allows for a clear separation of concerns hence the following subscriptions are create Development environment Production environment Management environment Connectivity subscription","title":"Subscription Design principle"},{"location":"lzdesignsample1/organdmgmt/#resources-groups-and-resources-design-principle","text":"Resources in a group should have the same life cycle. Grant access with resource groups: use resource groups to control access to your resources. Common services, usage permission to specific user groups Subscription List of components (qty) Resources groups Design Principle it-prod-management-eastus01 Key vault (1) Rg-prod-management-keyvault-eastus01 Common services, usage permission to specific user groups it-prod-management-eastus01 Sentinel (1) Long analytics for Sentinel (1) Rg-prod-management-security-eastus01 Common services, usage permission to specific user groups it-prod-management-eastus01 Storage Account (Log archival) (1) Rg-prod-management-platform-eastus01 Common services, usage permission to specific user groups it-prod-connectivity-eastus01 Virtual network (1) Subnets (3) VPN Gateway (1) Rg-prod-connectivity-networkhub-eastus01 Common services, usage permission to specific user groups it-prod-connectivity-eastus01 Baston (1) Firewall (1) Rg-prod-connectivity-networksecurity-eastus01 Common services, usage permission to specific user groups it-prod-app-eastus01 Common services Key vault (1) Recovery vault (1) Rg-prod-app-commonservices-eastus-01 Common services, usage permission to specific user groups it-prod-app-eastus01 .Net Application VNET (1) Subnets (5) Application gateway (2) VMs (6) Azure SQL (1) Rg-prod-app-app1-eastus01 Same life cycle it-prod-app-eastus01 Java Application VNET (1) Subnets (4) Application gateway (1) \u00b7 VM (6) Azure SQL (1) \u00b7 Rg-prod-app-app2-eastus01 Same life cycle it-dev-app-eastus01 Common services Key vault (1) Recovery vault (1) Rg-dev-app-commonservices-eastus-01 Common services, usage permission to specific user groups it-dev-app-eastus01 .Net Application VNET (1) Subnets (5) Application gateway (2) VMs (6) Azure SQL (1) Rg-dev-app-app1-eastus01 Same life cycle it-dev-app-eastus01 Java Application VNET (1) Subnets (4) Application gateway (1) \u00b7 VM (6) Azure SQL (1) \u00b7 Rg-dev-app-app2-eastus01 Same life cycle","title":"Resources groups and Resources Design principle"},{"location":"lzdesignsample1/placeholder/","text":"Placeholder - put content here \u2693\ufe0e in new files...","title":"Placeholder - put content here"},{"location":"lzdesignsample1/placeholder/#placeholder-put-content-here","text":"in new files...","title":"Placeholder - put content here"},{"location":"lzdesignsample1/platformauto/","text":"Workload Automation \u2693\ufe0e The modern application development and management practices emphasize speed, agility and consistency. To achieve this goal, it is important to automate the deployment, release and management functions of application workload. Azure provides native automation capability to automate deployment and repetitive operational tasks. In this section we describe the automation guidelines and best practices for a 3-tier web application workload. Key requirements \u2693\ufe0e In this release we have considered following requirements for automation \u00b7 Infrastructure deployment and configuration (Day-1 Operations) \u00b7 Operational tasks (Day-2 operations) Infrastructure deployment and configuration: \u2693\ufe0e It involves. \u00b7 Creation of base infrastructure such as resource groups, Virtual Network, subnets, Virtual machines, and SQL data bases. \u00b7 Installing software on a virtual machine, adding data to a database. \u00b7 Peering the workload Vnet with Hub network for on-premises integration and connectivity. \u00b7 Setting up of RBAC controls and access. \u00b7 Security and compliance configuration. \u00b7 Setting up of monitoring for application and the infrastructure. \u00b7 Backup configuration. \u00b7 Integration with backend systems. Steady-state operational tasks \u2693\ufe0e These are repetitive tasks such as \u00b7 Remediate configuration deviation. \u00b7 Add disks, extend disk to an existing VM. \u00b7 On demand backups. \u00b7 Resize the VMs. \u00b7 Database optimization. \u00b7 Add/update NSG rules. \u00b7 Provision cloud resources. \u00b7 Decommission cloud resources. Design recommendations \u2693\ufe0e Azure Automation provides a cloud-based automation and configuration service that supports consistent management across IaaS and PaaS resources. It comprises process automation, configuration management, update management, task automation, event automation. Infrastructure deployment and configuration: \u2693\ufe0e \u00b7 Use native Azure ARM templates to deploy and configure the base infrastructure needed for the workload. \u00b7 ARM templates can be invoked from an Azure Automation account or from a DevOps pipeline. Stead-state operational tasks: \u2693\ufe0e Kyndryl has already embraced automation as the foundation for every aspect of Infrastructure Services delivery for day two tasks such as security compliance, patching, identity & access management, monitoring, backup etc. There is a rich set of are pre-built automations available using Ansible playbooks. We recommend using these Ansible playbooks for operational efficiency. The high-level design is as below.","title":"8.Platform Automation & DevOps"},{"location":"lzdesignsample1/platformauto/#workload-automation","text":"The modern application development and management practices emphasize speed, agility and consistency. To achieve this goal, it is important to automate the deployment, release and management functions of application workload. Azure provides native automation capability to automate deployment and repetitive operational tasks. In this section we describe the automation guidelines and best practices for a 3-tier web application workload.","title":"Workload Automation"},{"location":"lzdesignsample1/platformauto/#key-requirements","text":"In this release we have considered following requirements for automation \u00b7 Infrastructure deployment and configuration (Day-1 Operations) \u00b7 Operational tasks (Day-2 operations)","title":"Key requirements"},{"location":"lzdesignsample1/platformauto/#infrastructure-deployment-and-configuration","text":"It involves. \u00b7 Creation of base infrastructure such as resource groups, Virtual Network, subnets, Virtual machines, and SQL data bases. \u00b7 Installing software on a virtual machine, adding data to a database. \u00b7 Peering the workload Vnet with Hub network for on-premises integration and connectivity. \u00b7 Setting up of RBAC controls and access. \u00b7 Security and compliance configuration. \u00b7 Setting up of monitoring for application and the infrastructure. \u00b7 Backup configuration. \u00b7 Integration with backend systems.","title":"Infrastructure deployment and configuration:"},{"location":"lzdesignsample1/platformauto/#steady-state-operational-tasks","text":"These are repetitive tasks such as \u00b7 Remediate configuration deviation. \u00b7 Add disks, extend disk to an existing VM. \u00b7 On demand backups. \u00b7 Resize the VMs. \u00b7 Database optimization. \u00b7 Add/update NSG rules. \u00b7 Provision cloud resources. \u00b7 Decommission cloud resources.","title":"Steady-state operational tasks"},{"location":"lzdesignsample1/platformauto/#design-recommendations","text":"Azure Automation provides a cloud-based automation and configuration service that supports consistent management across IaaS and PaaS resources. It comprises process automation, configuration management, update management, task automation, event automation.","title":"Design recommendations"},{"location":"lzdesignsample1/platformauto/#infrastructure-deployment-and-configuration_1","text":"\u00b7 Use native Azure ARM templates to deploy and configure the base infrastructure needed for the workload. \u00b7 ARM templates can be invoked from an Azure Automation account or from a DevOps pipeline.","title":"Infrastructure deployment and configuration:"},{"location":"lzdesignsample1/platformauto/#stead-state-operational-tasks","text":"Kyndryl has already embraced automation as the foundation for every aspect of Infrastructure Services delivery for day two tasks such as security compliance, patching, identity & access management, monitoring, backup etc. There is a rich set of are pre-built automations available using Ansible playbooks. We recommend using these Ansible playbooks for operational efficiency. The high-level design is as below.","title":"Stead-state operational tasks:"},{"location":"lzdesignsample1/wldescription/","text":"Overview \u2693\ufe0e This section walks through the design of a Landing Zone using a sample workload scenario. Design approach and recommendations described in the \"landing zone critical design areas\" from previous section will be applied against this workload to define the landing zone. It is to be noted that the intent is to emphasize the design approach and not to focus on the correctness of the solution that is arrived at. The plan is to validate the proposed solution as and when sufficient resources are available. Design Approach \u2693\ufe0e The approach consists of addressing the critical design areas outlined in the Design document , one after the other. Design considerations and recommendations are considered against the requirements that are either stated in the Workload description or assumed where needed. The outcome are design decisions that then drive the solution design. Practically, the approach may be more iterative to address the possible inter-dependencies between these areas. Workload Description \u2693\ufe0e Two incarnations of a three tier Web application, consisting of Web, Application and Database tiers, are considered as workload for the purpose creating a landing zone design. One incarnation is that of a J2EE application that leverages a load balancer only for the Web tier. The other is a .NET application that uses load balancers for both the Web and Application tiers. Two environments are considered \u2013 one for Dev/Test and the other for Production. Both environments need to be isolated from each other. Connectivity to the cloud environments from customer premises is needed for configuration of the cloud resources and management of the applications. The concurrent users and aggregate transaction rate in terms of the workload scalability and performance are not considered in this exercise. So, sizing of the solution components in the proposed design is not discussed.","title":"Workload Description"},{"location":"lzdesignsample1/wldescription/#overview","text":"This section walks through the design of a Landing Zone using a sample workload scenario. Design approach and recommendations described in the \"landing zone critical design areas\" from previous section will be applied against this workload to define the landing zone. It is to be noted that the intent is to emphasize the design approach and not to focus on the correctness of the solution that is arrived at. The plan is to validate the proposed solution as and when sufficient resources are available.","title":"Overview"},{"location":"lzdesignsample1/wldescription/#design-approach","text":"The approach consists of addressing the critical design areas outlined in the Design document , one after the other. Design considerations and recommendations are considered against the requirements that are either stated in the Workload description or assumed where needed. The outcome are design decisions that then drive the solution design. Practically, the approach may be more iterative to address the possible inter-dependencies between these areas.","title":"Design Approach"},{"location":"lzdesignsample1/wldescription/#workload-description","text":"Two incarnations of a three tier Web application, consisting of Web, Application and Database tiers, are considered as workload for the purpose creating a landing zone design. One incarnation is that of a J2EE application that leverages a load balancer only for the Web tier. The other is a .NET application that uses load balancers for both the Web and Application tiers. Two environments are considered \u2013 one for Dev/Test and the other for Production. Both environments need to be isolated from each other. Connectivity to the cloud environments from customer premises is needed for configuration of the cloud resources and management of the applications. The concurrent users and aggregate transaction rate in terms of the workload scalability and performance are not considered in this exercise. So, sizing of the solution components in the proposed design is not discussed.","title":"Workload Description"},{"location":"misc/designreview/","text":"Design Review Process \u2693\ufe0e Solutions developed for any client should follow the solution design process as explained in this section. This is required as part of Kyndryl's Cloud Architecture Design(CAD) requirement and Azure MSP(Managed Service Provider) requirement section 3.4.1 Practitioner Guidance: Key Assets \u2693\ufe0e Read and become familiar with the Kyndryl\u2019s Azure Solution Guidance . This provides guidance on how to get started with solutioning on Microsoft\u2019s Azure Cloud platform. Review reference architectures: \u2022 Microsoft published reference architectures . \u2022 Kyndryl solution guidance for sample workload . Practitioner Guidance: Design Steps \u2693\ufe0e Practitioner Guidance: Design Review Process \u2693\ufe0e Below workflow shows the design and review process the solution architect should follow before publishing the final Technical Solution Design document. Below process provides an outline of the process and details will be added later.","title":"Design Review Process"},{"location":"misc/designreview/#design-review-process","text":"Solutions developed for any client should follow the solution design process as explained in this section. This is required as part of Kyndryl's Cloud Architecture Design(CAD) requirement and Azure MSP(Managed Service Provider) requirement section 3.4.1","title":"Design Review Process"},{"location":"misc/designreview/#practitioner-guidance-key-assets","text":"Read and become familiar with the Kyndryl\u2019s Azure Solution Guidance . This provides guidance on how to get started with solutioning on Microsoft\u2019s Azure Cloud platform. Review reference architectures: \u2022 Microsoft published reference architectures . \u2022 Kyndryl solution guidance for sample workload .","title":"Practitioner Guidance: Key Assets"},{"location":"misc/designreview/#practitioner-guidance-design-steps","text":"","title":"Practitioner Guidance: Design Steps"},{"location":"misc/designreview/#practitioner-guidance-design-review-process","text":"Below workflow shows the design and review process the solution architect should follow before publishing the final Technical Solution Design document. Below process provides an outline of the process and details will be added later.","title":"Practitioner Guidance: Design Review Process"},{"location":"resource-service/aks/","text":"Azure Kubernetes Service (AKS) \u2693\ufe0e Azure Kubernetes Service (AKS) is Azure's managed service for Kubernetes. It reduces the complexity of building and managing Kubernetes clusters. A typical Kubernetes cluster consists of a number of master nodes that constitute the control plane and a number of worker nodes. A node within a Kubernetes cluster is equivalent to a server or a virtual machine (VM). The control plane contains the Kubernetes API and a database that persists the cluster state. The worker nodes are the compute resources that run actual workloads. When an AKS cluster is created, Azure sets up the master nodes and manages them. Worker nodes are created from one or more Virtual Machine Scale Sets (VMSS). The control plane can either be paid for or free depending on whether SLAs are supported or not. The purpose of this document is to lay out those aspects in which AKS is different from standalone Kubernetes distributions such as OpenShift and Rancher. A significant portion of the document focusses on the integration of AKS with functions/features of the Azure platform such as Load Balancers, Application Gateways, Azure Active Directory, Monitoring, and Security Center. The prominent topics in the subsequent sections are - AKS - Installation and Operation AKS - Networking AKS - Troubleshooting AKS - Identity and Access Management AKS - Security AKS - Azure PaaS AKS - Installation and Operation \u2693\ufe0e This section covers the installation and operational aspects of AKS Clusters. AKS Cluster Installation \u2693\ufe0e An AKS cluster can be created in a number of ways - Azure portal: leverages a wizard provided via a graphical user interface (GUI) for deploying the cluster. It is not suited for automated deployment or deployment of multiple clusters. Azure CLI: This is a cross\u2011platform CLI for managing Azure resources. It enables automated deployment of AKS clusters via scripts. Azure PowerShell: This is a set of PowerShell commands used for managing Azure resources directly from PowerShell. It also enables automated deployment of AKS clusters via PowerShell scripts. ARM templates: Azure Resource Manager (ARM) templates are an Azure\u2011native way to deploy Azure resources using Infrastructure as Code (IaC). An ARM template supports the declarative and parameterized definition of an AKS cluster that can be reused to create consistent cluster instances. Terraform for Azure: Terraform is an open-source IaC tool developed by HashiCorp and supports deployment of resources in a number of cloud platforms including Azure. Like ARM templates, Terraform supports the declarative and parameterized definition of an AKS cluster that can be reused to create consistent cluster instances. More detailed information on installation is provided in the section titled \"Deploy an AKS Cluster\" at https://docs.microsoft.com/azure/aks . AKS Cluster Dashboard \u2693\ufe0e Azure portal provides a dashboard for each AKS cluster. A search for \"AKS\" through the \"Search resources\" facility in the Azure portal lists all the AKS instances. Selecting one cluster brings up the dashboard for that cluster. The dashboard serves as a single pane of glass for the cluster providing functions such as - Cluster Settings - covers node pools, cluster configuration, networking, identity, role based access control, policies Management of Kubernetes resources - covers deployments, namespaces, config maps, secrets, storage, services and ingresses Monitoring - covers Metrics, Logs, Alerts, Insights, Diagnostic settings and pre-built workbooks to display custom views of the metric data Miscellaneous - view activity logs, identity/role lifecycle, security, support and automation More details on the Azure AKS Dashboard are provided at https://docs.microsoft.com/en-us/azure/aks/kubernetes-dashboard . AKS Cluster Access via CloudShell \u2693\ufe0e Azure provides, as a convenience, a command-line interface (Bash or Powershell) that can be used to connect to and operate AKS cluster instances. This is called Cloudshell and is accessible on the Azure portal as shown below - The first time CloudShell is launched, the following steps need to be performed one time - a storage account (either existing or new) needs to be associated with it in order to provide a file share to which files can be persisted for future sessions of CloudShell. Credentials for accessing the AKS cluster need to be obtained. For the bash shell the following command can be used - az aks get-credentials --resource-group \\<rg-name> --name \\<aks-cluster-name> Now on, kubectl can be used, just like with any Kubernetes cluster, to operate the AKS cluster. CloudShell comes with kubectl pre-installed. AKS Cluster Upgrade \u2693\ufe0e The upgrade of the AKS cluster can be performed from the AKS Cluster dashboard. The control plane needs to be first upgraded using the \"Cluster configuration\" pane. Then the worker nodes can be upgraded using the \"Node pools\" pane. More details on the upgrade process are provided at https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster . Azure AKS Cluster Auto-scaling \u2693\ufe0e It is possible for an AKS cluster to be scaled up and down automatically depending on the resources available to create new workloads. For example, if the Kubernetes control plane is unable to create the required pods due to lack of resources within the AKS cluster, the cluster autoscaler can be configured to add more worker nodes to the AKS cluster. As and when workloads finish or deleted and resources become available worker nodes can be removed from the AKS cluster. More details on cluster autoscaling are provided at https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler . Auto-repair of AKS Worker Node \u2693\ufe0e An AKS cluster continuously monitors the health of worker nodes and leverages Azure VMs to automatically repair a node if it not healthy. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/aks/node-auto-repair . AKS - Networking \u2693\ufe0e This section covers the networking aspects of AKS clusters. AKS Network Overlays \u2693\ufe0e Azure provides two options for network overlays that will provide network connectivity for the pods hosted in AKS clusters. One option is Kubenet wherein a virtual network and subnet are created for an AKS cluster. The nodes of the AKS cluster are given IP addresses from this virtual network and subnet while pods within the AKS cluster use a completely different IP address range. Traffic The diagram below exemplifies how pods are assigned IP addresses and communicate with one another and the external world. As can be seen above, Node 1 above assigns pods addresses from the 10.244.0.0/24 range while Node 2 does the same from the 10.244.2.0/24 range. For traffic from Pod 1 to Pod 3, User-Defined Routes (UDRs) are created in Node 1 that route the traffic between the nodes and vice versa. Traffic between pods and VMs (and all other entities external to the AKS cluster) can only be initiated at the pod and is source-NATed to the IP address of the source node. More details on the use of Kubenet in Azure AKS are provided at https://docs.microsoft.com/en-us/azure/aks/configure-kubenet . The other option is Azure CNI wherein pods are assigned IP addresses from the same ranges as the AKS cluster nodes themselves. The benefit from this approach is the support two-way initiations of communications between pods and VMs. The drawback is the extra planning required to make sure adequate addresses are available as containers are created and deleted. More details on the use of Azure CNI in Azure AKS are provided at https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni . AKS Cluster Load Balancer \u2693\ufe0e There are three possible options for load balancing traffic from outside the AKS cluster. Azure Load Balancer \u2693\ufe0e This is an L4 Load Balancer that is created by Azure when a workload is exposed via a Kubernetes Service of type LoadBalancer. For every workload and associated Service, a public IP address, front end, load balancing rules and backend pools are created. For services that are configured for access only from within Azure networks, an internal load balancer with a private IP address instead of a public IP address is created. The Azure Load Balancer basically distributes the incoming requests across the worker nodes of the AKS cluster using the Nodeport feature of Kubernetes. The kube-proxy component within each worker node does further load balancing within the cluster. The Azure Load Balancer does not perform any filtering, SSL offloading, cookie-based affinity, URL routing or application-level firewalling. More details on the Azure Load Balancer for AKS are provided at https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard . HTTP Application routing Add on \u2693\ufe0e This consists of two components - An external DNS controller that watches for new ingress rules and creates A records in a cluster-specific DNS zone that is external to the cluster. An Nginx Ingress controller that consists of an Azure Load Balancer Service and an Nginx deployment that is configured with L7 load balancing rules as and when ingress rules are created within AKS. As external traffic for a particular application deployed in AKS arrives at a site, it is directed via DNS resolution to the Azure Load Balancer which load balances the traffic across the Nginx servers. The Nginx servers use the HTTP headers in the requests and the configured ingress rules to direct the traffic to a suitable pod in the AKS cluster. The addon supports functions such as SSL termination, cookie-based affinity and URL based forwarding. The addon needs to be enabled for the cluster by running the following command in CloudShell - az aks enable-addons --resource-group \\<rg-for-AKS-cluster> --name \\<AKS-cluster-name> --addons http_application_routing. The purpose of this addon is to quickly create an ingress controller for testing applications. It is not meant for production environments. More details on this add-on are provided at https://docs.microsoft.com/en-us/azure/aks/http-application-routing . Application Gateway Ingress Controller (AGIC) \u2693\ufe0e The AGIC is the native ingress controller for AKS. It integrates the Azure Application Gateway into an AKS cluster to support the ingress controller functionality. Either an existing or a new Application Gateway instance can be used for the purpose. A new Application Gateway instance can be integrated into an AKS cluster using the following command in CloudShell - az aks enable-addons --resource-group \\<rg-for-AKS-cluster> --name \\<AKS-cluster-name> -a ingress-appgw --appgw-subnet-cidr \\<appgw-subnet> --appgw-name \\<app-gw-name> When an ingress rule is created, the Application Gateway instance integrated into the AKS cluster is configured to load balance traffic for the associated application across the requisite set of pods in the cluster. The Fully Qualified Domain Name (FQDN) for an application needs to be manually configured in an external DNS zone with resolution pointing to the IP address of the integrated Application Gateway instance. As external traffic for a particular application deployed in AKS arrives at a site, it is directed via DNS resolution to the Application Gateway instance which in turn uses the HTTP headers in the requests and the configured ingress rules to direct the traffic to a suitable pod in the AKS cluster. In addition to functions such as URL routing, cookie-based affinity and SSL termination, the AGIC provides web application firewall functions and lower latency. It is the recommended method to access applications running in AKS clusters. Other ingress controllers such as nginx, Traefik, HAProxy can be used where functionality such as canary deployments and more flexible load balancing strategies are required. More details on the AGIC are provided at https://docs.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview . AKS - Troubleshooting \u2693\ufe0e This section discusses troubleshooting and monitoring procedures for AKS clusters. AKS Diagnostics \u2693\ufe0e When experiencing issues with Azure AKS, a good place to start troubleshooting is the \"AKS Diagnose and solve problems\" option available via the AKS Dashboard. When this option is selected a choice of \"Cluster Insights\" and \"Networking\" is presented. Cluster Insights uses cluster logs and the cluster configuration to perform a health check and compare the cluster against best practices. It contains useful information and relevant health indicators in case anything is misconfigured in the cluster. The Networking section of AKS Diagnostics allows interactive troubleshooting of networking issues in the cluster. It presents several possible areas for troubleshooting. Selection of any area will then trigger network health checks and configuration reviews and provide helpful insights. More details on AKS Diagnostics are provided at https://docs.microsoft.com/en-us/azure/aks/concepts-diagnostics . AKS Container Insights \u2693\ufe0e This is different from the Cluster Insights described above in the sense it is more focused on the presenting different views and analysis of metrics and logs from the AKS cluster to gain deeper insights in the cluster behavior. It needs to be enabled on an AKS cluster that results in the collection of metrics and logs by a containerized version of the Log Analytics agent and aggregation of the same into a Log Analytics workspace. The agent leverages Kubernetes APIs to extract the data from the AKS cluster. Cluster Insights then provides the following functions based on the data stored in the Log Analytics workspace - Cluster Metrics - displays CPU utilization and the memory utilization of all the nodes in the cluster. Optionally additional filters can be added to filter to a particular namespace, node, or node pool. There also is a live option, which provides more real-time information on your cluster status. Reports - A number of pre-configured monitoring workbooks are provided that combine text, log queries, metrics and parameters to display rich interactive reports. Nodes - displays detailed metrics for the AKS cluster nodes as also the location of the various pods running in the AKS cluster. Event logs from the nodes can be viewed as well. Controllers - displays details on all the Kubernetes controllers (ReplicaSets, DaemonSets, StatefulSets, and others) and the pods running within them. Containers - displays the metrics, logs, and environment variables for a container. Deployments - displays the details of all the deployments in an AKS cluster. More details on AKS Container Insights is provided at https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview . AKS - Identity and Access Management \u2693\ufe0e This section covers the integration of AKS clusters with Azure for identity and access management. AKS-managed Azure Active Directory Integration \u2693\ufe0e Kubernetes clusters leverage authentication plugins to various authentication mechanisms (certificates, bearer tokens, HTTP basic authorization, file based userid-password) to perform authentication of users. AKS however, supports Azure Active Directory (AAD) in addition to all these. Azure users can thus use both the Azure platform and AKS clusters hosted within that platform using the same user-id. However, the Role-based Access Control is separate across the Azure platform and AKS cluster. It is therefore necessary to create roles and authorization rules separately for both. More details on AKS-managed AAD Integration is provided at https://docs.microsoft.com/en-us/azure/aks/managed-aad . Azure AD pod-managed identities in AKS \u2693\ufe0e Managed identities in Azure are used by applications or Azure functions running in VMs to authenticate to Azure AD and obtain authorization to access other resources. Azure supports two types of managed identities - System assigned - This type of managed identity is dedicated to one resource such as a VM. Its lifecycle is identical to that of the resource, which means it is deleted along with the resource. User assigned - this type of managed identity has its own lifecycle and can be shared by multiple resources. A managed identity is passed to Azure AD through a special endpoint called the Instance Metadata Service (IMDS) which uses a certificate configured for that managed identity. This results in a token being returned to the requestor that can be used to authenticate to other resources and authorize their use. Managed identities are easier to manage than service principals and therefore, the recommended approach for the authentication and authorization of applications that need access to other Azure resources. Note that VMs can be configured with managed identities which, by default, could be used by the multiple pods running in that VM. The Azure AD pod-managed identities add-on for AKS configures the AKS cluster such that pods can no longer access the IMDS endpoint directly. Instead all pods trying to access this endpoint connect to a DaemonSet called the Node Managed Identity (NMI) which validates the pod request then leverages the IMDS to acquire a token and return it to the requesting pod. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/aks/use-managed-identity . AKS - Security \u2693\ufe0e This section covers the integration of AKS clusters with Azure for security services. AKS Cluster Compliance and Threat Detection \u2693\ufe0e Azure Policy is a service that helps to enforce organizational standards and to assess compliance at-scale. For Kubernetes and specifially AKS, Azure Policy leverages a tool called Gatekeeper which functions as an Admission Controller webhook for an AKS cluster. All authentication requests and Kubernetes API calls are intercepted and sent to GateKeeper. Gatekeeper in turn uses a tool Open Policy Agent (OPA) to detect actions that are non-compliant with set policies and report to Azure Policy. OPA also watches for instances of Custom Resource Definitions (CRDs) that define policies and for non-compliance by standard Kubernetes resources. Azure Policy needs to be enabled in order for policies to be assigned to the AKS cluster and for the Admission Controller webhook to be established. More details on Azure Policy for AKS are provided at https://docs.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes . Azure Defender provides threat detection capabilities for AKS clusters by monitoring logs and behavior patterns. More details on Azure Defender for AKS are provided at https://docs.microsoft.com/en-us/azure/security-center/defender-for-kubernetes-introduction . AKS Cluster Secrets and Azure Key Vault \u2693\ufe0e Secrets in Kubernetes are used to hold sensitive information such as passwords or connection strings. Secrets are held in the persistent store used by the Kubernetes control plane and are unencrypted by default. Any one with access to the Kubernetes API or the persistent store can read secrets. Also any one with the ability to create pods in a Kubernetes namespace can read secrets in that namespace. For AKS clusters, Azure Key Vault provides a more secure way of working with secrets. This approach relies on a Container Storage Interface (CSI) driver that enables mounting of secrets as volumes. Typical steps to leverage Key Vault in AKS clusters are - Set up the CSI Driver for Key Vault in the AKS cluster Create a user assigned managed identity that will control access to a Key Vault instance Create a Key Vault instance Bind the user assigned managed identity to the Key Vault instance. This gives all resources that are given this managed identity permissions to operate the Key Vault instance Assign the user assigned managed identity to the resources that need access to secrets in the Key Vault instance More details on use of Azure Key Vault for AKS secrets are provided at https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver . AKS - Azure PaaS \u2693\ufe0e This section covers the use of Azure PaaS services by AKS clusters. Azure Service Operator (ASO) \u2693\ufe0e Azure provides many managed services, database services being one. The benefit to using these services is that aspects such scalability, security, high availability, disaster recovery and backup are all taken care as part of the managed service. Therefore it is beneficial for all workloads, including those running in AKS clusters, to leverage these managed services. ASO is a Kubernetes Operator for AKS clusters that provisions Azure services and connects applications running within an AKS cluster to these services. A Kubernetes Operator is basically a way to extend the Kubernetes APIs with custom resources and consists of two components - A Custom Resource Definition (CRD) that can be used to instantiate a custom resource in the AKS Cluster. In case of ASO, each custom resource represents one instance of an Azure service that can be managed like any other Kubernetes resource. A Controller that watches for the creation/changes/deletion of these custom resources and takes suitable action. In the case of ASO, it interacts with the Azure API to create/modify/delete the associated Azure resources. The Controller for ASO needs to be linked to a managed-identity that has permissions to create resources in Azure. In addition, the ASO Controller may leverage Key Vaults to hold secrets such as connection strings for databases. More details on ASO are provided at https://github.com/Azure/azure-service-operator . AKS Serverless and Azure Functions \u2693\ufe0e Serverless refers to an environment for a workload where servers need not be managed. Resources are consumed only when there is work to be done. Also the environment scales automatically with the workload. AKS clusters can support serverless environments using Azure Functions. This service consists of two components - the Functions runtime and the Kubernetes-based Event Driven Autoscaling scale controller. The latter monitors the rate at which events are arriving at a function and scales the function appropriately. In addition, a Container Registry to hold the images for the Azure Functions will be required. More details on use of Azure Functions with AKS are provided at https://docs.microsoft.com/en-us/azure/azure-functions/functions-kubernetes-keda .","title":"Azure Kubernetes Service"},{"location":"resource-service/aks/#azure-kubernetes-service-aks","text":"Azure Kubernetes Service (AKS) is Azure's managed service for Kubernetes. It reduces the complexity of building and managing Kubernetes clusters. A typical Kubernetes cluster consists of a number of master nodes that constitute the control plane and a number of worker nodes. A node within a Kubernetes cluster is equivalent to a server or a virtual machine (VM). The control plane contains the Kubernetes API and a database that persists the cluster state. The worker nodes are the compute resources that run actual workloads. When an AKS cluster is created, Azure sets up the master nodes and manages them. Worker nodes are created from one or more Virtual Machine Scale Sets (VMSS). The control plane can either be paid for or free depending on whether SLAs are supported or not. The purpose of this document is to lay out those aspects in which AKS is different from standalone Kubernetes distributions such as OpenShift and Rancher. A significant portion of the document focusses on the integration of AKS with functions/features of the Azure platform such as Load Balancers, Application Gateways, Azure Active Directory, Monitoring, and Security Center. The prominent topics in the subsequent sections are - AKS - Installation and Operation AKS - Networking AKS - Troubleshooting AKS - Identity and Access Management AKS - Security AKS - Azure PaaS","title":"Azure Kubernetes Service (AKS)"},{"location":"resource-service/aks/#aks-installation-and-operation","text":"This section covers the installation and operational aspects of AKS Clusters.","title":"AKS - Installation and Operation"},{"location":"resource-service/aks/#aks-cluster-installation","text":"An AKS cluster can be created in a number of ways - Azure portal: leverages a wizard provided via a graphical user interface (GUI) for deploying the cluster. It is not suited for automated deployment or deployment of multiple clusters. Azure CLI: This is a cross\u2011platform CLI for managing Azure resources. It enables automated deployment of AKS clusters via scripts. Azure PowerShell: This is a set of PowerShell commands used for managing Azure resources directly from PowerShell. It also enables automated deployment of AKS clusters via PowerShell scripts. ARM templates: Azure Resource Manager (ARM) templates are an Azure\u2011native way to deploy Azure resources using Infrastructure as Code (IaC). An ARM template supports the declarative and parameterized definition of an AKS cluster that can be reused to create consistent cluster instances. Terraform for Azure: Terraform is an open-source IaC tool developed by HashiCorp and supports deployment of resources in a number of cloud platforms including Azure. Like ARM templates, Terraform supports the declarative and parameterized definition of an AKS cluster that can be reused to create consistent cluster instances. More detailed information on installation is provided in the section titled \"Deploy an AKS Cluster\" at https://docs.microsoft.com/azure/aks .","title":"AKS Cluster Installation"},{"location":"resource-service/aks/#aks-cluster-dashboard","text":"Azure portal provides a dashboard for each AKS cluster. A search for \"AKS\" through the \"Search resources\" facility in the Azure portal lists all the AKS instances. Selecting one cluster brings up the dashboard for that cluster. The dashboard serves as a single pane of glass for the cluster providing functions such as - Cluster Settings - covers node pools, cluster configuration, networking, identity, role based access control, policies Management of Kubernetes resources - covers deployments, namespaces, config maps, secrets, storage, services and ingresses Monitoring - covers Metrics, Logs, Alerts, Insights, Diagnostic settings and pre-built workbooks to display custom views of the metric data Miscellaneous - view activity logs, identity/role lifecycle, security, support and automation More details on the Azure AKS Dashboard are provided at https://docs.microsoft.com/en-us/azure/aks/kubernetes-dashboard .","title":"AKS Cluster Dashboard"},{"location":"resource-service/aks/#aks-cluster-access-via-cloudshell","text":"Azure provides, as a convenience, a command-line interface (Bash or Powershell) that can be used to connect to and operate AKS cluster instances. This is called Cloudshell and is accessible on the Azure portal as shown below - The first time CloudShell is launched, the following steps need to be performed one time - a storage account (either existing or new) needs to be associated with it in order to provide a file share to which files can be persisted for future sessions of CloudShell. Credentials for accessing the AKS cluster need to be obtained. For the bash shell the following command can be used - az aks get-credentials --resource-group \\<rg-name> --name \\<aks-cluster-name> Now on, kubectl can be used, just like with any Kubernetes cluster, to operate the AKS cluster. CloudShell comes with kubectl pre-installed.","title":"AKS Cluster Access via CloudShell"},{"location":"resource-service/aks/#aks-cluster-upgrade","text":"The upgrade of the AKS cluster can be performed from the AKS Cluster dashboard. The control plane needs to be first upgraded using the \"Cluster configuration\" pane. Then the worker nodes can be upgraded using the \"Node pools\" pane. More details on the upgrade process are provided at https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster .","title":"AKS Cluster Upgrade"},{"location":"resource-service/aks/#azure-aks-cluster-auto-scaling","text":"It is possible for an AKS cluster to be scaled up and down automatically depending on the resources available to create new workloads. For example, if the Kubernetes control plane is unable to create the required pods due to lack of resources within the AKS cluster, the cluster autoscaler can be configured to add more worker nodes to the AKS cluster. As and when workloads finish or deleted and resources become available worker nodes can be removed from the AKS cluster. More details on cluster autoscaling are provided at https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler .","title":"Azure AKS Cluster Auto-scaling"},{"location":"resource-service/aks/#auto-repair-of-aks-worker-node","text":"An AKS cluster continuously monitors the health of worker nodes and leverages Azure VMs to automatically repair a node if it not healthy. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/aks/node-auto-repair .","title":"Auto-repair of AKS Worker Node"},{"location":"resource-service/aks/#aks-networking","text":"This section covers the networking aspects of AKS clusters.","title":"AKS - Networking"},{"location":"resource-service/aks/#aks-network-overlays","text":"Azure provides two options for network overlays that will provide network connectivity for the pods hosted in AKS clusters. One option is Kubenet wherein a virtual network and subnet are created for an AKS cluster. The nodes of the AKS cluster are given IP addresses from this virtual network and subnet while pods within the AKS cluster use a completely different IP address range. Traffic The diagram below exemplifies how pods are assigned IP addresses and communicate with one another and the external world. As can be seen above, Node 1 above assigns pods addresses from the 10.244.0.0/24 range while Node 2 does the same from the 10.244.2.0/24 range. For traffic from Pod 1 to Pod 3, User-Defined Routes (UDRs) are created in Node 1 that route the traffic between the nodes and vice versa. Traffic between pods and VMs (and all other entities external to the AKS cluster) can only be initiated at the pod and is source-NATed to the IP address of the source node. More details on the use of Kubenet in Azure AKS are provided at https://docs.microsoft.com/en-us/azure/aks/configure-kubenet . The other option is Azure CNI wherein pods are assigned IP addresses from the same ranges as the AKS cluster nodes themselves. The benefit from this approach is the support two-way initiations of communications between pods and VMs. The drawback is the extra planning required to make sure adequate addresses are available as containers are created and deleted. More details on the use of Azure CNI in Azure AKS are provided at https://docs.microsoft.com/en-us/azure/aks/configure-azure-cni .","title":"AKS Network Overlays"},{"location":"resource-service/aks/#aks-cluster-load-balancer","text":"There are three possible options for load balancing traffic from outside the AKS cluster.","title":"AKS Cluster Load Balancer"},{"location":"resource-service/aks/#azure-load-balancer","text":"This is an L4 Load Balancer that is created by Azure when a workload is exposed via a Kubernetes Service of type LoadBalancer. For every workload and associated Service, a public IP address, front end, load balancing rules and backend pools are created. For services that are configured for access only from within Azure networks, an internal load balancer with a private IP address instead of a public IP address is created. The Azure Load Balancer basically distributes the incoming requests across the worker nodes of the AKS cluster using the Nodeport feature of Kubernetes. The kube-proxy component within each worker node does further load balancing within the cluster. The Azure Load Balancer does not perform any filtering, SSL offloading, cookie-based affinity, URL routing or application-level firewalling. More details on the Azure Load Balancer for AKS are provided at https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard .","title":"Azure Load Balancer"},{"location":"resource-service/aks/#http-application-routing-add-on","text":"This consists of two components - An external DNS controller that watches for new ingress rules and creates A records in a cluster-specific DNS zone that is external to the cluster. An Nginx Ingress controller that consists of an Azure Load Balancer Service and an Nginx deployment that is configured with L7 load balancing rules as and when ingress rules are created within AKS. As external traffic for a particular application deployed in AKS arrives at a site, it is directed via DNS resolution to the Azure Load Balancer which load balances the traffic across the Nginx servers. The Nginx servers use the HTTP headers in the requests and the configured ingress rules to direct the traffic to a suitable pod in the AKS cluster. The addon supports functions such as SSL termination, cookie-based affinity and URL based forwarding. The addon needs to be enabled for the cluster by running the following command in CloudShell - az aks enable-addons --resource-group \\<rg-for-AKS-cluster> --name \\<AKS-cluster-name> --addons http_application_routing. The purpose of this addon is to quickly create an ingress controller for testing applications. It is not meant for production environments. More details on this add-on are provided at https://docs.microsoft.com/en-us/azure/aks/http-application-routing .","title":"HTTP Application routing Add on"},{"location":"resource-service/aks/#application-gateway-ingress-controller-agic","text":"The AGIC is the native ingress controller for AKS. It integrates the Azure Application Gateway into an AKS cluster to support the ingress controller functionality. Either an existing or a new Application Gateway instance can be used for the purpose. A new Application Gateway instance can be integrated into an AKS cluster using the following command in CloudShell - az aks enable-addons --resource-group \\<rg-for-AKS-cluster> --name \\<AKS-cluster-name> -a ingress-appgw --appgw-subnet-cidr \\<appgw-subnet> --appgw-name \\<app-gw-name> When an ingress rule is created, the Application Gateway instance integrated into the AKS cluster is configured to load balance traffic for the associated application across the requisite set of pods in the cluster. The Fully Qualified Domain Name (FQDN) for an application needs to be manually configured in an external DNS zone with resolution pointing to the IP address of the integrated Application Gateway instance. As external traffic for a particular application deployed in AKS arrives at a site, it is directed via DNS resolution to the Application Gateway instance which in turn uses the HTTP headers in the requests and the configured ingress rules to direct the traffic to a suitable pod in the AKS cluster. In addition to functions such as URL routing, cookie-based affinity and SSL termination, the AGIC provides web application firewall functions and lower latency. It is the recommended method to access applications running in AKS clusters. Other ingress controllers such as nginx, Traefik, HAProxy can be used where functionality such as canary deployments and more flexible load balancing strategies are required. More details on the AGIC are provided at https://docs.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview .","title":"Application Gateway Ingress Controller (AGIC)"},{"location":"resource-service/aks/#aks-troubleshooting","text":"This section discusses troubleshooting and monitoring procedures for AKS clusters.","title":"AKS - Troubleshooting"},{"location":"resource-service/aks/#aks-diagnostics","text":"When experiencing issues with Azure AKS, a good place to start troubleshooting is the \"AKS Diagnose and solve problems\" option available via the AKS Dashboard. When this option is selected a choice of \"Cluster Insights\" and \"Networking\" is presented. Cluster Insights uses cluster logs and the cluster configuration to perform a health check and compare the cluster against best practices. It contains useful information and relevant health indicators in case anything is misconfigured in the cluster. The Networking section of AKS Diagnostics allows interactive troubleshooting of networking issues in the cluster. It presents several possible areas for troubleshooting. Selection of any area will then trigger network health checks and configuration reviews and provide helpful insights. More details on AKS Diagnostics are provided at https://docs.microsoft.com/en-us/azure/aks/concepts-diagnostics .","title":"AKS Diagnostics"},{"location":"resource-service/aks/#aks-container-insights","text":"This is different from the Cluster Insights described above in the sense it is more focused on the presenting different views and analysis of metrics and logs from the AKS cluster to gain deeper insights in the cluster behavior. It needs to be enabled on an AKS cluster that results in the collection of metrics and logs by a containerized version of the Log Analytics agent and aggregation of the same into a Log Analytics workspace. The agent leverages Kubernetes APIs to extract the data from the AKS cluster. Cluster Insights then provides the following functions based on the data stored in the Log Analytics workspace - Cluster Metrics - displays CPU utilization and the memory utilization of all the nodes in the cluster. Optionally additional filters can be added to filter to a particular namespace, node, or node pool. There also is a live option, which provides more real-time information on your cluster status. Reports - A number of pre-configured monitoring workbooks are provided that combine text, log queries, metrics and parameters to display rich interactive reports. Nodes - displays detailed metrics for the AKS cluster nodes as also the location of the various pods running in the AKS cluster. Event logs from the nodes can be viewed as well. Controllers - displays details on all the Kubernetes controllers (ReplicaSets, DaemonSets, StatefulSets, and others) and the pods running within them. Containers - displays the metrics, logs, and environment variables for a container. Deployments - displays the details of all the deployments in an AKS cluster. More details on AKS Container Insights is provided at https://docs.microsoft.com/en-us/azure/azure-monitor/containers/container-insights-overview .","title":"AKS Container Insights"},{"location":"resource-service/aks/#aks-identity-and-access-management","text":"This section covers the integration of AKS clusters with Azure for identity and access management.","title":"AKS - Identity and Access Management"},{"location":"resource-service/aks/#aks-managed-azure-active-directory-integration","text":"Kubernetes clusters leverage authentication plugins to various authentication mechanisms (certificates, bearer tokens, HTTP basic authorization, file based userid-password) to perform authentication of users. AKS however, supports Azure Active Directory (AAD) in addition to all these. Azure users can thus use both the Azure platform and AKS clusters hosted within that platform using the same user-id. However, the Role-based Access Control is separate across the Azure platform and AKS cluster. It is therefore necessary to create roles and authorization rules separately for both. More details on AKS-managed AAD Integration is provided at https://docs.microsoft.com/en-us/azure/aks/managed-aad .","title":"AKS-managed Azure Active Directory Integration"},{"location":"resource-service/aks/#azure-ad-pod-managed-identities-in-aks","text":"Managed identities in Azure are used by applications or Azure functions running in VMs to authenticate to Azure AD and obtain authorization to access other resources. Azure supports two types of managed identities - System assigned - This type of managed identity is dedicated to one resource such as a VM. Its lifecycle is identical to that of the resource, which means it is deleted along with the resource. User assigned - this type of managed identity has its own lifecycle and can be shared by multiple resources. A managed identity is passed to Azure AD through a special endpoint called the Instance Metadata Service (IMDS) which uses a certificate configured for that managed identity. This results in a token being returned to the requestor that can be used to authenticate to other resources and authorize their use. Managed identities are easier to manage than service principals and therefore, the recommended approach for the authentication and authorization of applications that need access to other Azure resources. Note that VMs can be configured with managed identities which, by default, could be used by the multiple pods running in that VM. The Azure AD pod-managed identities add-on for AKS configures the AKS cluster such that pods can no longer access the IMDS endpoint directly. Instead all pods trying to access this endpoint connect to a DaemonSet called the Node Managed Identity (NMI) which validates the pod request then leverages the IMDS to acquire a token and return it to the requesting pod. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/aks/use-managed-identity .","title":"Azure AD pod-managed identities in AKS"},{"location":"resource-service/aks/#aks-security","text":"This section covers the integration of AKS clusters with Azure for security services.","title":"AKS - Security"},{"location":"resource-service/aks/#aks-cluster-compliance-and-threat-detection","text":"Azure Policy is a service that helps to enforce organizational standards and to assess compliance at-scale. For Kubernetes and specifially AKS, Azure Policy leverages a tool called Gatekeeper which functions as an Admission Controller webhook for an AKS cluster. All authentication requests and Kubernetes API calls are intercepted and sent to GateKeeper. Gatekeeper in turn uses a tool Open Policy Agent (OPA) to detect actions that are non-compliant with set policies and report to Azure Policy. OPA also watches for instances of Custom Resource Definitions (CRDs) that define policies and for non-compliance by standard Kubernetes resources. Azure Policy needs to be enabled in order for policies to be assigned to the AKS cluster and for the Admission Controller webhook to be established. More details on Azure Policy for AKS are provided at https://docs.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes . Azure Defender provides threat detection capabilities for AKS clusters by monitoring logs and behavior patterns. More details on Azure Defender for AKS are provided at https://docs.microsoft.com/en-us/azure/security-center/defender-for-kubernetes-introduction .","title":"AKS Cluster Compliance and Threat Detection"},{"location":"resource-service/aks/#aks-cluster-secrets-and-azure-key-vault","text":"Secrets in Kubernetes are used to hold sensitive information such as passwords or connection strings. Secrets are held in the persistent store used by the Kubernetes control plane and are unencrypted by default. Any one with access to the Kubernetes API or the persistent store can read secrets. Also any one with the ability to create pods in a Kubernetes namespace can read secrets in that namespace. For AKS clusters, Azure Key Vault provides a more secure way of working with secrets. This approach relies on a Container Storage Interface (CSI) driver that enables mounting of secrets as volumes. Typical steps to leverage Key Vault in AKS clusters are - Set up the CSI Driver for Key Vault in the AKS cluster Create a user assigned managed identity that will control access to a Key Vault instance Create a Key Vault instance Bind the user assigned managed identity to the Key Vault instance. This gives all resources that are given this managed identity permissions to operate the Key Vault instance Assign the user assigned managed identity to the resources that need access to secrets in the Key Vault instance More details on use of Azure Key Vault for AKS secrets are provided at https://docs.microsoft.com/en-us/azure/aks/csi-secrets-store-driver .","title":"AKS Cluster Secrets and Azure Key Vault"},{"location":"resource-service/aks/#aks-azure-paas","text":"This section covers the use of Azure PaaS services by AKS clusters.","title":"AKS - Azure PaaS"},{"location":"resource-service/aks/#azure-service-operator-aso","text":"Azure provides many managed services, database services being one. The benefit to using these services is that aspects such scalability, security, high availability, disaster recovery and backup are all taken care as part of the managed service. Therefore it is beneficial for all workloads, including those running in AKS clusters, to leverage these managed services. ASO is a Kubernetes Operator for AKS clusters that provisions Azure services and connects applications running within an AKS cluster to these services. A Kubernetes Operator is basically a way to extend the Kubernetes APIs with custom resources and consists of two components - A Custom Resource Definition (CRD) that can be used to instantiate a custom resource in the AKS Cluster. In case of ASO, each custom resource represents one instance of an Azure service that can be managed like any other Kubernetes resource. A Controller that watches for the creation/changes/deletion of these custom resources and takes suitable action. In the case of ASO, it interacts with the Azure API to create/modify/delete the associated Azure resources. The Controller for ASO needs to be linked to a managed-identity that has permissions to create resources in Azure. In addition, the ASO Controller may leverage Key Vaults to hold secrets such as connection strings for databases. More details on ASO are provided at https://github.com/Azure/azure-service-operator .","title":"Azure Service Operator (ASO)"},{"location":"resource-service/aks/#aks-serverless-and-azure-functions","text":"Serverless refers to an environment for a workload where servers need not be managed. Resources are consumed only when there is work to be done. Also the environment scales automatically with the workload. AKS clusters can support serverless environments using Azure Functions. This service consists of two components - the Functions runtime and the Kubernetes-based Event Driven Autoscaling scale controller. The latter monitors the rate at which events are arriving at a function and scales the function appropriately. In addition, a Container Registry to hold the images for the Azure Functions will be required. More details on use of Azure Functions with AKS are provided at https://docs.microsoft.com/en-us/azure/azure-functions/functions-kubernetes-keda .","title":"AKS Serverless and Azure Functions"},{"location":"resource-service/compute/","text":"Azure Compute \u2693\ufe0e Azure Compute provides computing resources to run applications on various Azure services like Virtual Machines, Dedicated Hosts, Containers, App Services, and serverless Azure Functions. Major Azure Compute services are as below: Azure Virtual Machine \u2693\ufe0e Azure Virtual Machine supports Linux and Windows Operating Systems (OS) with VM configuration upto 416 vCPUs, 12 TB memory and up to 3.7 million local storage IOPS per VM. We can have up to 30 Gbps Ethernet and 200 Gbps InfiniBand. Azure provides multiple VM families for supporting different type of workloads. Azure provides various Virtual Machine Series to provision Virtual Machines based on the workloads/applications which are going to run on them. Most common ones are general purpose compute (D-Series), economical burstable VMs (Bs-Series), optimized for In-memory and hyper-threaded applications (E-Series), compute optimized VMs (F-Series), memory and storage Optimized (G-Series), High Performance Computing (HPC) VMs (H-Series), GPU enabled VMs (N-Series). Pricing can be chosen as \u2018pay as you go\u2019 for per-second billing. For long term commitment, Reserved Instance (RI) are good which provides savings up to 72% than pay as you go model and RI with Hybrid benefits provides up to 80% cost saving. Spot instances can also be chosen for the workloads which don\u2019t run mission critical applications and can tolerate state loss. These instances provide better cost savings than pay as you go or reserved instances. For more details, please refer Azure Virtual Machine Virtual Machine Scale Sets (VMSS) \u2693\ufe0e If workloads are unpredictable in nature and require demand-based scalability, VMSS is the option which can scale from one to thousands of identical VMs instances behind Azure Load Balancer or Application Gateway. Auto scaling can be scheduled or automated based on VMs performance metrics. VM Scale Sets provides redundancy, improved performance, and resiliency to the applications. VMSS can perform automatic distribution of VM instances across Availability Zones or Availability Sets. For more details, please refer VMSS . There are two basic ways to configure VMs deployed in a scale set: \u00b7 Use extensions to configure the VM after it's deployed. With this approach, new VM instances may take longer to start up than a VM with no extensions. \u00b7 Deploy a managed disk with a custom disk image. This option may be quicker to deploy. However, this requires to keep the image up-to-date. Azure Dedicated Hosts \u2693\ufe0e Azure Dedicated Host should be the preferred choice if there is a need to control the maintenance window, gain visibility over the underlying infrastructure, and place Azure VMs on a single tenant server to satisfy specific compliance or regulatory requirements. For more details, please refer Azure Dedicated Hosts . Azure Batch \u2693\ufe0e Azure Batch helps in scheduling cloud-scale jobs and compute management for HPC applications. Customer can choose between Linux or Windows platform to run jobs which can auto scale based on jobs in the queue, and this can be used to stage data and execute compute pipelines. For more details, please refer Azure Batch Azure App Service \u2693\ufe0e Azure App Service is a fully managed service for building, deploying and scaling web and API apps. This provides support for apps written in multiple languages or containers and built in CI/CD integrations with zero-downtime deployments. For enterprise-grade service, deploy isolated web service with a single-tenancy model, use automatic performance and security patching for simplified operations, Web App Firewall for protecting apps, Azure Monitor for performance and end-to-end health monitoring, Application Insights for deeper insights like app\u2019s throughput, response time, dependencies and error trends; and Active Directory and other identity providers to authenticate and authorize app access. For more details, please refer Azure App Service . Azure Kubernetes Service \u2693\ufe0e Azure Kubernetes Service (AKS) should be chosen if customer has containerized applications and wants to deploy and manage easily with a fully managed Kubernetes service. AKS offers serverless Kubernetes, an integrated continuous integration and continuous delivery (CI/CD) experience, and enterprise-grade security and governance. This unites development and operations teams on a single platform to rapidly build, deliver, and scale containerized applications and simplifies the deployment, management, and operations of Kubernetes. AKS supports elastic compute capacity provisioning without the need to manage the infrastructure and provides ability to add event-driven autoscaling and triggers through KEDA (Kubernetes Event Driven Autoscaling). This provisions fully managed clusters with Prometheus-based monitoring capabilities. AKS provides operational efficiencies with automated provisioning, repair, monitoring, and scaling. We can save on costs by using deeply discounted capacity with Azure Spot and can achieve higher availability and protect applications from datacenter failures using Availability Zones. AKS provides advanced cluster management features with environment visibility through Kubernetes resource\u2019s view, control-plane telemetry, log aggregation, and container health. These can be accessed through Azure portal, CLI and PowerShell. For more details, please refer AKS . Azure Container Instances (ACI) \u2693\ufe0e Azure Container Instances help in running containers on Azure without managing any underlying servers. This can be used for elastic bursting with Azure Kubernetes Service (AKS), i.e. When traffic spikes, provision additional compute resources with virtual kubelet for demanding AKS workloads within ACI. ACI can also be used for data processing jobs and event-driven applications with Azure logic Apps. For more details, please refer ACI . Azure Service Fabric \u2693\ufe0e Azure Service Fabric can be used for building and operating scalable, distributed apps. This can be used with various languages and programming models, simplifying microservices development and application lifecycle management. This can be scaled to thousands of VMs and provides data-aware platform for low-latency and high-throughput workloads with stateful containers and microservices. For more details, please refer Azure Service Fabric Azure Functions \u2693\ufe0e Azure Functions is an event-driven serverless compute platform that can solve complex orchestration problems. This allows to build and debug locally without additional setup, deploy and operate at scale in the cloud, and integrate services using triggers and bindings. For more details, please refer Azure Functions . Azure Cloud Services \u2693\ufe0e Azure Cloud Services is one the oldest Azure PaaS technology and provides Web and Worker roles to deploy web and backend applications respectively. Customer needs to just provide application code along with desired configuration in form of a package and Azure creates VMs and deploys applications on top of the VMs to auto scale and run applications reliably. Cloud Services takes care of operating system and application updates and provides increased security and integrated health monitoring and load balancing capabilities. Cloud Services offers a lot of flexibility when compared with WebApps and has quite less management overhead than Azure Virtual Machines (IaaS), but as this service can get deprecated soon, the recommendation is to consider other PaaS offering like WebApps. For more details, please refer Azure Cloud Services . Pre-Provisioned VMs and partner solutions \u2693\ufe0e Identify if any application needs to be deployed from Azure Marketplace for infrastructure, apps or custom solutions, or from Microsoft\u2019s AppSource for line of business apps and services. This provides readily available, quick and easy solution deployment and maintenance for partner solutions. Azure Compute Selection \u2693\ufe0e Azure Compute selection should be based on workload\u2019s technical, business and governance requirements. While defining Azure Landing Zone, identifying compute needs which needs to be supported by the design is one of the critical requirements. There may be one or more Azure compute service which are required to satisfy customer needs and below decision tree can be leveraged to identify the right Azure Compute selection. source Sr No. Use case Suggested Compute Service 1. \u00b7 Need full control and build new Linux/Windows VM \u00b7 Lift and Shift migration \u00b7 Wants full OS control and can\u2019t containerize \u00b7 Wants to reuse existing Windows licenses \u00b7 Quickly build/test/decommission VMs for PoC/test/dev purposes \u00b7 Create purpose-built hosts, e.g., Bastion Host/Jump Host, Active Directory Domain Server, SQL Server on VM etc. \u00b7 Run workloads like SAP, Oracle, High-Performance Computing applications, Graphics Processing Unit applications etc. Azure Virtual Machine 2. \u00b7 Achieve high availability by autoscaling to create thousands of VMs in minutes \u00b7 Quickly scale out/scale in VMs or autoscaling is required \u00b7 Application/workload demand is un-predictable \u00b7 To centrally manage, configure, and update many VMs together Virtual Machine Scale Sets 3. \u00b7 Require full control of underlying resources due to compliance, licensing, or custom update cycle \u00b7 Wants to have own virtualization Azure Dedicated Hosts 4. \u00b7 Wants to run High Performance Computing (HPC) workloads \u00b7 Cloud-scale job scheduling and compute management with the ability to scale to thousands of virtual machines Azure Batch 5. \u00b7 No need of full-fledged orchestration and wants to build web or mobile app on a fully Azure managed platform \u00b7 \u201cLift and Shift\u201d migration, can\u2019t containerize and wants to run Web/API app \u00b7 Create web or mobile apps quickly Azure App Service 6. \u00b7 Apps can be containerized and require full-fledged orchestration of the underlying platform \u00b7 \u201cLift and Shift\u201d container migration where apps are already containerized and needs full-fledged container orchestration platform \u00b7 Requires simplified Kubernetes deployment, management, and operations Azure Kubernetes Service (AKS) 7. \u00b7 Build New, apps can be containerized and don\u2019t require full-fledged container orchestration platform Azure Container Instances (ACI) 8. \u00b7 Needs .NET integration or fully supported Microsoft stack \u00b7 Develop microservices and orchestrate containers on Windows and Linux Azure Service Fabric 9. \u00b7 Event-driven workload with short lived processes \u00b7 Workloads with serverless architecture Azure Functions 10. \u00b7 Create highly available, scalable cloud applications and APIs that can help to focus on applications instead of hardware. *One of the oldest PaaS service and can get deprecated soon. Azure Cloud Services 11. \u00b7 Looking for readily available deployable solutions from partners \u00b7 Need quick deployment/requires supported services Pre-Provisioned VMs and partner solutions Design Recommendations \u2693\ufe0e Sr No. Area Recommendations 1. Region Selection \u00b7 Resource deployment region selection should be made based on user\u2019s locations and customer\u2019s business strategy \u00b7 Validate that required resources are available in the region 2. SLAs \u00b7 Identify SLAs requirement and consider deployment accordingly. \u00b7 For High SLAs, please consider Azure service and architecture accordingly. E.g., For Single Azure VM with standards Disks, it provides 99.5% SLA while with premium Disks, it provides 99.9% SLA, for higher SLA deploy VMs in at least 2 Fault Domains in VMSS which will result in 99.95% SLA and for further higher SLA deploy 2 or more VMs across two or more Availability Zones which will provide 99.99% SLA. Hence based on need, please include/exclude components with right design aspects. \u2693\ufe0e","title":"Azure Compute"},{"location":"resource-service/compute/#azure-compute","text":"Azure Compute provides computing resources to run applications on various Azure services like Virtual Machines, Dedicated Hosts, Containers, App Services, and serverless Azure Functions. Major Azure Compute services are as below:","title":"Azure Compute"},{"location":"resource-service/compute/#azure-virtual-machine","text":"Azure Virtual Machine supports Linux and Windows Operating Systems (OS) with VM configuration upto 416 vCPUs, 12 TB memory and up to 3.7 million local storage IOPS per VM. We can have up to 30 Gbps Ethernet and 200 Gbps InfiniBand. Azure provides multiple VM families for supporting different type of workloads. Azure provides various Virtual Machine Series to provision Virtual Machines based on the workloads/applications which are going to run on them. Most common ones are general purpose compute (D-Series), economical burstable VMs (Bs-Series), optimized for In-memory and hyper-threaded applications (E-Series), compute optimized VMs (F-Series), memory and storage Optimized (G-Series), High Performance Computing (HPC) VMs (H-Series), GPU enabled VMs (N-Series). Pricing can be chosen as \u2018pay as you go\u2019 for per-second billing. For long term commitment, Reserved Instance (RI) are good which provides savings up to 72% than pay as you go model and RI with Hybrid benefits provides up to 80% cost saving. Spot instances can also be chosen for the workloads which don\u2019t run mission critical applications and can tolerate state loss. These instances provide better cost savings than pay as you go or reserved instances. For more details, please refer Azure Virtual Machine","title":"Azure Virtual Machine"},{"location":"resource-service/compute/#virtual-machine-scale-sets-vmss","text":"If workloads are unpredictable in nature and require demand-based scalability, VMSS is the option which can scale from one to thousands of identical VMs instances behind Azure Load Balancer or Application Gateway. Auto scaling can be scheduled or automated based on VMs performance metrics. VM Scale Sets provides redundancy, improved performance, and resiliency to the applications. VMSS can perform automatic distribution of VM instances across Availability Zones or Availability Sets. For more details, please refer VMSS . There are two basic ways to configure VMs deployed in a scale set: \u00b7 Use extensions to configure the VM after it's deployed. With this approach, new VM instances may take longer to start up than a VM with no extensions. \u00b7 Deploy a managed disk with a custom disk image. This option may be quicker to deploy. However, this requires to keep the image up-to-date.","title":"Virtual Machine Scale Sets (VMSS)"},{"location":"resource-service/compute/#azure-dedicated-hosts","text":"Azure Dedicated Host should be the preferred choice if there is a need to control the maintenance window, gain visibility over the underlying infrastructure, and place Azure VMs on a single tenant server to satisfy specific compliance or regulatory requirements. For more details, please refer Azure Dedicated Hosts .","title":"Azure Dedicated Hosts"},{"location":"resource-service/compute/#azure-batch","text":"Azure Batch helps in scheduling cloud-scale jobs and compute management for HPC applications. Customer can choose between Linux or Windows platform to run jobs which can auto scale based on jobs in the queue, and this can be used to stage data and execute compute pipelines. For more details, please refer Azure Batch","title":"Azure Batch"},{"location":"resource-service/compute/#azure-app-service","text":"Azure App Service is a fully managed service for building, deploying and scaling web and API apps. This provides support for apps written in multiple languages or containers and built in CI/CD integrations with zero-downtime deployments. For enterprise-grade service, deploy isolated web service with a single-tenancy model, use automatic performance and security patching for simplified operations, Web App Firewall for protecting apps, Azure Monitor for performance and end-to-end health monitoring, Application Insights for deeper insights like app\u2019s throughput, response time, dependencies and error trends; and Active Directory and other identity providers to authenticate and authorize app access. For more details, please refer Azure App Service .","title":"Azure App Service"},{"location":"resource-service/compute/#azure-kubernetes-service","text":"Azure Kubernetes Service (AKS) should be chosen if customer has containerized applications and wants to deploy and manage easily with a fully managed Kubernetes service. AKS offers serverless Kubernetes, an integrated continuous integration and continuous delivery (CI/CD) experience, and enterprise-grade security and governance. This unites development and operations teams on a single platform to rapidly build, deliver, and scale containerized applications and simplifies the deployment, management, and operations of Kubernetes. AKS supports elastic compute capacity provisioning without the need to manage the infrastructure and provides ability to add event-driven autoscaling and triggers through KEDA (Kubernetes Event Driven Autoscaling). This provisions fully managed clusters with Prometheus-based monitoring capabilities. AKS provides operational efficiencies with automated provisioning, repair, monitoring, and scaling. We can save on costs by using deeply discounted capacity with Azure Spot and can achieve higher availability and protect applications from datacenter failures using Availability Zones. AKS provides advanced cluster management features with environment visibility through Kubernetes resource\u2019s view, control-plane telemetry, log aggregation, and container health. These can be accessed through Azure portal, CLI and PowerShell. For more details, please refer AKS .","title":"Azure Kubernetes Service"},{"location":"resource-service/compute/#azure-container-instances-aci","text":"Azure Container Instances help in running containers on Azure without managing any underlying servers. This can be used for elastic bursting with Azure Kubernetes Service (AKS), i.e. When traffic spikes, provision additional compute resources with virtual kubelet for demanding AKS workloads within ACI. ACI can also be used for data processing jobs and event-driven applications with Azure logic Apps. For more details, please refer ACI .","title":"Azure Container Instances (ACI)"},{"location":"resource-service/compute/#azure-service-fabric","text":"Azure Service Fabric can be used for building and operating scalable, distributed apps. This can be used with various languages and programming models, simplifying microservices development and application lifecycle management. This can be scaled to thousands of VMs and provides data-aware platform for low-latency and high-throughput workloads with stateful containers and microservices. For more details, please refer Azure Service Fabric","title":"Azure Service Fabric"},{"location":"resource-service/compute/#azure-functions","text":"Azure Functions is an event-driven serverless compute platform that can solve complex orchestration problems. This allows to build and debug locally without additional setup, deploy and operate at scale in the cloud, and integrate services using triggers and bindings. For more details, please refer Azure Functions .","title":"Azure Functions"},{"location":"resource-service/compute/#azure-cloud-services","text":"Azure Cloud Services is one the oldest Azure PaaS technology and provides Web and Worker roles to deploy web and backend applications respectively. Customer needs to just provide application code along with desired configuration in form of a package and Azure creates VMs and deploys applications on top of the VMs to auto scale and run applications reliably. Cloud Services takes care of operating system and application updates and provides increased security and integrated health monitoring and load balancing capabilities. Cloud Services offers a lot of flexibility when compared with WebApps and has quite less management overhead than Azure Virtual Machines (IaaS), but as this service can get deprecated soon, the recommendation is to consider other PaaS offering like WebApps. For more details, please refer Azure Cloud Services .","title":"Azure Cloud Services"},{"location":"resource-service/compute/#pre-provisioned-vms-and-partner-solutions","text":"Identify if any application needs to be deployed from Azure Marketplace for infrastructure, apps or custom solutions, or from Microsoft\u2019s AppSource for line of business apps and services. This provides readily available, quick and easy solution deployment and maintenance for partner solutions.","title":"Pre-Provisioned VMs and partner solutions"},{"location":"resource-service/compute/#azure-compute-selection","text":"Azure Compute selection should be based on workload\u2019s technical, business and governance requirements. While defining Azure Landing Zone, identifying compute needs which needs to be supported by the design is one of the critical requirements. There may be one or more Azure compute service which are required to satisfy customer needs and below decision tree can be leveraged to identify the right Azure Compute selection. source Sr No. Use case Suggested Compute Service 1. \u00b7 Need full control and build new Linux/Windows VM \u00b7 Lift and Shift migration \u00b7 Wants full OS control and can\u2019t containerize \u00b7 Wants to reuse existing Windows licenses \u00b7 Quickly build/test/decommission VMs for PoC/test/dev purposes \u00b7 Create purpose-built hosts, e.g., Bastion Host/Jump Host, Active Directory Domain Server, SQL Server on VM etc. \u00b7 Run workloads like SAP, Oracle, High-Performance Computing applications, Graphics Processing Unit applications etc. Azure Virtual Machine 2. \u00b7 Achieve high availability by autoscaling to create thousands of VMs in minutes \u00b7 Quickly scale out/scale in VMs or autoscaling is required \u00b7 Application/workload demand is un-predictable \u00b7 To centrally manage, configure, and update many VMs together Virtual Machine Scale Sets 3. \u00b7 Require full control of underlying resources due to compliance, licensing, or custom update cycle \u00b7 Wants to have own virtualization Azure Dedicated Hosts 4. \u00b7 Wants to run High Performance Computing (HPC) workloads \u00b7 Cloud-scale job scheduling and compute management with the ability to scale to thousands of virtual machines Azure Batch 5. \u00b7 No need of full-fledged orchestration and wants to build web or mobile app on a fully Azure managed platform \u00b7 \u201cLift and Shift\u201d migration, can\u2019t containerize and wants to run Web/API app \u00b7 Create web or mobile apps quickly Azure App Service 6. \u00b7 Apps can be containerized and require full-fledged orchestration of the underlying platform \u00b7 \u201cLift and Shift\u201d container migration where apps are already containerized and needs full-fledged container orchestration platform \u00b7 Requires simplified Kubernetes deployment, management, and operations Azure Kubernetes Service (AKS) 7. \u00b7 Build New, apps can be containerized and don\u2019t require full-fledged container orchestration platform Azure Container Instances (ACI) 8. \u00b7 Needs .NET integration or fully supported Microsoft stack \u00b7 Develop microservices and orchestrate containers on Windows and Linux Azure Service Fabric 9. \u00b7 Event-driven workload with short lived processes \u00b7 Workloads with serverless architecture Azure Functions 10. \u00b7 Create highly available, scalable cloud applications and APIs that can help to focus on applications instead of hardware. *One of the oldest PaaS service and can get deprecated soon. Azure Cloud Services 11. \u00b7 Looking for readily available deployable solutions from partners \u00b7 Need quick deployment/requires supported services Pre-Provisioned VMs and partner solutions","title":"Azure Compute Selection"},{"location":"resource-service/compute/#design-recommendations","text":"Sr No. Area Recommendations 1. Region Selection \u00b7 Resource deployment region selection should be made based on user\u2019s locations and customer\u2019s business strategy \u00b7 Validate that required resources are available in the region 2. SLAs \u00b7 Identify SLAs requirement and consider deployment accordingly. \u00b7 For High SLAs, please consider Azure service and architecture accordingly. E.g., For Single Azure VM with standards Disks, it provides 99.5% SLA while with premium Disks, it provides 99.9% SLA, for higher SLA deploy VMs in at least 2 Fault Domains in VMSS which will result in 99.95% SLA and for further higher SLA deploy 2 or more VMs across two or more Availability Zones which will provide 99.99% SLA. Hence based on need, please include/exclude components with right design aspects.","title":"Design Recommendations"},{"location":"resource-service/compute/#_1","text":"","title":""},{"location":"resource-service/storage/","text":"Azure Storage \u2693\ufe0e Azure storage provides unmanaged/managed, secure, scalable, durable, and highly available storage solutions which should be chosen based on workload\u2019s technical, business and governance requirements. Please refer storage concepts for Azure storage basics. Various Azure storage options are as below: Azure Blobs \u2693\ufe0e Azure Blob storage is Microsoft\u2019s object storage, which is ideal for storing massive amounts of unstructured data. Azure Storage supports three types of blobs: \u00b7 Block blobs store text and binary data. Block blobs are made up of blocks of data that can be managed individually. \u00b7 Append blobs are made up of blocks like block blobs but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines. \u00b7 Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines. Azure Files \u2693\ufe0e Azure Files offers fully managed file shares in the cloud which can be accessed using Server Message Block (SMB) or Network File System (NFS) protocols. Azure file shares can be mounted concurrently by cloud or on-premises deployments. Azure Files SMB file shares are accessible from Windows, Linux, and macOS clients and Azure Files NFS file shares are accessible from Linux or macOS clients. Additionally, Azure Files SMB file shares can be cached on Windows Servers with Azure File Sync for fast access near where the data is being used. Azure NetApp Files \u2693\ufe0e For enterprises, running bare-metal performance and sub-millisecond latency-sensitive file workloads consider Azure NetApp Files which helps to migrate and run complex, file-based applications with no code change. Azure NetApp Files can be used as underlying shared file-storage service for various scenarios like migration (lift and shift) of POSIX-compliant Linux and Windows applications, SAP HANA, databases, high-performance compute (HPC) infrastructure, complex enterprise workloads, virtual desktop infrastructure (VDI, apps and enterprise web applications. Azure NetApp Files provides three performance tiers: Standard, Premium and Ultra which can be provided with a simple click, allowing unmatched flexibility. Azure Queues \u2693\ufe0e Azure Queues Storage is used to store large numbers of messages which can be accessed via HTTP or HTTPS authenticated calls from anywhere in the world. A queue message can be up to 64 KB in size and a queue may contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously. Azure Tables \u2693\ufe0e Azure Table storage stores large amounts of structured data. This can be used to store and query huge sets of structured, non-relational data, and tables can scale as the demand increases. The service is a NoSQL datastore which accepts authenticated calls from inside and outside the Azure cloud. \u2693\ufe0e Azure Cosmos DB Table API \u2693\ufe0e Azure Cosmos DB Table API, alternative of Azure table storage API offers high performance and availability, global distribution and automatic secondary indexes. This is also available in consumption-based serverless mode. For more details, can refer Azure Cosmos DB Table API . Azure Disks \u2693\ufe0e Azure managed disks are block-level storage volumes that are managed by Azure and used with Azure Virtual Machines. The available types of disks are ultra-disks, premium solid-state drives (SSD), standard SSDs, and standard hard disk drives (HDD). For more details about each individual disk type, please refer Select a disk type for IaaS VMs . Azure Data Lake Storage Gen2 \u2693\ufe0e Azure Blob storage also supports Azure Data Lake Storage Gen2, Microsoft's enterprise big data analytics solution for the cloud. It offers a hierarchical file system in addition to Blob storage advantages including low-cost, tiered storage, High availability, strong consistency, Disaster recovery capabilities. Azure Databox \u2693\ufe0e Azure Databox should be used when customer has to move large amount of data to Azure within limited time, network availability and costs. All data is AES encrypted and devices are wiped clean after upload. Azure Databox provides below options: \u00b7 Data Box : This comes with 100TB capacity and supports NAS protocol \u00b7 Data Box Disk : This comes with 8-TB SSD with a USB/SATA interface and has 128-bit encryption. This comes in packs of up to five for a total of 40 TB and can be customized based on the needs. \u00b7 Data Box Heavy : This is self-contained device and is designed to lift 1 PB of data to the cloud. \u00b7 Data Box Gateway : Data Box Gateway also transfers data to and from Azure\u2014but it is a virtual appliance. \u2693\ufe0e Azure Confidential Ledger (Preview) \u2693\ufe0e Azure Confidential Ledger provides a managed and decentralised ledger for data entries backed by Blockchain. It helps in maintaining data integrity by preventing unauthorised or accidental modification with tamperproof storage. It protects data at rest, in transit and in use with hardware-backed secure enclaves used in Azure confidential computing. This stores unstructured data and can be cryptographically verified. Azure Storage Selections \u2693\ufe0e While defining Azure Landing Zone and Workloads, identifying right Azure Storage needs along with right sizing is a critical requirement. Sr No. Use case Suggested Storage Service 1. \u00b7 Store VM & application logs and analytics data \u00b7 Store downloadable media (images, audio, videos, documents, or other media) \u00b7 For Backup, DR, Archiving \u00b7 For Data Lakes, High Performance Computing, ML Azure Blob storage 2. \u00b7 Solution which requires high performance, high transactions rates, smaller objects, low storage latency etc. \u00b7 For AI, ML, IoT, Analytics use cases Azure Premium Block Blob 3. \u00b7 File shares (SMB/NFS) for high-performance scale applications on cloud or on-premises deployments \u00b7 Serverless enterprise-grade cloud file shares Azure Files 4. \u00b7 Enterprise file storage like SAP, powered by NetApp Azure NetApp Files 5. \u00b7 Expand an existing on-premises file share with on-prem windows VM as caching server for Azure File share \u00b7 Migration of unstructured data Azure File Sync or partner solutions 6. \u00b7 Premium blobs share for use cases like Azure Disks, collaborative video editing, incremental snapshot, app and data live migration, SAS based shared access etc. Azure Page Blobs 7. \u00b7 Build flexible applications and separate functions for better durability across large workloads. \u00b7 Messaging store for reliable messaging between application components Azure Queue Storage 8. \u00b7 NoSQL datastore for schema less storage of structured, non-relational data \u00b7 Storing datasets that don't require complex joins, foreign keys, or stored procedures \u00b7 Quickly querying data using a clustered index Azure Table Storage 9. \u00b7 High performing global distribution and automatic secondary indexes \u00b7 Dedicated throughput worldwide and Single digit millisecond latency Azure Cosmos DB Table API 10. \u00b7 High-performance Azure managed VM Disks \u00b7 Variety of use cases support, using ultra disks, premium solid-state drives (SSD), standard SSDs, and standard hard disk drives (HDD) Azure Disks 11. \u00b7 Big Data Analytics Workloads \u00b7 Massively scalable and secure data lake for high-performance analytics workloads Azure Data Lake Storage gen 2 12. \u00b7 Appliances and solutions for offline data transfer to and from Azure \u00b7 Large-scale archiving and syncing of on-premises data to the cloud Azure Data Box 13. \u00b7 Store unstructured data that is completely tamper-proof and can be cryptographically verified Microsoft Azure Confidential Ledger 14. \u00b7 Hybrid cloud storage for on-premises High-Performance Computing (HPC) Read-Heavy workloads Avere vFXT for Azure Storage Design Recommendations \u2693\ufe0e Sr No. Area Recommendations 1. Region Selection \u00b7 Azure storage region should be selected based on based on various parameters likes legal and organization requirement user\u2019s locations, cost etc. \u00b7 Validate that required services are available in the chosen region 2. Data governance, management and Migration Requirement \u00b7 Consider legal, location and business sector data residency and compliance requirements and select services accordingly. Refer whitepaper for more details. \u00b7 Various Azure partners solutions can be leveraged for same if needed. 3. Data Availability & Retention \u00b7 Data replication across datacenters or geographical regions should be chosen for additional protection against local catastrophe or natural disaster. \u00b7 LRS, ZRS, GRS, RA-GRS, GZRS, RA-GZRS should be chosen based on need and availability. \u00b7 This will also ensure that replicated data remains highly available in case of unexpected outage. \u00b7 Consider data retention duration as per business needs. 4. Data Encryption \u00b7 Data at rest, and Data in flight should be encrypted and Azure Key Management Service (KMS) can be leveraged for data encryption using Server-Side Encryption (SSE) or Client-Side Encryption (CSE). Can refer more details at Data encryption models 5. Data Accessibility Authorization \u00b7 Azure Storage supports fine-grained control to define who has data access and same should be reviewed and finalized carefully before providing data access to any application, user or service account. \u00b7 Various data access mechanisms, like Azure Active Directory (AD), Shared Access Signature (SAS), Shared Key, Anonymous public read access etc. should be considered based on data type, usage and priority. Please refer link for more details. 6. SLAs \u00b7 Identify business storage SLAs requirement and choose storage services based on their SLAs accordingly. 7. Data Accessibility Methods \u00b7 Object in Azure Blob storage is accessible across world over HTTP or HTTPS. This can also be accessed from various language client libraries like .NET, Java, Node.js, Python, PHP, Ruby, Go and others, as well as REST API. Azure Storage also supports scripting in Azure PowerShell or Azure CLI. This data can also be easily visualized through Azure Portal or Azure Storage Explorer. \u00b7 Files in Azure Files can be accessed using SMB protocol, REST interface or other storage client libraries. These can also be accessed using URLs with SAS tokens which are applicable only for specified duration. \u00b7 Finalize required data accessibility and don\u2019t provide access for other mechanisms. 8. Azure account storage capacity \u00b7 For large accounts, please refer storage account capacity and consumed/available quota for the chosen service. See Scalability and performance targets for standard storage accounts","title":"Azure Storage"},{"location":"resource-service/storage/#azure-storage","text":"Azure storage provides unmanaged/managed, secure, scalable, durable, and highly available storage solutions which should be chosen based on workload\u2019s technical, business and governance requirements. Please refer storage concepts for Azure storage basics. Various Azure storage options are as below:","title":"Azure Storage"},{"location":"resource-service/storage/#azure-blobs","text":"Azure Blob storage is Microsoft\u2019s object storage, which is ideal for storing massive amounts of unstructured data. Azure Storage supports three types of blobs: \u00b7 Block blobs store text and binary data. Block blobs are made up of blocks of data that can be managed individually. \u00b7 Append blobs are made up of blocks like block blobs but are optimized for append operations. Append blobs are ideal for scenarios such as logging data from virtual machines. \u00b7 Page blobs store virtual hard drive (VHD) files and serve as disks for Azure virtual machines.","title":"Azure Blobs"},{"location":"resource-service/storage/#azure-files","text":"Azure Files offers fully managed file shares in the cloud which can be accessed using Server Message Block (SMB) or Network File System (NFS) protocols. Azure file shares can be mounted concurrently by cloud or on-premises deployments. Azure Files SMB file shares are accessible from Windows, Linux, and macOS clients and Azure Files NFS file shares are accessible from Linux or macOS clients. Additionally, Azure Files SMB file shares can be cached on Windows Servers with Azure File Sync for fast access near where the data is being used.","title":"Azure Files"},{"location":"resource-service/storage/#azure-netapp-files","text":"For enterprises, running bare-metal performance and sub-millisecond latency-sensitive file workloads consider Azure NetApp Files which helps to migrate and run complex, file-based applications with no code change. Azure NetApp Files can be used as underlying shared file-storage service for various scenarios like migration (lift and shift) of POSIX-compliant Linux and Windows applications, SAP HANA, databases, high-performance compute (HPC) infrastructure, complex enterprise workloads, virtual desktop infrastructure (VDI, apps and enterprise web applications. Azure NetApp Files provides three performance tiers: Standard, Premium and Ultra which can be provided with a simple click, allowing unmatched flexibility.","title":"Azure NetApp Files"},{"location":"resource-service/storage/#azure-queues","text":"Azure Queues Storage is used to store large numbers of messages which can be accessed via HTTP or HTTPS authenticated calls from anywhere in the world. A queue message can be up to 64 KB in size and a queue may contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously.","title":"Azure Queues"},{"location":"resource-service/storage/#azure-tables","text":"Azure Table storage stores large amounts of structured data. This can be used to store and query huge sets of structured, non-relational data, and tables can scale as the demand increases. The service is a NoSQL datastore which accepts authenticated calls from inside and outside the Azure cloud.","title":"Azure Tables"},{"location":"resource-service/storage/#_1","text":"","title":""},{"location":"resource-service/storage/#azure-cosmos-db-table-api","text":"Azure Cosmos DB Table API, alternative of Azure table storage API offers high performance and availability, global distribution and automatic secondary indexes. This is also available in consumption-based serverless mode. For more details, can refer Azure Cosmos DB Table API .","title":"Azure Cosmos DB Table API"},{"location":"resource-service/storage/#azure-disks","text":"Azure managed disks are block-level storage volumes that are managed by Azure and used with Azure Virtual Machines. The available types of disks are ultra-disks, premium solid-state drives (SSD), standard SSDs, and standard hard disk drives (HDD). For more details about each individual disk type, please refer Select a disk type for IaaS VMs .","title":"Azure Disks"},{"location":"resource-service/storage/#azure-data-lake-storage-gen2","text":"Azure Blob storage also supports Azure Data Lake Storage Gen2, Microsoft's enterprise big data analytics solution for the cloud. It offers a hierarchical file system in addition to Blob storage advantages including low-cost, tiered storage, High availability, strong consistency, Disaster recovery capabilities.","title":"Azure Data Lake Storage Gen2"},{"location":"resource-service/storage/#azure-databox","text":"Azure Databox should be used when customer has to move large amount of data to Azure within limited time, network availability and costs. All data is AES encrypted and devices are wiped clean after upload. Azure Databox provides below options: \u00b7 Data Box : This comes with 100TB capacity and supports NAS protocol \u00b7 Data Box Disk : This comes with 8-TB SSD with a USB/SATA interface and has 128-bit encryption. This comes in packs of up to five for a total of 40 TB and can be customized based on the needs. \u00b7 Data Box Heavy : This is self-contained device and is designed to lift 1 PB of data to the cloud. \u00b7 Data Box Gateway : Data Box Gateway also transfers data to and from Azure\u2014but it is a virtual appliance.","title":"Azure Databox"},{"location":"resource-service/storage/#_2","text":"","title":""},{"location":"resource-service/storage/#azure-confidential-ledger-preview","text":"Azure Confidential Ledger provides a managed and decentralised ledger for data entries backed by Blockchain. It helps in maintaining data integrity by preventing unauthorised or accidental modification with tamperproof storage. It protects data at rest, in transit and in use with hardware-backed secure enclaves used in Azure confidential computing. This stores unstructured data and can be cryptographically verified.","title":"Azure Confidential Ledger (Preview)"},{"location":"resource-service/storage/#azure-storage-selections","text":"While defining Azure Landing Zone and Workloads, identifying right Azure Storage needs along with right sizing is a critical requirement. Sr No. Use case Suggested Storage Service 1. \u00b7 Store VM & application logs and analytics data \u00b7 Store downloadable media (images, audio, videos, documents, or other media) \u00b7 For Backup, DR, Archiving \u00b7 For Data Lakes, High Performance Computing, ML Azure Blob storage 2. \u00b7 Solution which requires high performance, high transactions rates, smaller objects, low storage latency etc. \u00b7 For AI, ML, IoT, Analytics use cases Azure Premium Block Blob 3. \u00b7 File shares (SMB/NFS) for high-performance scale applications on cloud or on-premises deployments \u00b7 Serverless enterprise-grade cloud file shares Azure Files 4. \u00b7 Enterprise file storage like SAP, powered by NetApp Azure NetApp Files 5. \u00b7 Expand an existing on-premises file share with on-prem windows VM as caching server for Azure File share \u00b7 Migration of unstructured data Azure File Sync or partner solutions 6. \u00b7 Premium blobs share for use cases like Azure Disks, collaborative video editing, incremental snapshot, app and data live migration, SAS based shared access etc. Azure Page Blobs 7. \u00b7 Build flexible applications and separate functions for better durability across large workloads. \u00b7 Messaging store for reliable messaging between application components Azure Queue Storage 8. \u00b7 NoSQL datastore for schema less storage of structured, non-relational data \u00b7 Storing datasets that don't require complex joins, foreign keys, or stored procedures \u00b7 Quickly querying data using a clustered index Azure Table Storage 9. \u00b7 High performing global distribution and automatic secondary indexes \u00b7 Dedicated throughput worldwide and Single digit millisecond latency Azure Cosmos DB Table API 10. \u00b7 High-performance Azure managed VM Disks \u00b7 Variety of use cases support, using ultra disks, premium solid-state drives (SSD), standard SSDs, and standard hard disk drives (HDD) Azure Disks 11. \u00b7 Big Data Analytics Workloads \u00b7 Massively scalable and secure data lake for high-performance analytics workloads Azure Data Lake Storage gen 2 12. \u00b7 Appliances and solutions for offline data transfer to and from Azure \u00b7 Large-scale archiving and syncing of on-premises data to the cloud Azure Data Box 13. \u00b7 Store unstructured data that is completely tamper-proof and can be cryptographically verified Microsoft Azure Confidential Ledger 14. \u00b7 Hybrid cloud storage for on-premises High-Performance Computing (HPC) Read-Heavy workloads Avere vFXT for Azure","title":"Azure Storage Selections"},{"location":"resource-service/storage/#storage-design-recommendations","text":"Sr No. Area Recommendations 1. Region Selection \u00b7 Azure storage region should be selected based on based on various parameters likes legal and organization requirement user\u2019s locations, cost etc. \u00b7 Validate that required services are available in the chosen region 2. Data governance, management and Migration Requirement \u00b7 Consider legal, location and business sector data residency and compliance requirements and select services accordingly. Refer whitepaper for more details. \u00b7 Various Azure partners solutions can be leveraged for same if needed. 3. Data Availability & Retention \u00b7 Data replication across datacenters or geographical regions should be chosen for additional protection against local catastrophe or natural disaster. \u00b7 LRS, ZRS, GRS, RA-GRS, GZRS, RA-GZRS should be chosen based on need and availability. \u00b7 This will also ensure that replicated data remains highly available in case of unexpected outage. \u00b7 Consider data retention duration as per business needs. 4. Data Encryption \u00b7 Data at rest, and Data in flight should be encrypted and Azure Key Management Service (KMS) can be leveraged for data encryption using Server-Side Encryption (SSE) or Client-Side Encryption (CSE). Can refer more details at Data encryption models 5. Data Accessibility Authorization \u00b7 Azure Storage supports fine-grained control to define who has data access and same should be reviewed and finalized carefully before providing data access to any application, user or service account. \u00b7 Various data access mechanisms, like Azure Active Directory (AD), Shared Access Signature (SAS), Shared Key, Anonymous public read access etc. should be considered based on data type, usage and priority. Please refer link for more details. 6. SLAs \u00b7 Identify business storage SLAs requirement and choose storage services based on their SLAs accordingly. 7. Data Accessibility Methods \u00b7 Object in Azure Blob storage is accessible across world over HTTP or HTTPS. This can also be accessed from various language client libraries like .NET, Java, Node.js, Python, PHP, Ruby, Go and others, as well as REST API. Azure Storage also supports scripting in Azure PowerShell or Azure CLI. This data can also be easily visualized through Azure Portal or Azure Storage Explorer. \u00b7 Files in Azure Files can be accessed using SMB protocol, REST interface or other storage client libraries. These can also be accessed using URLs with SAS tokens which are applicable only for specified duration. \u00b7 Finalize required data accessibility and don\u2019t provide access for other mechanisms. 8. Azure account storage capacity \u00b7 For large accounts, please refer storage account capacity and consumed/available quota for the chosen service. See Scalability and performance targets for standard storage accounts","title":"Storage Design Recommendations"},{"location":"resource-service/vmagents/","text":"Azure Virtual Machine (VM) Agents \u2693\ufe0e The core functionality from a workload perspective is generally built into the operating system (OS) image that is run in a VM. However, there are additional requirements such as configuration, additional software installation, monitoring, security and resiliency that are in general addressed by agents/extensions that are installed in the Guest OS. Azure VM extensions are small applications that provide post-deployment configuration and automation tasks on Azure VMs. For example, if a virtual machine requires software installation, anti-virus protection, or to run a script inside of it, a VM extension can be used. Azure VM extensions can be run with the Azure CLI, PowerShell, Azure Resource Manager templates, and the Azure portal. Extensions can be bundled with a new VM deployment, or run against any existing system. This document discusses the functions and installation of the agents/extensions that are pertinent to managed services such as monitoring, security and backup that have been covered in previous sections. The intent is to detail the additional installation and configuration required to support managed services for VMs. The agents/extensions to be discussed are classified into the following groups - Core Agents for Windows VMs Core Agents for Linux VMs VM Extensions for Security VM Extensions for Resiliency VM Extensions for Azure Monitor Third-party VM Extensions Core Agents for Windows VMs \u2693\ufe0e Functions Supported \u2693\ufe0e The Azure VM Agent is a secure lightweight process running in a Windows VM that performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Azure VM Agent supports automatic collection and transfer of Event Logs, OS Logs, Azure Logs and some registry keys to the VM's host for use in the investigation of issues. Install Extensions NOTE: The Azure Fabric Controller is part of the Azure management plane. It is responsible to assign infrastructure resources that Azure resources being created by customers map to. Agents on VMs access this Controller using the special IP address of 168.63.129.16. Azure VM Agent Installation \u2693\ufe0e The Azure VM Agent is pre-installed on any Windows VM deployed from an Azure Marketplace image and can be configured using any of the Azure Portal, PowerShell, Command Line Interface, or an Azure Resource Manager template. In this case updates of the Azure VM Agent are automatically performed. When using a custom VM image, the VM Agent installer can be downloaded and manually installed. In this case updates of the Azure VM Agent need to be performed manually. The Windows package has two parts - Provisioning Agent (PA) and Windows Guest Agent (WinGA). It is assumed that the PA assists with the VM configuration while the WinGA gathers and loads various logs into the VM's host and also provides an environment for other VM extensions. Verification of this assumption will be taken up shortly and the document updated based on the findings. Core Agents for Linux VMs \u2693\ufe0e Functions Supported \u2693\ufe0e The Linux VM (Guest) Agent runs in a Linux VM and performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Image Provisioning, Networking and Kernel Configuration (Details can be viewed at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-linux ) Install Extensions Linux VM Agent Installation \u2693\ufe0e The Linux VM Agent is pre-installed in images obtained from the Azure Marketplace. Azure endorses certain Linux distributions that integrate the Linux VM Agent package into their images and repositories. When creating custom images from such endorsed Linux distributions, Installation can use the relevant RPM or DEB package. The Linux VM Agent uses a configuration file (/etc/waagent.conf) that specifies if configuration is to be performed at provisioning time and if so, what the configuration should be. It is our understanding a default configuration is provided with all images obtained from the Azure Marketplace. For other images it is expected the configuration is provided by the image builder. With regard to updates, it is our understanding VM instances based on images obtained from the Azure Marketplace will have their Linux VM Agents updated automatically. Other instances will need manual updation. Virtual Machine (VM) Extensions for Security \u2693\ufe0e Some commonly used extensions for Security are covered briefly below. Azure Disk Encryption for Linux \u2693\ufe0e This extension leverages the dm-crypt subsystem in Linux to provide full disk encryption on select Azure Linux distributions and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-linux . Azure Disk Encryption for Windows \u2693\ufe0e This extension leverages BitLocker to provide full disk encryption on Azure virtual machines running Windows and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-windows . Key Vault for Linux \u2693\ufe0e This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-linux . Key Vault for Windows \u2693\ufe0e This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-windows . Azure Policy guest configuration for Linux and Windows \u2693\ufe0e This extension performs audit and configuration operations inside virtual machines for policies such as security baseline definitions. It is needed by policies such as security baseline definitions for VMs that need to check settings inside the VMs. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/guest-configuration . Desired State Configuration (DSC) for Linux \u2693\ufe0e This extension is used to install the Open Management Infrastructure (OMI) and DSC agents that in turn leverage the Azure Automation service to ensure the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. Note that currently this extension cannot co-exist with Log Analytics Agent. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-linux . Desired State Configuration (DSC) for Windows \u2693\ufe0e This extension is used to download a Powershell DSC configuration to a VM and to call into the Powershell DSC to enact the received DSC configuration, thus ensuring the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-windows . Azure Defender for Servers \u2693\ufe0e Azure Defender adds a number of security protections for Windows and Linux VMs two of which are Defender for Endpoint and Vulnerability scanning. Defender for Endpoint provides comprehensive endpoint threat detection and response. When enabled for Windows VMs, Defender for Endpoint uses the sensor built into Windows that provides data to Defender for Endpoint, presumably through the Log Analytics agent. When enabled for Linux VMs that run the supported Linux distributions, the integration of the auditd service with the Log Analytics agent is leveraged. There is no need to explicitly install an agent for either Windows or Linux VMs. Vulnerability scanning uses a vulnerability scanner provided by Qualys. An extension is provided that needs to be run on those Windows and Linux VMs for which Azure Defender is enabled and that are identified by Security Center as suitable to run the extension. So installation and configuration is typically driven through the Security Center interface on the Azure Portal. Microsoft Antimalware for Windows \u2693\ufe0e This is a real-time protection that helps identify and remove viruses, spyware, and other malicious software. It leverages the Antimalware extension that needs to be installed on a Windows VM when it is being either created or existing. Along with the extension binary a configuration file is pushed to the Windows VM for use in the configuration of the extension. Note that for certain older versions of Windows, this feature needs to be installed for malware protection, along with Defender for Endpoint. For the more recent versions of Windows, it is our understanding that Defender for Endpoint will cover malware protection as well. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/security/fundamentals/antimalware . VM Extensions for Resiliency \u2693\ufe0e Some of the extensions that support Backup are discussed briefly below. Azure Backup for SQL Server \u2693\ufe0e Azure Backup supports backup of SQL Server running in Azure VMs. Since it needs permission to access the application and fetch the necessary details, it installs the AzureBackupWindowsWorkload extension on the VM when it is registered for backup. More details on this are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/backup-azure-sql-server-running-azure-vm . Microsoft Azure Recovery Services (MARS) Agent \u2693\ufe0e This agent is used to selectively protect Windows files and folders, protect entire Windows volumes and entire Windows system state. It is to be first downloaded from the Azure Portal and installed manually on the Windows VMs that need any of the protections listed above. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-about-mars . Microsoft Azure Backup Server (MABS) for VMware \u2693\ufe0e MABS can be used to protect VMware VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. No agents are required as MABS uses the vCenter Server and the hypervisor to perform the operations. More details on this approach are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-backup-server-vmware . Microsoft Azure Backup Server (MABS) for Hyper-V \u2693\ufe0e MABS can do a host or guest-level backup of Hyper-V VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. At the host level, the MABS protection agent is installed on the Hyper-V host server or cluster and protects the entire VMs and data files running on that host. At the guest level, the agent is installed on each virtual machine and protects the workload present on that machine. There are pros and cons for both approaches that are detailed at https://docs.microsoft.com/en-us/azure/backup/back-up-hyper-v-virtual-machines-mabs . Azure Disk Backup \u2693\ufe0e Azure Disk Backup is a native, cloud-based backup solution that protects data in managed disks. It is a crash-consistent solution that uses incremental snapshots. Azure Backup Vault is used to configure backup for the managed disks. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/disk-backup-overview . Azure Blob Backup \u2693\ufe0e This is a managed, local data protection solution that protects blobs from corruption and accidental deletion. It uses the same storage account as the blob itself. It is managed from either the Backup Vault or the Backup Center. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/blob-backup-overview . Azure File Share Backup \u2693\ufe0e This is a native, cloud based backup solution that protects file shares and eliminates any overheads due to on-premises backup solutions. It leverages the Azure Backup services to take snapshots that are then held in the same storage account as the file share being backed up. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/azure-file-share-backup-overview . SAP HANA Database on Azure VM Backup \u2693\ufe0e Azure Backup service supports backup of SAP HANA databases running on Azure VMs. Each Azure VM running SAP HANA databases is registered with a Recovery Services vault and the databases to be backed up are discovered. Azure Backup service then installs an Azure Backup Plugin for HANA on the Azure VM. Backup of databases can now be configured. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/sap-hana-db-about . VM Snapshot for Azure Backup for Linux \u2693\ufe0e This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-linux . VM Snapshot for Azure Backup for Windows \u2693\ufe0e This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-windows . VM Extensions for Azure Monitor \u2693\ufe0e Azure Monitor leverages a number of options in terms of agents that warrant a more detailed discussion. Linux VM Extensions \u2693\ufe0e The recently launched Azure Monitor agent consolidates the main functions of Metrics and Log gathering and provides additional capabilities such as sending data to multiple workspaces and improved management of extensions. However it has certain drawbacks in comparison to the existing agents such as lack of support for gathering file based and IIS logs. It is to be installed manually via the Portal or CLI. Simultaneous installation on multiple VMs can be performed through the creation of Data Collection Rules. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/azure-monitor-agent-overview?tabs=PowerShellWindows . In contrast there are multiple existing agents each of which is described in terms of its functionality below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Syslog and Performance data to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-linux . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-linux . Linux Diagnostic Extension (LAD) and Telegraf agent - Can be installed on a VM either as it is deployed or existing. The LAD gathers syslog, other file based logs and performance counters and sends them to Azure Blob Storage and Event Hubs. The Telegraf agent can be used to send performance counters to Azure Monitor Metrics. More details on these are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-linux . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on an existing VM. For unknown reasons this extension does not appear to be available for installation on VMs as they are being created. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics Windows VM Extensions \u2693\ufe0e The recently launched Azure Monitor agent functions in the same manner for Windows as described above for Linux. In contrast the multiple existing agents are described below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Event Logs, Performance data, File based logs, IIS logs and other data in support of various Insights services to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-windows . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-windows Windows Diagnostics Extension (WAD) - Can be installed on a VM either as it is deployed or existing. The WAD gathers Windows Event Logs, a variety of other logs and performance counters and sends them to Azure Storage, Application Insights and Event Hub. It also sends the performance counters to Azure Monitor Metrics. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-windows . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics Recommendations \u2693\ufe0e The recently launched Azure Monitor agent has the following limitations - Lack of production support for Azure Security Center and Azure Sentinel services Lack of production support for Azure Monitor services such as VM Insights, VM Insights guest health, SQL Insights Lack of Production support for Azure solutions such as Change Tracking and Update Management The recommendation, given the expectation that most engagements will involve managed services for Production workloads, is to continue with the use of the multiple existing agents while waiting for the newly launched Azure Monitor agent to mature. A detailed comparison of the newly launched Azure Monitor agent with the existing agents can be viewed at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/agents-overview . Third-party VM Extensions \u2693\ufe0e A number of third party VM extensions are also available covering monitoring, security, resiliency and installation of drivers for special-purpose hardware such as GPUs. This section covers two such third-party extensions that are worth mentioning. Datadog Extension for Monitoring of Linux and Windows VMs \u2693\ufe0e Datadog agents can be be installed as extensions to gather metrics, traces and logs from Azure VMs (both Linux and Windows) and send them to the Datadog service which can then allow users to view the data through dashboards, graphs and monitors. The service can also be configured to send alerts. The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Datadog account needs to be set up and an API Key obtained that can then be used by agents to connect to the Datadog service. The agents can also collect application metrics so correlation of an application\u2019s performance with the VM level metrics can be performed. The agent monitors services running in an Azure VM, such as IIS and SQL Server, as well as non-Windows integrations such as MySQL, NGINX, and Cassandra. More details on this extension are provided at https://docs.datadoghq.com/integrations/guide/azure-portal/ . Symantec Cloud Workload Protection (CWP) Extension for Linux and Windows VMs \u2693\ufe0e Symantec CWP agents can be installed as extensions to provide security for Azure VMs (both Linux and Windows) with application protection, intrusion detection/prevention, real-time Anti-Malware, and real-time file integrity monitoring (RT-FIM). The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Cloud Workload Protection account needs to be set up with Symantec from which credentials such as Customer ID, Domain ID and Customer Secret Key need to be obtained. These credentials then need to be used in the configuration of the agent on a VM. More details on this extension are provided at https://techdocs.broadcom.com/us/en/symantec-security-software/endpoint-security-and-management/cloud-workload-protection/1-0/Getting_Started_1/architecture-azure-v133783726-d187e9316.html . Summary \u2693\ufe0e The table below summarizes the manner of installation for all the agents/extensions discussed above. It is assumed that the core VM agents are present by default, either because Azure standard images were used or the core VM agents were installed as one of the first configuration steps performed on VMs that were booted from custom images. VM Agent/Extension Standard Image Custom Image Disk Encryption for Linux Manual Manual Disk Encryption for Windows Manual Manual Key Vault for Linux Manual Manual Key Vault for Windows Manual Manual Policy Guest Configuration (DSC) for Linux Manual Manual Policy Guest Configuration (DSC) for Windows Manual Manual Defender for Endpoint for Linux Pre-installed Pre-installed Defender for Endpoint for Windows Pre-installed Pre-installed Vulnerability Scanner for Linux Manual Manual Vulnerability Scanner for Windows Manual Manual Microsoft Antimalware for Windows Manual Manual Backup for SQL Server for Windows Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server MARS Agent for Windows Manual Manual MABS for VMware No agents required No agents required MABS for Hyper-V Manual Manual Disk Backup No agents required No agents required Blob Backup No agents required No agents required File Share Backup No agents required No agents required SAP HANA Database Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server VM Snapshot for Azure Backup for Linux Auto-installed on triggering of first backup Auto-installed on triggering of first backup VM Snapshot for Azure Backup for Windows Auto-installed on triggering of first backup Auto-installed on triggering of first backup Monitor Agent for Linux Manual Manual Monitor Agent for Windows Manual Manual Log Analytics Agent for Linux Auto-installed on Security Center enablement Auto-installed on Security Center enablement Log Analytics Agent for Windows Auto-installed on Security Center enablement Auto-installed on Security Center enablement Dependency Agent for Linux Manual Manual Dependency Agent for Windows Manual Manual Linux Diagnostic Agent and Telegraf Agent Manual Manual Windows Diagnostic Agent Manual Manual Network Watcher for Linux Manual Manual Network Watcher for Windows Manual Manual Performance Diagnostics agent for Linux Manual Manual Performance Diagnostics agent for Manual Manual Datadog extenstion for Monitoring of Linux and Windows Manual Manual Symantec Cloud Workload Protection extension for Linux and Windows Manual Manual","title":"VM Agents"},{"location":"resource-service/vmagents/#azure-virtual-machine-vm-agents","text":"The core functionality from a workload perspective is generally built into the operating system (OS) image that is run in a VM. However, there are additional requirements such as configuration, additional software installation, monitoring, security and resiliency that are in general addressed by agents/extensions that are installed in the Guest OS. Azure VM extensions are small applications that provide post-deployment configuration and automation tasks on Azure VMs. For example, if a virtual machine requires software installation, anti-virus protection, or to run a script inside of it, a VM extension can be used. Azure VM extensions can be run with the Azure CLI, PowerShell, Azure Resource Manager templates, and the Azure portal. Extensions can be bundled with a new VM deployment, or run against any existing system. This document discusses the functions and installation of the agents/extensions that are pertinent to managed services such as monitoring, security and backup that have been covered in previous sections. The intent is to detail the additional installation and configuration required to support managed services for VMs. The agents/extensions to be discussed are classified into the following groups - Core Agents for Windows VMs Core Agents for Linux VMs VM Extensions for Security VM Extensions for Resiliency VM Extensions for Azure Monitor Third-party VM Extensions","title":"Azure Virtual Machine (VM) Agents"},{"location":"resource-service/vmagents/#core-agents-for-windows-vms","text":"","title":"Core Agents for Windows VMs"},{"location":"resource-service/vmagents/#functions-supported","text":"The Azure VM Agent is a secure lightweight process running in a Windows VM that performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Azure VM Agent supports automatic collection and transfer of Event Logs, OS Logs, Azure Logs and some registry keys to the VM's host for use in the investigation of issues. Install Extensions NOTE: The Azure Fabric Controller is part of the Azure management plane. It is responsible to assign infrastructure resources that Azure resources being created by customers map to. Agents on VMs access this Controller using the special IP address of 168.63.129.16.","title":"Functions Supported"},{"location":"resource-service/vmagents/#azure-vm-agent-installation","text":"The Azure VM Agent is pre-installed on any Windows VM deployed from an Azure Marketplace image and can be configured using any of the Azure Portal, PowerShell, Command Line Interface, or an Azure Resource Manager template. In this case updates of the Azure VM Agent are automatically performed. When using a custom VM image, the VM Agent installer can be downloaded and manually installed. In this case updates of the Azure VM Agent need to be performed manually. The Windows package has two parts - Provisioning Agent (PA) and Windows Guest Agent (WinGA). It is assumed that the PA assists with the VM configuration while the WinGA gathers and loads various logs into the VM's host and also provides an environment for other VM extensions. Verification of this assumption will be taken up shortly and the document updated based on the findings.","title":"Azure VM Agent Installation"},{"location":"resource-service/vmagents/#core-agents-for-linux-vms","text":"","title":"Core Agents for Linux VMs"},{"location":"resource-service/vmagents/#functions-supported_1","text":"The Linux VM (Guest) Agent runs in a Linux VM and performs the following functions Interact with the Azure Fabric Controller to Obtain an IP address via DHCP Perform name resolution using Azure DNS Respond to health check requests from Load Balancers Image Provisioning, Networking and Kernel Configuration (Details can be viewed at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-linux ) Install Extensions","title":"Functions Supported"},{"location":"resource-service/vmagents/#linux-vm-agent-installation","text":"The Linux VM Agent is pre-installed in images obtained from the Azure Marketplace. Azure endorses certain Linux distributions that integrate the Linux VM Agent package into their images and repositories. When creating custom images from such endorsed Linux distributions, Installation can use the relevant RPM or DEB package. The Linux VM Agent uses a configuration file (/etc/waagent.conf) that specifies if configuration is to be performed at provisioning time and if so, what the configuration should be. It is our understanding a default configuration is provided with all images obtained from the Azure Marketplace. For other images it is expected the configuration is provided by the image builder. With regard to updates, it is our understanding VM instances based on images obtained from the Azure Marketplace will have their Linux VM Agents updated automatically. Other instances will need manual updation.","title":"Linux VM Agent Installation"},{"location":"resource-service/vmagents/#virtual-machine-vm-extensions-for-security","text":"Some commonly used extensions for Security are covered briefly below.","title":"Virtual Machine (VM) Extensions for Security"},{"location":"resource-service/vmagents/#azure-disk-encryption-for-linux","text":"This extension leverages the dm-crypt subsystem in Linux to provide full disk encryption on select Azure Linux distributions and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-linux .","title":"Azure Disk Encryption for Linux"},{"location":"resource-service/vmagents/#azure-disk-encryption-for-windows","text":"This extension leverages BitLocker to provide full disk encryption on Azure virtual machines running Windows and is integrated with Azure Key Vault to manage disk encryption keys and secrets. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/azure-disk-enc-windows .","title":"Azure Disk Encryption for Windows"},{"location":"resource-service/vmagents/#key-vault-for-linux","text":"This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-linux .","title":"Key Vault for Linux"},{"location":"resource-service/vmagents/#key-vault-for-windows","text":"This extension monitors a list of observed certificates stored in key vaults, and, upon detecting a change, retrieves, and installs the corresponding certificates on the VM. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/key-vault-windows .","title":"Key Vault for Windows"},{"location":"resource-service/vmagents/#azure-policy-guest-configuration-for-linux-and-windows","text":"This extension performs audit and configuration operations inside virtual machines for policies such as security baseline definitions. It is needed by policies such as security baseline definitions for VMs that need to check settings inside the VMs. It is installed on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/guest-configuration .","title":"Azure Policy guest configuration for Linux and Windows"},{"location":"resource-service/vmagents/#desired-state-configuration-dsc-for-linux","text":"This extension is used to install the Open Management Infrastructure (OMI) and DSC agents that in turn leverage the Azure Automation service to ensure the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. Note that currently this extension cannot co-exist with Log Analytics Agent. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-linux .","title":"Desired State Configuration (DSC) for Linux"},{"location":"resource-service/vmagents/#desired-state-configuration-dsc-for-windows","text":"This extension is used to download a Powershell DSC configuration to a VM and to call into the Powershell DSC to enact the received DSC configuration, thus ensuring the VM configuration is consistent with desired state. The extension is installed and run on a VM either as it is created or existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/dsc-windows .","title":"Desired State Configuration (DSC) for Windows"},{"location":"resource-service/vmagents/#azure-defender-for-servers","text":"Azure Defender adds a number of security protections for Windows and Linux VMs two of which are Defender for Endpoint and Vulnerability scanning. Defender for Endpoint provides comprehensive endpoint threat detection and response. When enabled for Windows VMs, Defender for Endpoint uses the sensor built into Windows that provides data to Defender for Endpoint, presumably through the Log Analytics agent. When enabled for Linux VMs that run the supported Linux distributions, the integration of the auditd service with the Log Analytics agent is leveraged. There is no need to explicitly install an agent for either Windows or Linux VMs. Vulnerability scanning uses a vulnerability scanner provided by Qualys. An extension is provided that needs to be run on those Windows and Linux VMs for which Azure Defender is enabled and that are identified by Security Center as suitable to run the extension. So installation and configuration is typically driven through the Security Center interface on the Azure Portal.","title":"Azure Defender for Servers"},{"location":"resource-service/vmagents/#microsoft-antimalware-for-windows","text":"This is a real-time protection that helps identify and remove viruses, spyware, and other malicious software. It leverages the Antimalware extension that needs to be installed on a Windows VM when it is being either created or existing. Along with the extension binary a configuration file is pushed to the Windows VM for use in the configuration of the extension. Note that for certain older versions of Windows, this feature needs to be installed for malware protection, along with Defender for Endpoint. For the more recent versions of Windows, it is our understanding that Defender for Endpoint will cover malware protection as well. More details on this feature are provided at https://docs.microsoft.com/en-us/azure/security/fundamentals/antimalware .","title":"Microsoft Antimalware for Windows"},{"location":"resource-service/vmagents/#vm-extensions-for-resiliency","text":"Some of the extensions that support Backup are discussed briefly below.","title":"VM Extensions for Resiliency"},{"location":"resource-service/vmagents/#azure-backup-for-sql-server","text":"Azure Backup supports backup of SQL Server running in Azure VMs. Since it needs permission to access the application and fetch the necessary details, it installs the AzureBackupWindowsWorkload extension on the VM when it is registered for backup. More details on this are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/backup-azure-sql-server-running-azure-vm .","title":"Azure Backup for SQL Server"},{"location":"resource-service/vmagents/#microsoft-azure-recovery-services-mars-agent","text":"This agent is used to selectively protect Windows files and folders, protect entire Windows volumes and entire Windows system state. It is to be first downloaded from the Azure Portal and installed manually on the Windows VMs that need any of the protections listed above. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-about-mars .","title":"Microsoft Azure Recovery Services (MARS) Agent"},{"location":"resource-service/vmagents/#microsoft-azure-backup-server-mabs-for-vmware","text":"MABS can be used to protect VMware VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. No agents are required as MABS uses the vCenter Server and the hypervisor to perform the operations. More details on this approach are provided at https://docs.microsoft.com/en-us/azure/backup/backup-azure-backup-server-vmware .","title":"Microsoft Azure Backup Server (MABS) for VMware"},{"location":"resource-service/vmagents/#microsoft-azure-backup-server-mabs-for-hyper-v","text":"MABS can do a host or guest-level backup of Hyper-V VMs. MABS itself needs to be installed on a physical server for backing up on-premises hosts and on an Azure VM for backing up Azure VMs. At the host level, the MABS protection agent is installed on the Hyper-V host server or cluster and protects the entire VMs and data files running on that host. At the guest level, the agent is installed on each virtual machine and protects the workload present on that machine. There are pros and cons for both approaches that are detailed at https://docs.microsoft.com/en-us/azure/backup/back-up-hyper-v-virtual-machines-mabs .","title":"Microsoft Azure Backup Server (MABS) for Hyper-V"},{"location":"resource-service/vmagents/#azure-disk-backup","text":"Azure Disk Backup is a native, cloud-based backup solution that protects data in managed disks. It is a crash-consistent solution that uses incremental snapshots. Azure Backup Vault is used to configure backup for the managed disks. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/disk-backup-overview .","title":"Azure Disk Backup"},{"location":"resource-service/vmagents/#azure-blob-backup","text":"This is a managed, local data protection solution that protects blobs from corruption and accidental deletion. It uses the same storage account as the blob itself. It is managed from either the Backup Vault or the Backup Center. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/blob-backup-overview .","title":"Azure Blob Backup"},{"location":"resource-service/vmagents/#azure-file-share-backup","text":"This is a native, cloud based backup solution that protects file shares and eliminates any overheads due to on-premises backup solutions. It leverages the Azure Backup services to take snapshots that are then held in the same storage account as the file share being backed up. No agent/extension needs to be set up in VMs for the purpose. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/azure-file-share-backup-overview .","title":"Azure File Share Backup"},{"location":"resource-service/vmagents/#sap-hana-database-on-azure-vm-backup","text":"Azure Backup service supports backup of SAP HANA databases running on Azure VMs. Each Azure VM running SAP HANA databases is registered with a Recovery Services vault and the databases to be backed up are discovered. Azure Backup service then installs an Azure Backup Plugin for HANA on the Azure VM. Backup of databases can now be configured. More details on this solution are provided at https://docs.microsoft.com/en-us/azure/backup/sap-hana-db-about .","title":"SAP HANA Database on Azure VM Backup"},{"location":"resource-service/vmagents/#vm-snapshot-for-azure-backup-for-linux","text":"This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-linux .","title":"VM Snapshot for Azure Backup for Linux"},{"location":"resource-service/vmagents/#vm-snapshot-for-azure-backup-for-windows","text":"This extension supports an application consistent backup of the Azure virtual machine without the need to shutdown the VM. An installation of this extension is performed by Azure Backup when a backup is initiated for the first time. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/vmsnapshot-windows .","title":"VM Snapshot for Azure Backup for Windows"},{"location":"resource-service/vmagents/#vm-extensions-for-azure-monitor","text":"Azure Monitor leverages a number of options in terms of agents that warrant a more detailed discussion.","title":"VM Extensions for Azure Monitor"},{"location":"resource-service/vmagents/#linux-vm-extensions","text":"The recently launched Azure Monitor agent consolidates the main functions of Metrics and Log gathering and provides additional capabilities such as sending data to multiple workspaces and improved management of extensions. However it has certain drawbacks in comparison to the existing agents such as lack of support for gathering file based and IIS logs. It is to be installed manually via the Portal or CLI. Simultaneous installation on multiple VMs can be performed through the creation of Data Collection Rules. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/azure-monitor-agent-overview?tabs=PowerShellWindows . In contrast there are multiple existing agents each of which is described in terms of its functionality below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Syslog and Performance data to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-linux . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-linux . Linux Diagnostic Extension (LAD) and Telegraf agent - Can be installed on a VM either as it is deployed or existing. The LAD gathers syslog, other file based logs and performance counters and sends them to Azure Blob Storage and Event Hubs. The Telegraf agent can be used to send performance counters to Azure Monitor Metrics. More details on these are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-linux . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on an existing VM. For unknown reasons this extension does not appear to be available for installation on VMs as they are being created. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics","title":"Linux VM Extensions"},{"location":"resource-service/vmagents/#windows-vm-extensions","text":"The recently launched Azure Monitor agent functions in the same manner for Windows as described above for Linux. In contrast the multiple existing agents are described below. Log Analytics Agent - Installed by the associated VM extension which can be set up either automatically when Security Center is enabled for a subscription or manually. The agent sends Event Logs, Performance data, File based logs, IIS logs and other data in support of various Insights services to Log Analytics workspace. More details on this agent are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/oms-windows . Dependency Agent extension - The extension can be installed on a VM either as it is created or existing. The extension in turn sets up an agent that sends Process Dependencies and Network connection metrics via the Log Analytics agent to Log Analytics workspace. This information is leveraged by the Maps feature of Azure Monitor for VMs. More details on these are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/agent-dependency-windows Windows Diagnostics Extension (WAD) - Can be installed on a VM either as it is deployed or existing. The WAD gathers Windows Event Logs, a variety of other logs and performance counters and sends them to Azure Storage, Application Insights and Event Hub. It also sends the performance counters to Azure Monitor Metrics. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/diagnostics-extension-overview?context=/azure/virtual-machines/context/context . In addition to the above, there are certain extensions pertaining to performance that are documented below. Network Watcher - needed to support some of the Network Watcher features such as Connection Monitor and capturing network traffic on demand. It can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/azure/virtual-machines/extensions/network-watcher-windows . Performance Diagnostics extension - runs a diagnostics tool named PerfInsights. The resulting data is held in a storage account. The extension can be run on a VM as it is deployed or already existing. More details on this extension are provided at https://docs.microsoft.com/en-us/troubleshoot/azure/virtual-machines/performance-diagnostics","title":"Windows VM Extensions"},{"location":"resource-service/vmagents/#recommendations","text":"The recently launched Azure Monitor agent has the following limitations - Lack of production support for Azure Security Center and Azure Sentinel services Lack of production support for Azure Monitor services such as VM Insights, VM Insights guest health, SQL Insights Lack of Production support for Azure solutions such as Change Tracking and Update Management The recommendation, given the expectation that most engagements will involve managed services for Production workloads, is to continue with the use of the multiple existing agents while waiting for the newly launched Azure Monitor agent to mature. A detailed comparison of the newly launched Azure Monitor agent with the existing agents can be viewed at https://docs.microsoft.com/en-us/azure/azure-monitor/agents/agents-overview .","title":"Recommendations"},{"location":"resource-service/vmagents/#third-party-vm-extensions","text":"A number of third party VM extensions are also available covering monitoring, security, resiliency and installation of drivers for special-purpose hardware such as GPUs. This section covers two such third-party extensions that are worth mentioning.","title":"Third-party VM Extensions"},{"location":"resource-service/vmagents/#datadog-extension-for-monitoring-of-linux-and-windows-vms","text":"Datadog agents can be be installed as extensions to gather metrics, traces and logs from Azure VMs (both Linux and Windows) and send them to the Datadog service which can then allow users to view the data through dashboards, graphs and monitors. The service can also be configured to send alerts. The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Datadog account needs to be set up and an API Key obtained that can then be used by agents to connect to the Datadog service. The agents can also collect application metrics so correlation of an application\u2019s performance with the VM level metrics can be performed. The agent monitors services running in an Azure VM, such as IIS and SQL Server, as well as non-Windows integrations such as MySQL, NGINX, and Cassandra. More details on this extension are provided at https://docs.datadoghq.com/integrations/guide/azure-portal/ .","title":"Datadog Extension for Monitoring of Linux and Windows VMs"},{"location":"resource-service/vmagents/#symantec-cloud-workload-protection-cwp-extension-for-linux-and-windows-vms","text":"Symantec CWP agents can be installed as extensions to provide security for Azure VMs (both Linux and Windows) with application protection, intrusion detection/prevention, real-time Anti-Malware, and real-time file integrity monitoring (RT-FIM). The extension can be run on a VM as it is deployed or already existing. Prior to the installation of agents, a Cloud Workload Protection account needs to be set up with Symantec from which credentials such as Customer ID, Domain ID and Customer Secret Key need to be obtained. These credentials then need to be used in the configuration of the agent on a VM. More details on this extension are provided at https://techdocs.broadcom.com/us/en/symantec-security-software/endpoint-security-and-management/cloud-workload-protection/1-0/Getting_Started_1/architecture-azure-v133783726-d187e9316.html .","title":"Symantec Cloud Workload Protection (CWP) Extension for Linux and Windows VMs"},{"location":"resource-service/vmagents/#summary","text":"The table below summarizes the manner of installation for all the agents/extensions discussed above. It is assumed that the core VM agents are present by default, either because Azure standard images were used or the core VM agents were installed as one of the first configuration steps performed on VMs that were booted from custom images. VM Agent/Extension Standard Image Custom Image Disk Encryption for Linux Manual Manual Disk Encryption for Windows Manual Manual Key Vault for Linux Manual Manual Key Vault for Windows Manual Manual Policy Guest Configuration (DSC) for Linux Manual Manual Policy Guest Configuration (DSC) for Windows Manual Manual Defender for Endpoint for Linux Pre-installed Pre-installed Defender for Endpoint for Windows Pre-installed Pre-installed Vulnerability Scanner for Linux Manual Manual Vulnerability Scanner for Windows Manual Manual Microsoft Antimalware for Windows Manual Manual Backup for SQL Server for Windows Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server MARS Agent for Windows Manual Manual MABS for VMware No agents required No agents required MABS for Hyper-V Manual Manual Disk Backup No agents required No agents required Blob Backup No agents required No agents required File Share Backup No agents required No agents required SAP HANA Database Auto-installed on VM registration with Backup Server Auto-installed on VM registration with Backup Server VM Snapshot for Azure Backup for Linux Auto-installed on triggering of first backup Auto-installed on triggering of first backup VM Snapshot for Azure Backup for Windows Auto-installed on triggering of first backup Auto-installed on triggering of first backup Monitor Agent for Linux Manual Manual Monitor Agent for Windows Manual Manual Log Analytics Agent for Linux Auto-installed on Security Center enablement Auto-installed on Security Center enablement Log Analytics Agent for Windows Auto-installed on Security Center enablement Auto-installed on Security Center enablement Dependency Agent for Linux Manual Manual Dependency Agent for Windows Manual Manual Linux Diagnostic Agent and Telegraf Agent Manual Manual Windows Diagnostic Agent Manual Manual Network Watcher for Linux Manual Manual Network Watcher for Windows Manual Manual Performance Diagnostics agent for Linux Manual Manual Performance Diagnostics agent for Manual Manual Datadog extenstion for Monitoring of Linux and Windows Manual Manual Symantec Cloud Workload Protection extension for Linux and Windows Manual Manual","title":"Summary"},{"location":"security/azureactivedirectory/","text":"Section Title Here \u2693\ufe0e Add content 1.1 - Adding Links and Highlighting \u2693\ufe0e No need to section numbers for each sub-sections, just shown here for representation Type content here, highlight where needed. Add links where needed CAF 1.1.1 - How to add images \u2693\ufe0e Add Pictures to /images folder under respective sub-directories Example: for section \"Landing Zone Design - Sample Workload 1\", the foder is \"lzdesignsample1\", and the images folder will be \"lzdesignsample1/images\" Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size: 1.1.1.1 - How to Add TABLE \u2693\ufe0e Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"Section Title Here"},{"location":"security/azureactivedirectory/#section-title-here","text":"Add content","title":"Section Title Here"},{"location":"security/azureactivedirectory/#11-adding-links-and-highlighting","text":"No need to section numbers for each sub-sections, just shown here for representation Type content here, highlight where needed. Add links where needed CAF","title":"1.1 - Adding Links and Highlighting"},{"location":"security/azureactivedirectory/#111-how-to-add-images","text":"Add Pictures to /images folder under respective sub-directories Example: for section \"Landing Zone Design - Sample Workload 1\", the foder is \"lzdesignsample1\", and the images folder will be \"lzdesignsample1/images\" Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size:","title":"1.1.1 - How to add images"},{"location":"security/azureactivedirectory/#1111-how-to-add-table","text":"Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"1.1.1.1 - How to Add TABLE"},{"location":"security/sample/","text":"Section Title Here \u2693\ufe0e Add content 1.1 - Adding Links and Highlighting \u2693\ufe0e No need to section numbers for each sub-sections, just shown here for representation Type content here, highlight where needed. Add links where needed CAF 1.1.1 - How to add images \u2693\ufe0e Add Pictures to /images folder under respective sub-directories Example: for section \"Landing Zone Design - Sample Workload 1\", the foder is \"lzdesignsample1\", and the images folder will be \"lzdesignsample1/images\" Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size: 1.1.1.1 - How to Add TABLE \u2693\ufe0e Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"Section Title Here"},{"location":"security/sample/#section-title-here","text":"Add content","title":"Section Title Here"},{"location":"security/sample/#11-adding-links-and-highlighting","text":"No need to section numbers for each sub-sections, just shown here for representation Type content here, highlight where needed. Add links where needed CAF","title":"1.1 - Adding Links and Highlighting"},{"location":"security/sample/#111-how-to-add-images","text":"Add Pictures to /images folder under respective sub-directories Example: for section \"Landing Zone Design - Sample Workload 1\", the foder is \"lzdesignsample1\", and the images folder will be \"lzdesignsample1/images\" Pics should be with this naming standard: sectionname-topic-detail.jpg/.png/.svg example: lzdesign-managementgroup-overview.svg Syntax for adding pics, with size:","title":"1.1.1 - How to add images"},{"location":"security/sample/#1111-how-to-add-table","text":"Sample Table: Column A Column B Column C Column D xx b. yy Row1 Row 1 Row1 xx b. yy Row2 Row 2 Row2 xx b. yy Row3 Row 3 Row3 xx b. yy Row4 Row 4 Row4","title":"1.1.1.1 - How to Add TABLE"},{"location":"stratplan/adoptionplan/","text":"Cloud Adoption Plan \u2693\ufe0e Now that the cloud adoption strategy and business outcomes have been identified, the next step is to plan for cloud adoption. Microsoft\u2019s cloud adoption plan provides broad, detailed guidance and approach in defining this plan, which will be used as a reference for Kyndryl\u2019s cloud adoption plan. Kyndryl\u2019s cloud adoption is customized to meet our organization and delivery model and hence may not be following all the steps listed in above cloud adoption plan, but will be in alignment with the CAF plan Goal is to provide a broad guidance for solution architects to prepare their client towards cloud adoption and migration. This plan phase can be broadly classified into following areas: 1. Organization Alignment. 2. Skills Readiness. 3. Digital Estate. 4. Build migration plan 1.Organization Alignment \u2693\ufe0e In order to realize the business outcomes identified during strategy phase, organization and the people should be aligned towards the business outcomes and the plan. True organizational alignment takes time and full organization alignment is not a required component of the cloud adoption plan. At the minimum the client should be guided towards having people accountable for cloud governance if they don\u2019t have already. Cloud Governance team provides the necessary checks and balances to ensure that cloud adoption doesn't expose the business to any new risks. When risks must be taken, this team ensures that proper processes and controls are implemented to mitigate or govern those risks Cloud Adoption team which executes the technical tasks including discovery, design, migration etc. will be part of Kyndryl managed services team. Long Term Organization Alignment \u2693\ufe0e It will become important to establish long-term organizational alignment, especially as cloud adoption scales across the business and IT culture. Alignment is so important that an entire section has been dedicated to it in the organize methodology of the Cloud Adoption Framework. This is outside the scope of this document and given here as guidance. 1.1 Actions for Organization Alignment \u2693\ufe0e Validate with client if Cloud Governance team exists, and if not, guide the client in forming a cloud adoption team using CAF organization alignment as reference. Validate with client if cloud governance team exists, and if not, guide the client in forming a cloud governance team It is important to emphasize that the both teams are knowledgeable with Azure fundamentals, terminologies and portfolio including 5 R\u2019s . 2.Skills Readiness \u2693\ufe0e Roles, in the client organization, will change as client shift to cloud computing. For example, data center specialists might be replaced with cloud administrators or cloud architects. Even though Kyndryl, as the managed service provider, will be handling the technical tasks, there may still be employees in the client team who will be administrating and managing some aspects of the cloud, and hence these resources should be skilled up. At the minimum the following actions should be performed, by the client, guided by the solution architect: 1. Start with review of Microsoft CAF\u2019s skill readiness plan . 2. Identify the responsibilities that is required for this transformation. 3. Identify skills required to support each responsibility. 4. Identify roles that will execute these skills, existing roles as well as new roles (for example: DevOps Architect). 5. Finally, guide the client team to acquire skills required for these specific roles thru\u2019 Microsoft Learn Catalog . 3.Digital Estate \u2693\ufe0e Digital estate is the collection of IT assets that power business processes and supporting operations. What to Measure : Measurement of digital estate that the organization owns today is critical step required for planning and execution. What to measure in the digital estate depends on the type of business outcome identified as part of strategy: Migration, Innovation \u2013 Application & Data, and also Operational stability. Review details for each of these explained in CAF . ( NOTE : CAF calls this \u201chow...\u201d, though the contents talks about \u201cwhat\u201d should be the focus \u2013 VMs, Servers, etc) How to Measure : Digital Estate Planning - There are various approaches on how to measure the digital estate: \u2022 Top-Down workload driven approach. \u2022 Bottoms-up asset-driven approach \u2022 Incremental approach \u2013 Recommended Inventory : Following the incremental approach, the first step in planning the digital estate is gathering inventory. Again, depending on the approach, inventory methods could vary from collecting inventory of servers, databases, networks etc, to identifying applications, APIs, data etc, as explained in CAF. There are various methods to perform inventory \u2013 Agent less, Agent based, Azure native tools like Azure Migrate and 3 rd party discovery tools like Cloudscape Rationalize : Once inventory is identified, the next step is evaluating the assets to determine the best approach to hosting them in the cloud. It is very difficult to rationalize at enterprise scale and hence recommended to align with the incremental rationalization approach prescribed in CAF. This incremental rationalization process starts by focusing on reduced or specific data points from the discovery, example focus on specific data centers, or regions to be closed, or critical application workloads that need to be scaled or a combination of all. It is possible to have more than one rationalization \u2013 example, re-host and retire for specific data centers, whereas use PaaS services for specific applications 3.1 Actions for Digital Estate Planning \u2693\ufe0e Ensure the client is aligned with the incremental approach for digital estate planning. Gather Inventory through discovery. Kyndryl will follow CMM (Cloud Migration & Modernization) method for discovery & inventory which uses a combination of 3 rd party tool, Cloudscape, home grown tool and Azure migrate for discovery and assessment. Refer to \u201c Cloud Migration & Modernization Capability Guide \u201d for details. o CMM performs the discovery and stores all the results in \u201cTransition Manager\u201d repository. This repository contains the following outputs: \uf0a7 Azure Discovery/Inventory output. \uf0a7 Azure Affinity/Dependency output. \uf0a7 Azure Application inventory. Rationalize \u2013 identify, along with the client, specific workloads or data centers or application specific \u2013 and choose the best approach for migrating these to the cloud, as well as for retiring and releasing migrated assets. 4.Review Cloud Adoption Best Practices & Anti-Patterns \u2693\ufe0e Microsoft\u2019s CAF has provided few samples and best practices for cloud adoption. Review these best practices which will help in fine tuning the planning process. It is also important to validate that the plan approach against anti-patterns , defined in CAF 5. Build the plan \u2693\ufe0e Now that the inventory is available, and rationalized approach to cloud adoption has been identified, best practices and anti-patterns have been considered, next step is to start planning for the adoption. Define and prioritize workloads: Prioritize first 5-10 workloads to establish an initial adoption backlog. Map assets to workloads: Identify which assets (proposed or existing) are required to support the prioritized workloads. Review rationalization decisions: Review rationalization decisions to refine adoption path decisions: migrate or innovate. Establish iterations and release plans: Iterations are the time blocks allocated to do work. Releases are the definition of the work to be done before triggering a change to production processes. Estimate timelines: Establish rough timelines for release planning purposes, based on initial estimates REFERENCE : a. Additional planning details included in CAF . b. Planning Tool(s): Azure has a native tool, Azure DevOps Services demo generator , to assist with the planning which can be used for this purpose. Alternate planning tools can also be used by Kyndryl account team and the client. 5.1 Validate the plan \u2013 SMART Assessment \u2693\ufe0e It is important to validate the plan and get an external view of how far the plan is actually ready for migration. Have all the elements on Azure ready to on-board/migrate the workload to Azure. Fortunately, Azure\u2019s SMART (Strategic Migration & Assessment Tool) provides an easy way to assess this readiness. Perform the assessment and save/download the assessment results. 6. Update Strategy & Plan Template \u2693\ufe0e All the above actions should be recorded for client validation as well as for auditing purposes. Update Kyndryl-Azure Strategy & Plan template with the following, under \u201cCloud Adoption Plan\u201d slide SMART Assessment Results Link: Discovery/Inventory Links (from CMM): a.Azure Discovery/Inventory output. b.Azure Affinity/Dependency output. c.Azure Application inventory. d.Transition Manager Export. Link to High level adoption plan","title":"Plan"},{"location":"stratplan/adoptionplan/#cloud-adoption-plan","text":"Now that the cloud adoption strategy and business outcomes have been identified, the next step is to plan for cloud adoption. Microsoft\u2019s cloud adoption plan provides broad, detailed guidance and approach in defining this plan, which will be used as a reference for Kyndryl\u2019s cloud adoption plan. Kyndryl\u2019s cloud adoption is customized to meet our organization and delivery model and hence may not be following all the steps listed in above cloud adoption plan, but will be in alignment with the CAF plan Goal is to provide a broad guidance for solution architects to prepare their client towards cloud adoption and migration. This plan phase can be broadly classified into following areas: 1. Organization Alignment. 2. Skills Readiness. 3. Digital Estate. 4. Build migration plan","title":"Cloud Adoption Plan"},{"location":"stratplan/adoptionplan/#1organization-alignment","text":"In order to realize the business outcomes identified during strategy phase, organization and the people should be aligned towards the business outcomes and the plan. True organizational alignment takes time and full organization alignment is not a required component of the cloud adoption plan. At the minimum the client should be guided towards having people accountable for cloud governance if they don\u2019t have already. Cloud Governance team provides the necessary checks and balances to ensure that cloud adoption doesn't expose the business to any new risks. When risks must be taken, this team ensures that proper processes and controls are implemented to mitigate or govern those risks Cloud Adoption team which executes the technical tasks including discovery, design, migration etc. will be part of Kyndryl managed services team.","title":"1.Organization Alignment"},{"location":"stratplan/adoptionplan/#long-term-organization-alignment","text":"It will become important to establish long-term organizational alignment, especially as cloud adoption scales across the business and IT culture. Alignment is so important that an entire section has been dedicated to it in the organize methodology of the Cloud Adoption Framework. This is outside the scope of this document and given here as guidance.","title":"Long Term Organization Alignment"},{"location":"stratplan/adoptionplan/#11-actions-for-organization-alignment","text":"Validate with client if Cloud Governance team exists, and if not, guide the client in forming a cloud adoption team using CAF organization alignment as reference. Validate with client if cloud governance team exists, and if not, guide the client in forming a cloud governance team It is important to emphasize that the both teams are knowledgeable with Azure fundamentals, terminologies and portfolio including 5 R\u2019s .","title":"1.1 Actions for Organization Alignment"},{"location":"stratplan/adoptionplan/#2skills-readiness","text":"Roles, in the client organization, will change as client shift to cloud computing. For example, data center specialists might be replaced with cloud administrators or cloud architects. Even though Kyndryl, as the managed service provider, will be handling the technical tasks, there may still be employees in the client team who will be administrating and managing some aspects of the cloud, and hence these resources should be skilled up. At the minimum the following actions should be performed, by the client, guided by the solution architect: 1. Start with review of Microsoft CAF\u2019s skill readiness plan . 2. Identify the responsibilities that is required for this transformation. 3. Identify skills required to support each responsibility. 4. Identify roles that will execute these skills, existing roles as well as new roles (for example: DevOps Architect). 5. Finally, guide the client team to acquire skills required for these specific roles thru\u2019 Microsoft Learn Catalog .","title":"2.Skills Readiness"},{"location":"stratplan/adoptionplan/#3digital-estate","text":"Digital estate is the collection of IT assets that power business processes and supporting operations. What to Measure : Measurement of digital estate that the organization owns today is critical step required for planning and execution. What to measure in the digital estate depends on the type of business outcome identified as part of strategy: Migration, Innovation \u2013 Application & Data, and also Operational stability. Review details for each of these explained in CAF . ( NOTE : CAF calls this \u201chow...\u201d, though the contents talks about \u201cwhat\u201d should be the focus \u2013 VMs, Servers, etc) How to Measure : Digital Estate Planning - There are various approaches on how to measure the digital estate: \u2022 Top-Down workload driven approach. \u2022 Bottoms-up asset-driven approach \u2022 Incremental approach \u2013 Recommended Inventory : Following the incremental approach, the first step in planning the digital estate is gathering inventory. Again, depending on the approach, inventory methods could vary from collecting inventory of servers, databases, networks etc, to identifying applications, APIs, data etc, as explained in CAF. There are various methods to perform inventory \u2013 Agent less, Agent based, Azure native tools like Azure Migrate and 3 rd party discovery tools like Cloudscape Rationalize : Once inventory is identified, the next step is evaluating the assets to determine the best approach to hosting them in the cloud. It is very difficult to rationalize at enterprise scale and hence recommended to align with the incremental rationalization approach prescribed in CAF. This incremental rationalization process starts by focusing on reduced or specific data points from the discovery, example focus on specific data centers, or regions to be closed, or critical application workloads that need to be scaled or a combination of all. It is possible to have more than one rationalization \u2013 example, re-host and retire for specific data centers, whereas use PaaS services for specific applications","title":"3.Digital Estate"},{"location":"stratplan/adoptionplan/#31-actions-for-digital-estate-planning","text":"Ensure the client is aligned with the incremental approach for digital estate planning. Gather Inventory through discovery. Kyndryl will follow CMM (Cloud Migration & Modernization) method for discovery & inventory which uses a combination of 3 rd party tool, Cloudscape, home grown tool and Azure migrate for discovery and assessment. Refer to \u201c Cloud Migration & Modernization Capability Guide \u201d for details. o CMM performs the discovery and stores all the results in \u201cTransition Manager\u201d repository. This repository contains the following outputs: \uf0a7 Azure Discovery/Inventory output. \uf0a7 Azure Affinity/Dependency output. \uf0a7 Azure Application inventory. Rationalize \u2013 identify, along with the client, specific workloads or data centers or application specific \u2013 and choose the best approach for migrating these to the cloud, as well as for retiring and releasing migrated assets.","title":"3.1 Actions for Digital Estate Planning"},{"location":"stratplan/adoptionplan/#4review-cloud-adoption-best-practices-anti-patterns","text":"Microsoft\u2019s CAF has provided few samples and best practices for cloud adoption. Review these best practices which will help in fine tuning the planning process. It is also important to validate that the plan approach against anti-patterns , defined in CAF","title":"4.Review Cloud Adoption Best Practices &amp; Anti-Patterns"},{"location":"stratplan/adoptionplan/#5-build-the-plan","text":"Now that the inventory is available, and rationalized approach to cloud adoption has been identified, best practices and anti-patterns have been considered, next step is to start planning for the adoption. Define and prioritize workloads: Prioritize first 5-10 workloads to establish an initial adoption backlog. Map assets to workloads: Identify which assets (proposed or existing) are required to support the prioritized workloads. Review rationalization decisions: Review rationalization decisions to refine adoption path decisions: migrate or innovate. Establish iterations and release plans: Iterations are the time blocks allocated to do work. Releases are the definition of the work to be done before triggering a change to production processes. Estimate timelines: Establish rough timelines for release planning purposes, based on initial estimates REFERENCE : a. Additional planning details included in CAF . b. Planning Tool(s): Azure has a native tool, Azure DevOps Services demo generator , to assist with the planning which can be used for this purpose. Alternate planning tools can also be used by Kyndryl account team and the client.","title":"5. Build the plan"},{"location":"stratplan/adoptionplan/#51-validate-the-plan-smart-assessment","text":"It is important to validate the plan and get an external view of how far the plan is actually ready for migration. Have all the elements on Azure ready to on-board/migrate the workload to Azure. Fortunately, Azure\u2019s SMART (Strategic Migration & Assessment Tool) provides an easy way to assess this readiness. Perform the assessment and save/download the assessment results.","title":"5.1 Validate the plan \u2013 SMART Assessment"},{"location":"stratplan/adoptionplan/#6-update-strategy-plan-template","text":"All the above actions should be recorded for client validation as well as for auditing purposes. Update Kyndryl-Azure Strategy & Plan template with the following, under \u201cCloud Adoption Plan\u201d slide SMART Assessment Results Link: Discovery/Inventory Links (from CMM): a.Azure Discovery/Inventory output. b.Azure Affinity/Dependency output. c.Azure Application inventory. d.Transition Manager Export. Link to High level adoption plan","title":"6. Update Strategy &amp; Plan Template"},{"location":"stratplan/adoptionstrategy/","text":"Cloud Adoption Strategy \u2693\ufe0e There are four steps to be followed to understand client needs and define the cloud adoption strategy. Cloud adoption strategy can then be mapped to specific cloud capabilities and business strategies to meet the desired state. This section provides brief on each of these four steps and the actions need to be taken to define the strategy. For reference, details on the four areas are available in CAF . It is important this effort should be performed along with client stakeholders , and the stakeholder names should be documented. 1.Define Motivation \u2693\ufe0e Business transformations that are supported by cloud adoption can be driven by various motivations. It is possible and likely that several motivations apply at the same time. Microsoft CAF provides a list of relevant motivations that should be considered, at the minimum. CAF broadly classifies the motivations into three areas: \u2022 Critical Business Event \u2022 Migration \u2022 Innovation There may be additional motivations for a specific account(s), in which case, map those motivation to one of the three classifications. 1.1 ACTION \u2013 Document Motivations & Classifications \u2693\ufe0e It is important for the solution architect to document the motivations and the priority along with client stakeholders. Update the Kyndryl-Azure Strategy & Plan template to document the motivations : 1) Identify the list of motivations, using CAF motivations as the starting point. 2) Prioritize each motivation into High, Medium & Low, based on business needs. 3) Identify which classification has the highest priority for the business. In the example below Data Center exit has got the high priority and hence critical business event classification shall be given priority, over others. Classification Motivations with priority Critical Business Event 1. Data center exit (H), 2. Regulatory Compliance (M) Migration 1. Cost Savings (M), 2. Capex Reduction (M), 3. Reduce Carbon Footprint(L) Innovation 1. Scaling to meet geographic demands (M), 2. Other justifications (L) The identified priority from above helps in driving the conversation towards the next set of actions, including choosing the first adoption project, which is explained later in this chapter. 2.Define Business Outcomes \u2693\ufe0e The next step is to help customers identify desired business outcomes that are concise, defined, and drive observable results or change in business performance, supported by a specific measure. Review business outcomes in Microsoft CAF, to learn and understand this topic. After gaining an understanding of business outcomes, review how to document business outcomes . 2.1 ACTION - Document Business Outcomes \u2693\ufe0e Document the business outcomes in Kyndryl-Azure Strategy & Plan template At the minimum document the following in the template, sample included in above template: o Key business outcomes o Key capabilities required o KPIs (if data available) 3. Define Business Case/Justification \u2693\ufe0e A business case provides a view of the technical and financial timeline of your environment and can represent the opportunities for reinvestment into further modernization. Developing a business case includes building a financial plan that takes technical considerations into account and aligns with business outcomes Business case should consider financial aspects, environment scope, projected savings and many more. Use the business case guidance provided in CAF, as the starting point. In addition, there are native tools in Azure that could be used for estimating costs - Azure Pricing Calculator and Total Cost of Ownership (TCO) & ROI Calculator Technical Considerations: When making the shift to the cloud, there are technical considerations around how it will help improve manage and maintain cloud and workloads. This guidance will help discover the technical flexibility, efficiencies, capabilities and various options for cost optimization which aren\u2019t possible with on-premises IT infrastructure. Review the technical considerations which will help the business case to migrate to the cloud. 3.1 ACTION - Document Business Justification \u2013 Technical & Financial \u2693\ufe0e Document business justification in the in the Kyndryl-Azure Strategy & Plan template 1) This section in the above template should list tangible and intangible benefits to the client by adopting the cloud strategy and the capabilities from Azure. Some examples are listed below, and more can be found in CAF links above: Estimated Cost Savings Reduced Data Center Footprint Increased Productivity & Service Delivery 2) Use TCO calculator to estimate the TCO and ROI and add these links in the Kyndryl-Azure Strategy & Plan template 4.Choose First Adoption Project \u2693\ufe0e The first adoption project should align with the motivations behind cloud adoption. Whenever possible, the first project should also demonstrate progress towards a defined business outcome. Refer to the priority that was derived in the motivations ADD LINK section. Based on the priorities assigned to each of the motivations, one of the three classifications would result in higher priority. Below table provides guidance on the action to be taken based on the classification. Motivation Classifications & Priority Action Critical Business Event Accelerated migration or adoption at the earliest possible timeframe, in parallel with strategy & planning Migration Strategy & Planning Others (Optional) - if there are other business motivations not fitting into the above classification, define a new one. Example is \u201cOperational Stability\u201d Below are some sample actions for guidance: 1) If the priority is to respond to a critical business event, consider implementing Azure Site Recovery and set it up as a disaster recovery tool for the first project to quickly reduce current dependencies within the data center. 2) If a server migration has priority, consider starting with the migration of a noncritical workload and leveraging the Azure Readiness Guide and the Azure Migration Guide as guidance for the migration of that first workload. 3) If the priority is to innovate or modernize current application, consider creating a targeted dev/test environment for your application. 4.1 ACTION \u2013 Identify & Document First adoption Project \u2693\ufe0e Based on above guidelines, identify an application or set of workloads for initial adoption, which could be considered a Proof-of-Concept (PoC). For example, if Critical Business Event classification had high priority, then identify an application for which Azure Site Recovery can be implemented. Document the first adoption project in Kyndryl-Azure Strategy & Plan template .","title":"Cloud Adoption Strategy"},{"location":"stratplan/adoptionstrategy/#cloud-adoption-strategy","text":"There are four steps to be followed to understand client needs and define the cloud adoption strategy. Cloud adoption strategy can then be mapped to specific cloud capabilities and business strategies to meet the desired state. This section provides brief on each of these four steps and the actions need to be taken to define the strategy. For reference, details on the four areas are available in CAF . It is important this effort should be performed along with client stakeholders , and the stakeholder names should be documented.","title":"Cloud Adoption Strategy"},{"location":"stratplan/adoptionstrategy/#1define-motivation","text":"Business transformations that are supported by cloud adoption can be driven by various motivations. It is possible and likely that several motivations apply at the same time. Microsoft CAF provides a list of relevant motivations that should be considered, at the minimum. CAF broadly classifies the motivations into three areas: \u2022 Critical Business Event \u2022 Migration \u2022 Innovation There may be additional motivations for a specific account(s), in which case, map those motivation to one of the three classifications.","title":"1.Define Motivation"},{"location":"stratplan/adoptionstrategy/#11-action-document-motivations-classifications","text":"It is important for the solution architect to document the motivations and the priority along with client stakeholders. Update the Kyndryl-Azure Strategy & Plan template to document the motivations : 1) Identify the list of motivations, using CAF motivations as the starting point. 2) Prioritize each motivation into High, Medium & Low, based on business needs. 3) Identify which classification has the highest priority for the business. In the example below Data Center exit has got the high priority and hence critical business event classification shall be given priority, over others. Classification Motivations with priority Critical Business Event 1. Data center exit (H), 2. Regulatory Compliance (M) Migration 1. Cost Savings (M), 2. Capex Reduction (M), 3. Reduce Carbon Footprint(L) Innovation 1. Scaling to meet geographic demands (M), 2. Other justifications (L) The identified priority from above helps in driving the conversation towards the next set of actions, including choosing the first adoption project, which is explained later in this chapter.","title":"1.1 ACTION \u2013 Document Motivations &amp; Classifications"},{"location":"stratplan/adoptionstrategy/#2define-business-outcomes","text":"The next step is to help customers identify desired business outcomes that are concise, defined, and drive observable results or change in business performance, supported by a specific measure. Review business outcomes in Microsoft CAF, to learn and understand this topic. After gaining an understanding of business outcomes, review how to document business outcomes .","title":"2.Define Business Outcomes"},{"location":"stratplan/adoptionstrategy/#21-action-document-business-outcomes","text":"Document the business outcomes in Kyndryl-Azure Strategy & Plan template At the minimum document the following in the template, sample included in above template: o Key business outcomes o Key capabilities required o KPIs (if data available)","title":"2.1 ACTION - Document Business Outcomes"},{"location":"stratplan/adoptionstrategy/#3-define-business-casejustification","text":"A business case provides a view of the technical and financial timeline of your environment and can represent the opportunities for reinvestment into further modernization. Developing a business case includes building a financial plan that takes technical considerations into account and aligns with business outcomes Business case should consider financial aspects, environment scope, projected savings and many more. Use the business case guidance provided in CAF, as the starting point. In addition, there are native tools in Azure that could be used for estimating costs - Azure Pricing Calculator and Total Cost of Ownership (TCO) & ROI Calculator Technical Considerations: When making the shift to the cloud, there are technical considerations around how it will help improve manage and maintain cloud and workloads. This guidance will help discover the technical flexibility, efficiencies, capabilities and various options for cost optimization which aren\u2019t possible with on-premises IT infrastructure. Review the technical considerations which will help the business case to migrate to the cloud.","title":"3. Define Business Case/Justification"},{"location":"stratplan/adoptionstrategy/#31-action-document-business-justification-technical-financial","text":"Document business justification in the in the Kyndryl-Azure Strategy & Plan template 1) This section in the above template should list tangible and intangible benefits to the client by adopting the cloud strategy and the capabilities from Azure. Some examples are listed below, and more can be found in CAF links above: Estimated Cost Savings Reduced Data Center Footprint Increased Productivity & Service Delivery 2) Use TCO calculator to estimate the TCO and ROI and add these links in the Kyndryl-Azure Strategy & Plan template","title":"3.1 ACTION - Document Business Justification \u2013 Technical &amp; Financial"},{"location":"stratplan/adoptionstrategy/#4choose-first-adoption-project","text":"The first adoption project should align with the motivations behind cloud adoption. Whenever possible, the first project should also demonstrate progress towards a defined business outcome. Refer to the priority that was derived in the motivations ADD LINK section. Based on the priorities assigned to each of the motivations, one of the three classifications would result in higher priority. Below table provides guidance on the action to be taken based on the classification. Motivation Classifications & Priority Action Critical Business Event Accelerated migration or adoption at the earliest possible timeframe, in parallel with strategy & planning Migration Strategy & Planning Others (Optional) - if there are other business motivations not fitting into the above classification, define a new one. Example is \u201cOperational Stability\u201d Below are some sample actions for guidance: 1) If the priority is to respond to a critical business event, consider implementing Azure Site Recovery and set it up as a disaster recovery tool for the first project to quickly reduce current dependencies within the data center. 2) If a server migration has priority, consider starting with the migration of a noncritical workload and leveraging the Azure Readiness Guide and the Azure Migration Guide as guidance for the migration of that first workload. 3) If the priority is to innovate or modernize current application, consider creating a targeted dev/test environment for your application.","title":"4.Choose First Adoption Project"},{"location":"stratplan/adoptionstrategy/#41-action-identify-document-first-adoption-project","text":"Based on above guidelines, identify an application or set of workloads for initial adoption, which could be considered a Proof-of-Concept (PoC). For example, if Critical Business Event classification had high priority, then identify an application for which Azure Site Recovery can be implemented. Document the first adoption project in Kyndryl-Azure Strategy & Plan template .","title":"4.1  ACTION \u2013 Identify &amp; Document First adoption Project"},{"location":"stratplan/stratplanoverview/","text":"Strategy & Plan Overview \u2693\ufe0e Purpose of this section is to guide cloud solution architects and cloud practitioners to define a cloud adoption strategy & plan for Kyndryl\u2019s clients. Guidance in this document is based on Kyndryl's Cloud Technology Strategy Roadmap and Microsoft's Cloud Adoption Strategy . CAF is a full lifecycle framework, supporting customers throughout each phase of adoption. As shown in the diagram below, it consists of the Strategy, Plan, Readiness, Adopt, Govern and Manage phases. This section covers the Strategy and Plan phases. The other phases (sans Adopt) are covered in the Landing Zone design section. Expected Outcome from Solution Architects \u2693\ufe0e Solution Architects will use this guidance to create a Strategy & Plan artifact for each client, in the format provided in the Strategy & Plan template . Follow the steps in this section to populate the above template . This is required for auditing purposes , and a compliancy requirement from Microsoft as part of MSP (Managed Service Provider) responsibilities . Cloud Adoption Fundamentals - Pre-Requisite Learning \u2693\ufe0e In order to define the right cloud adoption strategy in Microsoft Azure, cloud solution Architects should be familiar and have an understanding of cloud fundamentals. Below is a filtered list of topics from Microsoft\u2019s CAF and the reader is expected to be through with these key cloud fundamentals. Though these are specific to Azure, some of the contents are generic enough to be applied and adopted for any cloud. Choose the cloud adoption scenario that best supports your strategy Hybrid & Multi-Cloud Adoption Unified Operations Examine antipatterns across methodologies and their solutions Adopt the cloud to deliver business and technical outcomes sooner","title":"Strategy & Planning Overview"},{"location":"stratplan/stratplanoverview/#strategy-plan-overview","text":"Purpose of this section is to guide cloud solution architects and cloud practitioners to define a cloud adoption strategy & plan for Kyndryl\u2019s clients. Guidance in this document is based on Kyndryl's Cloud Technology Strategy Roadmap and Microsoft's Cloud Adoption Strategy . CAF is a full lifecycle framework, supporting customers throughout each phase of adoption. As shown in the diagram below, it consists of the Strategy, Plan, Readiness, Adopt, Govern and Manage phases. This section covers the Strategy and Plan phases. The other phases (sans Adopt) are covered in the Landing Zone design section.","title":"Strategy &amp; Plan Overview"},{"location":"stratplan/stratplanoverview/#expected-outcome-from-solution-architects","text":"Solution Architects will use this guidance to create a Strategy & Plan artifact for each client, in the format provided in the Strategy & Plan template . Follow the steps in this section to populate the above template . This is required for auditing purposes , and a compliancy requirement from Microsoft as part of MSP (Managed Service Provider) responsibilities .","title":"Expected Outcome from Solution Architects"},{"location":"stratplan/stratplanoverview/#cloud-adoption-fundamentals-pre-requisite-learning","text":"In order to define the right cloud adoption strategy in Microsoft Azure, cloud solution Architects should be familiar and have an understanding of cloud fundamentals. Below is a filtered list of topics from Microsoft\u2019s CAF and the reader is expected to be through with these key cloud fundamentals. Though these are specific to Azure, some of the contents are generic enough to be applied and adopted for any cloud. Choose the cloud adoption scenario that best supports your strategy Hybrid & Multi-Cloud Adoption Unified Operations Examine antipatterns across methodologies and their solutions Adopt the cloud to deliver business and technical outcomes sooner","title":"Cloud Adoption Fundamentals - Pre-Requisite Learning"},{"location":"virtualwan/overview/","text":"Virtual WAN - Design and Implementation \u2693\ufe0e The purpose of this proof of concept is to identify a suitable use case that leverages the features unique to Azure Virtual WAN provide a detailed design for a solution that addresses this use case implement and test the design to demonstrate the intended features of Virtual WAN The detailed document covering all of the above is provided at https://kyndryl.box.com/s/iie5zba8znj56gs7st73w7rzmi6k5hwy .","title":"Overview and Link to Details"},{"location":"virtualwan/overview/#virtual-wan-design-and-implementation","text":"The purpose of this proof of concept is to identify a suitable use case that leverages the features unique to Azure Virtual WAN provide a detailed design for a solution that addresses this use case implement and test the design to demonstrate the intended features of Virtual WAN The detailed document covering all of the above is provided at https://kyndryl.box.com/s/iie5zba8znj56gs7st73w7rzmi6k5hwy .","title":"Virtual WAN - Design and Implementation"}]}